[
  {
    "job_id": "CHEMISTRY-25-505",
    "title": "Chemist",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$35 - $69 / hour",
    "pay_min": 35,
    "pay_max": 69,
    "currency": "USD",
    "num_people": 70,
    "priority_level": 1,
    "role_description": "We’re seeking chemists to join our AI training project. In this role, you will review AI-generated responses, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in calculations, methodology, or conceptual understanding; fact-check chemical information across; write high-quality chemical explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on chemical correctness and reasoning quality.",
    "keywords": "Chemistry, Organic Chemistry, Analytical Chemistry, Medicinal Chemistry, Inorganic Chemistry, English, Writing, Teaching, Research, Reasoning Skills",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "You hold an MS or PhD in Chemistry or a related field in a high ranking university. | Confident in chemical reactions and stoichiometry, organic mechanisms, analytical methods, and physical/quantum chemistry. | Expert at fact-checking information using public sources across multiple domains. You communicate chemistry clearly and turn advanced theory into real-world solutions. | English proficiency C1 and above with excellent writing skills. | Background in research, analytical writing, debate, programming, or chemistry is preferred. | Bonus: Data labeling, RLHF, or AI model evaluation experience. | You enjoy applying chemical reasoning to evaluate and improve AI capabilities. | For PHD holders: you have developed or critically reviewed complex chemistry content (e.g., problem banks, proofs, textbook sections, research notes).",
    "sample_interview_questions": "Can you walk me through how you would verify the correctness of a multistep stoichiometric calculation for a limiting reagent problem? | How do you typically check whether a proposed acid base equilibrium setup and resulting pH calculation are conceptually and numerically sound? | Describe how you evaluate the plausibility of an organic reaction mechanism that a student or colleague has drawn. | When reviewing a synthetic route, what specific factors do you consider to judge whether it is practical, efficient, and reasonably safe? | If you were asked to review an explanation of Le Chatelier’s principle aimed at high school students, what features would you expect in a strong explanation and what red flags would concern you? | How do you approach situations where different reputable references provide slightly different data, such as pKa values or standard potentials? | Explain how you would check a model’s explanation for a buffer preparation problem that appears correct numerically but feels conceptually confusing. | Which areas of chemistry do you consider your strongest, and which areas would you need to refresh to confidently review detailed material? | Tell me about your experience, if any, in teaching, tutoring, grading, or peer reviewing chemistry work, and how that experience shapes the way you give feedback. | How do you keep your chemistry knowledge current, and what resources do you rely on most frequently when you need to double check a concept or data point?",
    "source_url": "https://editor.superannotate.com/jobs/CHEMISTRY-25-505"
  },
  {
    "job_id": "CHEMISTRY-25-504",
    "title": "Chemist",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$20 - $40 / hour",
    "pay_min": 20,
    "pay_max": 40,
    "currency": "USD",
    "num_people": 30,
    "priority_level": 0,
    "role_description": "We’re seeking chemists to join our AI training project. In this role, you will review AI-generated responses, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in calculations, methodology, or conceptual understanding; fact-check chemical information across; write high-quality chemical explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on chemical correctness and reasoning quality.",
    "keywords": "Chemistry, Organic Chemistry, Analytical Chemistry, Medicinal Chemistry, Inorganic Chemistry, English, Writing, Teaching, Research, Reasoning Skills",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "You hold a BS degree in Chemistry or a related field in a high ranking university. | Confident in chemical reactions and stoichiometry, organic mechanisms, analytical methods, and physical/quantum chemistry. | Expert at fact-checking information using public sources across multiple domains. You communicate chemistry clearly and turn advanced theory into real-world solutions. | English proficiency C1 and above with excellent writing skills. | Background in research, analytical writing, debate, programming, or chemistry is preferred. | Bonus: Data labeling, RLHF, or AI model evaluation experience. | You enjoy applying chemical reasoning to evaluate and improve AI capabilities.",
    "sample_interview_questions": "Can you walk me through how you would verify the correctness of a multistep stoichiometric calculation for a limiting reagent problem? | How do you typically check whether a proposed acid base equilibrium setup and resulting pH calculation are conceptually and numerically sound? | Describe how you evaluate the plausibility of an organic reaction mechanism that a student or colleague has drawn. | When reviewing a synthetic route, what specific factors do you consider to judge whether it is practical, efficient, and reasonably safe? | Describe a situation where you spotted a subtle error in a chemistry solution or lab report that others had missed. What was the issue and how did you notice it? | How do you approach situations where different reputable references provide slightly different data, such as pKa values or standard potentials? | Explain how you would check a model’s explanation for a buffer preparation problem that appears correct numerically but feels conceptually confusing. | Which areas of chemistry do you consider your strongest, and which areas would you need to refresh to confidently review detailed material? | Tell me about your experience, if any, in teaching, tutoring, grading, or peer reviewing chemistry work, and how that experience shapes the way you give feedback. | How do you keep your chemistry knowledge current, and what resources do you rely on most frequently when you need to double check a concept or data point?",
    "source_url": "https://editor.superannotate.com/jobs/CHEMISTRY-25-504"
  },
  {
    "job_id": "MATH-25-501",
    "title": "Mathematics Specialist",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$22 - $45 / hour",
    "pay_min": 22,
    "pay_max": 45,
    "currency": "USD",
    "num_people": 30,
    "priority_level": 0,
    "role_description": "SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate, is seeking mathematicians to join our AI training project. In this role, you will review AI-generated responses, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in calculations, methodology, or conceptual understanding; fact-check quantitative information across; write high-quality mathematical explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on mathematical correctness and reasoning quality.",
    "keywords": "Mathematics, Problem Solving, Pure Mathematics, Applied Mathematics, Fact-checking, Advanced Mathematics, Discrete Mathematics, English, Reasoning, Business Mathematics",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "You hold BS degree in Math/Stats or a related field in one of the high ranking universities | Confident in pure and applied mathematics, proofs, modeling, statistics/probability, and optimization. | Expert at fact-checking information using public sources across multiple domains. | You communicate mathematics clearly and turn advanced theory into real-world solutions. | Background in research, analytical writing, debate, programming, or mathematics is preferred. | English proficiency C1 and above with excellent writing skills. | Bonus: Data labeling, RLHF, or AI model evaluation experience.",
    "sample_interview_questions": "Can you walk me through your academic background in mathematics and which core areas you feel strongest in? | When reviewing an algebraic solution with several manipulation steps, what specific details do you focus on to catch mistakes? | Describe how you would explain the concept of a limit in calculus to someone who has only studied algebra so far. | How do you decide whether a solution that reaches the correct numerical answer but uses unclear or imprecise reasoning is acceptable? | How do you approach evaluating the clarity of a math explanation for a non-expert audience while still preserving rigor? | Tell me about a time you followed a detailed grading rubric or marking scheme for math assignments or exams. How did you ensure consistency? | If you are given two different correct solutions to the same problem, what criteria would you use to judge which one is more helpful for learning? | How do you handle math topics that are less fresh in your mind while still ensuring accurate judgments and feedback? | Can you describe how you would explain the difference between correlation and causation to a student using a simple example? | How would you verbally describe the idea of mathematical proof to a learner who has only seen worked examples, not formal proofs?",
    "source_url": "https://editor.superannotate.com/jobs/MATH-25-501"
  },
  {
    "job_id": "MATH-25-502",
    "title": "Mathematics Specialist",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$45 - $69 / hour",
    "pay_min": 45,
    "pay_max": 69,
    "currency": "USD",
    "num_people": 70,
    "priority_level": 3,
    "role_description": "SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate, is seeking mathematicians to join our AI training project. In this role, you will review AI-generated responses, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in calculations, methodology, or conceptual understanding; fact-check quantitative information across; write high-quality mathematical explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on mathematical correctness and reasoning quality.",
    "keywords": "Mathematics, Problem Solving, Pure Mathematics, Applied Mathematics, Fact-checking, Advanced Mathematics, Discrete Mathematics, English, Reasoning, Business Mathematics",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "You hold a MS or PhD in Math/Stats or a related field in one of the top 100 universities. | Confident in pure and applied mathematics, proofs, modeling, statistics/probability, and optimization. | Expert at fact-checking information using public sources across multiple domains. | You communicate mathematics clearly and turn advanced theory into real-world solutions. | Background in research, analytical writing, debate, programming, or mathematics is preferred. | English proficiency C1 and above with excellent writing skills. | For PhD holderss: you have developed or critically reviewed complex math content (e.g., problem banks, proofs, textbook sections, research notes). | Bonus: Data labeling, RLHF, or AI model evaluation experience.",
    "sample_interview_questions": "Can you walk me through your academic background in mathematics and which core areas you feel strongest in? | When reviewing an algebraic solution with several manipulation steps, what specific details do you focus on to catch mistakes? | Describe how you would explain the concept of a limit in calculus to someone who has only studied algebra so far. | How do you decide whether a solution that reaches the correct numerical answer but uses unclear or imprecise reasoning is acceptable? | How do you approach evaluating the clarity of a math explanation for a non-expert audience while still preserving rigor? | Tell me about a time you followed a detailed grading rubric or marking scheme for math assignments or exams. How did you ensure consistency? | If you are given two different correct solutions to the same problem, what criteria would you use to judge which one is more helpful for learning? | How do you handle math topics that are less fresh in your mind while still ensuring accurate judgments and feedback? | Can you describe how you would explain the difference between correlation and causation to a student using a simple example? | How would you verbally describe the idea of mathematical proof to a learner who has only seen worked examples, not formal proofs?",
    "source_url": "https://editor.superannotate.com/jobs/MATH-25-502"
  },
  {
    "job_id": "PHYSICS-25-502",
    "title": "Physicist",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$22 - $45 / hour",
    "pay_min": 22,
    "pay_max": 45,
    "currency": "USD",
    "num_people": 30,
    "priority_level": 0,
    "role_description": "We’re seeking physicists to join our AI training project. In this role, you will review AI-generated responses, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in calculations, methodology, or conceptual understanding; fact-check physics information; write high-quality physics explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on physics correctness and reasoning quality.",
    "keywords": "Physics, Reasoning, Experimental Physics, Laser Physics, Medical Physics, Computational Physics, English, Nuclear Physics, Theoretical Physics, Research",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "You hold a Bachelor's Degree in Physics or a related field in one of the top 100 universities. | Confident in physical reasoning, mechanics, electromagnetism, optics, thermodynamics/statistical mechanics, and modeling/systems thinking | Expert at fact-checking information using public sources across multiple domains. | You communicate physics clearly and turn advanced theory into real-world solutions. | Background in research, analytical writing, debate, programming, or physics is preferred. | English proficiency C1 and above with excellent writing skills. | Bonus: Data labeling, RLHF, or AI model evaluation experience.",
    "sample_interview_questions": "How would you explain the difference between scalar and vector quantities to an undergraduate student encountering the concept for the first time? | When you check a multi-step mechanics solution, what specific elements do you look at to decide whether the reasoning is correct even if the final answer is wrong? | How do you distinguish between a minor algebraic mistake and a serious conceptual error when reviewing a physics problem solution? | In your experience, what makes a physics explanation clear and accessible for a learner who is struggling with the topic? | Imagine two different solutions to the same electromagnetism problem: one is very short but uses advanced math, the other is longer but more step-by-step. How would you decide which one is better for an undergraduate audience? | Describe how you would review a thermodynamics solution that uses many approximations. What would you pay attention to when judging its quality? | How would you identify and handle potentially unsafe or misleading physics-related content, such as poorly described lab procedures or unrealistic safety assumptions? | How would you approach learning and consistently applying a detailed set of written guidelines for judging physics answers and explanations? | How do you ensure that your own biases or preferred solution methods do not interfere with your ability to fairly evaluate alternative valid approaches? | What strategies do you use to stay accurate and focused when performing repetitive, detail-heavy review work for several hours in a row?",
    "source_url": "https://editor.superannotate.com/jobs/PHYSICS-25-502"
  },
  {
    "job_id": "PHYSICS-25-503",
    "title": "Physicist",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$40 - $69 / hour",
    "pay_min": 40,
    "pay_max": 69,
    "currency": "USD",
    "num_people": 70,
    "priority_level": 1,
    "role_description": "We’re seeking physicists to join our AI training project. In this role, you will review AI-generated responses, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in calculations, methodology, or conceptual understanding; fact-check physics information; write high-quality physics explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on physics correctness and reasoning quality.",
    "keywords": "Physics, Reasoning, Experimental Physics, Laser Physics, Medical Physics, Computational Physics, English, Nuclear Physics, Theoretical Physics, Research",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "You hold an MS or PhD in Physics or a related field in one of the top 100 universities. | Confident in physical reasoning, mechanics, electromagnetism, optics, thermodynamics/statistical mechanics, and modeling/systems thinking | You have developed or critically reviewed complex physics content (e.g., problem banks, proofs, textbook sections, research notes). | Expert at fact-checking information using public sources across multiple domains. | You communicate physics clearly and turn advanced theory into real-world solutions. | Background in research, analytical writing, debate, programming, or physics is preferred. | Published in a peer-reviewed journal. | English proficiency C1 and above with excellent writing skills. | Bonus: Data labeling, RLHF, or AI model evaluation experience.",
    "sample_interview_questions": "Describe how you would detect and handle a subtle conceptual error in a submitted statistical physics derivation that only appears under certain limiting cases. | How do you validate that a proposed derivation involving tensor calculus and General Relativity notation is dimensionally and index-consistently correct? | In your experience, what distinguishes a clear and pedagogically effective explanation of a graduate-level physics concept from one that is technically correct but hard to understand for learners? | Given two different solution approaches to the same quantum mechanics problem — one using operator formalism, another using wave functions — how would you choose which to include for model training and why? | How would you approach giving feedback when a solution is correct but uses unconventional notation or style that might confuse other users or learners? | How comfortable are you assessing physics content in advanced fields (e.g. quantum field theory or general relativity) even if the tasks don’t require full research-level depth, and what would you do when you’re uncertain about an edge case? | What tools (e.g. symbolic math software, LaTeX, computational notebooks) do you typically use to double-check complex calculations or derivations, and how would they integrate into a remote annotation workflow? | How would you identify and flag physics content that could be misleading or dangerous (e.g. incorrectly described radiation procedures or unrealistic thought experiments) in a dataset intended for public use? | How would you handle a scenario where you need to follow a detailed but partially ambiguous guideline to evaluate or annotate graduate-level physics content — what steps would you take for consistency and quality? | Give an example of how you would simplify a high-level physics explanation to make it more accessible to undergraduates while preserving technical correctness.",
    "source_url": "https://editor.superannotate.com/jobs/PHYSICS-25-503"
  },
  {
    "job_id": "BIOLOGY-25-503",
    "title": "Biologist",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$18 - $40 / hour",
    "pay_min": 18,
    "pay_max": 40,
    "currency": "USD",
    "num_people": 30,
    "priority_level": 0,
    "role_description": "We’re seeking biologists to join our AI training project. In this role, you will review AI-generated responses, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check biological information; write high-quality biological explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on biological correctness and reasoning quality.",
    "keywords": "Molecular Biology, Computational Biology, Systems Biology, Molecular & Cellular Biology, Evolutionary Biology, Wildlife Biology, Biology, Research, English, Cell Biology",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "You hold a BS Degree in Biology  Biology, Molecular Biology, Biochemistry, Physiology, or a closely related life-science field in one of the top 100 universities. | Confident in human and animal biology, molecular pathways, physiology, disease/health fundamentals, and lab/experimental reasoning. | Expert at fact-checking information using public sources across multiple domains. | You communicate biology clearly and turn advanced theory into real-world solutions. | English proficiency C1 and above with excellent writing skills. | Bonus: Data labeling, RLHF, or AI model evaluation experience.",
    "sample_interview_questions": "State the central dogma of molecular biology and name one exception. | Which cellular organelles are primarily involved in protein synthesis and processing, and how do they interact in a typical eukaryotic cell? | In population genetics, what does Hardy–Weinberg equilibrium assume, and what real-world factors commonly cause deviations from it? | What factors determine whether an amino acid side chain is classified as polar, nonpolar, acidic, or basic, and why does that classification matter for protein structure? | A paper claims that a new treatment significantly increases survival in a mouse model. What aspects of the study design and data would you examine to evaluate the strength of this claim? | If you are given an AI-generated explanation of photosynthesis, what specific checks would you perform to detect subtle inaccuracies or misleading simplifications? | How are human blood glucose levels regulated in the short term, and which hormones and organs are most directly involved? | How would you approach reviewing a long technical biology explanation under time pressure while still catching small factual or numerical errors? | If you observe a strong correlation between two biological variables in a dataset, what additional information or analyses would you seek before inferring causation? | When two reputable sources provide slightly different values or interpretations for a biological concept, how do you decide which one to rely on for your work?",
    "source_url": "https://editor.superannotate.com/jobs/BIOLOGY-25-503"
  },
  {
    "job_id": "BIOLOGY-25-504",
    "title": "Biologist",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$35 - $69 / hour",
    "pay_min": 35,
    "pay_max": 69,
    "currency": "USD",
    "num_people": 70,
    "priority_level": 1,
    "role_description": "We’re seeking biologists to join our AI training project. In this role, you will review AI-generated responses, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check biological information; write high-quality biological explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on biological correctness and reasoning quality.",
    "keywords": "Molecular Biology, Computational Biology, Systems Biology, Molecular & Cellular Biology, Evolutionary Biology, Wildlife Biology, Biology, Research, English, Cell Biology",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "You hold a MS or PhD in Biology, Molecular Biology, Biochemistry, Physiology, or a closely related life-science field.in one of the top 100 universities. | Confident in human and animal biology, molecular pathways, physiology, disease/health fundamentals, and lab/experimental reasoning. | You have developed or critically reviewed complex biology content (e.g., problem banks, proofs, textbook sections, research notes). | Expert at fact-checking information using public sources across multiple domains. | Published in a peer-reviewed journal. | You communicate biology clearly and turn advanced theory into real-world solutions. | English proficiency C1 and above with excellent writing skills. | Bonus: Data labeling, RLHF, or AI model evaluation experience.",
    "sample_interview_questions": "State the central dogma of molecular biology and name one exception. | Which cellular organelles are primarily involved in protein synthesis and processing, and how do they interact in a typical eukaryotic cell? | In population genetics, what does Hardy–Weinberg equilibrium assume, and what real-world factors commonly cause deviations from it? | What factors determine whether an amino acid side chain is classified as polar, nonpolar, acidic, or basic, and why does that classification matter for protein structure? | A paper claims that a new treatment significantly increases survival in a mouse model. What aspects of the study design and data would you examine to evaluate the strength of this claim? | If you are given an AI-generated explanation of photosynthesis, what specific checks would you perform to detect subtle inaccuracies or misleading simplifications? | How are human blood glucose levels regulated in the short term, and which hormones and organs are most directly involved? | How would you approach reviewing a long technical biology explanation under time pressure while still catching small factual or numerical errors? | If you observe a strong correlation between two biological variables in a dataset, what additional information or analyses would you seek before inferring causation? | When two reputable sources provide slightly different values or interpretations for a biological concept, how do you decide which one to rely on for your work?",
    "source_url": "https://editor.superannotate.com/jobs/BIOLOGY-25-504"
  },
  {
    "job_id": "ArabicQ-25-319",
    "title": "Arabic - Data Trainer Lead",
    "company": "SuperAnnotate",
    "status": "closed",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Egypt",
    "pay_rate": "$10 - $33 / hour",
    "pay_min": 10,
    "pay_max": 33,
    "currency": "USD",
    "num_people": 7,
    "priority_level": 3,
    "role_description": "Are you ready to take on a pivotal role in ensuring the highest standards in AI training? We are looking for an Arabic-speaking Quality Assurance Lead to help ensure consistently high standards in our AI data training projects. This role combines the opportunity to work with modern AI systems with the responsibility of maintaining high-quality Arabic content, making it a strong fit for people who are passionate about both technology and precision. You will play a crucial role in guaranteeing the quality and efficiency of our AI training processes, ultimately contributing to the success of our projects.",
    "keywords": "English, Arabic, Writing, Research, Documentation, Detail Oriented, Communication, Leading, Proofreading, Editing",
    "key_responsibilities": "Quality: Conduct regular spot checks on items, provide clear and constructive feedback to trainers, and escalate issues when necessary. | Communication: Keep trainers and QAs updated on Discord about new items or project changes, and respond to their questions in a timely and professional manner. | Activation management: Monitor trainer/QA activity and proactively communicate to inactive contributors to understand blockers and encourage re-engagement. | Documentation: Create, maintain, and improve key documents such as style guides, trackers, FAQs, honeypots, and related documents. | Onboarding & training: Schedule, organize, and run onboarding and training calls with trainers/QAs to ensure they understand the guidelines and expectations.",
    "your_profile": "Academic / professional background: Bachelor’s or Master’s degree in Linguistics, Communication, Journalism or related field. | Arabic mastery: Exceptional written Arabic with excellent grammar, syntax, and stylistic flexibility (formal, neutral, conversational). | LLM training experience: Hands-on experience with LLM projects preferably in QA or lead roles | English proficiency: minimum C1 English for documentation, async communication, and collaboration with a global team. | Analytical & detail-oriented: You notice inconsistencies, edge cases, and guideline gaps, and are comfortable working with detailed instructions. | Leadership mindset: Comfortable working independently, giving feedback, and keeping the trainer/QA community engaged and supported.",
    "sample_interview_questions": "What interests you most about working as an Arabic-speaking Quality Assurance Lead in AI data training, and why do you think you are a strong fit for this role? | When you review a piece of written Arabic, what are the main aspects you focus on first (e.g., grammar, register, clarity, cultural appropriateness), and why? | How do you approach differences between Modern Standard Arabic and dialectal Arabic in content for AI training? When would you accept or reject dialectal usage? | Please describe any previous experience you have with LLM projects, data annotation, or AI training in Arabic. What were your responsibilities and what did you learn from them? | If you notice recurring issues in Arabic outputs from trainers (for example, consistent tense errors or inappropriate register), how would you diagnose the problem and address it with the team? | How would you give feedback to a trainer whose work does not follow the style guide? Please walk through what you would say and how you would say it. | Several Arabic trainers have become inactive. How would you reach out to them, and what steps would you take to understand their blockers and encourage them to return? | Describe how you would structure an Arabic style guide or FAQ for trainers. What sections would you include to make it clear and easy to follow for a distributed team? | If you encounter an Arabic example that is grammatically correct but stylistically borderline or culturally sensitive, and the guidelines do not cover it, how would you decide what to do? | Tell me about a situation where you worked mostly independently while guiding others (for example, reviewers or annotators). How did you keep quality high and communication clear?",
    "source_url": "https://editor.superannotate.com/jobs/ArabicQ-25-319"
  },
  {
    "job_id": "ItalianQ-25-317",
    "title": "Italian - Data Trainer Lead",
    "company": "SuperAnnotate",
    "status": "closed",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Italy",
    "pay_rate": "$25 - $50 / hour",
    "pay_min": 25,
    "pay_max": 50,
    "currency": "USD",
    "num_people": 5,
    "priority_level": 1,
    "role_description": "Are you ready to take on a pivotal role in ensuring the highest standards in AI training? We are looking for an Italian-speaking Quality Assurance Lead to help ensure consistently high standards in our AI data training projects. This role combines the opportunity to work with modern AI systems with the responsibility of maintaining high-quality Italian content, making it a strong fit for people who are passionate about both technology and precision.\n\nYou will play a crucial role in guaranteeing the quality and efficiency of our AI training processes, ultimately contributing to the success of our projects.",
    "keywords": "English, Italian, Writing, Research, Documentation, Detail Oriented, Communication, Leading, Proofreading, Editing",
    "key_responsibilities": "Quality: Conduct regular spot checks on items, provide clear and constructive feedback to trainers, and escalate issues when necessary. | Communication: Keep trainers and QAs updated on Discord about new items or project changes, and respond to their questions in a timely and professional manner. | Activation management: Monitor trainer/QA activity and proactively reach out to inactive contributors to understand blockers and encourage re-engagement. | Documentation: Create, maintain, and improve key documents such as style guides, trackers, FAQs, honeypots, and related documents. | Onboarding & training: Schedule, organize, and run onboarding and training calls with trainers/QAs to ensure they understand the guidelines and expectations.",
    "your_profile": "Academic / professional background: Bachelor’s or Master’s degree in Linguistics, Communication, Journalism or related field (strong writers/editors from other backgrounds also welcome). | Italian mastery: Exceptional written Italian with excellent grammar, syntax, and stylistic flexibility (formal, neutral, conversational). | LLM training experience: Hands-on experience with LLM projects, preferably in QA or lead roles. | English proficiency: Minimum C1 English for documentation, async communication, and collaboration with a global team. | Analytical & detail-oriented: You notice inconsistencies, edge cases, and guideline gaps, and are comfortable working with detailed instructions. | Leadership mindset: Comfortable working independently, giving feedback, and keeping the trainer/QA community engaged and supported.",
    "sample_interview_questions": "What motivates you to work as an Italian-speaking Quality Assurance Lead in AI data training, and why does this role appeal to you specifically? | When reviewing Italian text, what are the most common issues you look for (e.g., register, agreement, punctuation, idiomatic usage), and how do you prioritize them? | Describe any experience you have with LLM projects, data annotation, or AI training involving Italian. What tasks did you handle and what challenges did you face? | If you see repeated mistakes from Italian trainers (for example, misuse of formal pronouns or anglicisms), how would you address this pattern with them and with project documentation? | Give an example of how you would phrase constructive feedback in English to an Italian trainer whose work does not fully meet the guidelines, while keeping the relationship positive. | How would you handle a situation where a previously active Italian trainer stops participating? What messages or steps would you use to bring them back into the project? | If you had to create or refine an Italian style guide for trainers, what key rules or examples would you definitely include to ensure consistency and high quality? | In Italian, sometimes more than one phrasing is correct. If the guidelines do not clearly prefer one, how do you make a decision and how do you explain it to the team? | Describe a time when you took ownership of quality for Italian content or a similar domain. How did you organize the work, support others, and measure improvement? | Sometimes the task requirements for are incomplete or ambiguous. Can you describe a time you dealt with unclear requirements and how you clarified them or made reasonable review decisions?",
    "source_url": "https://editor.superannotate.com/jobs/ItalianQ-25-317"
  },
  {
    "job_id": "FrenchQ-25-301",
    "title": "French - Data Trainer Lead",
    "company": "SuperAnnotate",
    "status": "closed",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "France",
    "pay_rate": "$25 - $50 / hour",
    "pay_min": 25,
    "pay_max": 50,
    "currency": "USD",
    "num_people": 4,
    "priority_level": 1,
    "role_description": "Are you ready to take on a pivotal role in ensuring the highest standards in AI training? We are looking for a French-speaking Quality Assurance Lead to help ensure consistently high standards in our AI data training projects. This role combines the opportunity to work with modern AI systems with the responsibility of maintaining high-quality French content, making it a strong fit for people who are passionate about both technology and precision.\n\nYou will play a crucial role in guaranteeing the quality and efficiency of our AI training processes, ultimately contributing to the success of our projects.",
    "keywords": "English, French, Writing, Research, Documentation, Detail Oriented, Communication, Leading, Proofreading, Editing",
    "key_responsibilities": "Quality: Conduct regular spot checks on items, provide clear and constructive feedback to trainers, and escalate issues when necessary. | Communication: Keep trainers and QAs updated on Discord about new items or project changes, and respond to their questions in a timely and professional manner. | Activation management: Monitor trainer/QA activity and proactively reach out to inactive contributors to understand blockers and encourage re-engagement. | Documentation: Create, maintain, and improve key documents such as style guides, trackers, FAQs, honeypots, and related documents. | Onboarding & training: Schedule, organize, and run onboarding and training calls with trainers/QAs to ensure they understand the guidelines and expectations.",
    "your_profile": "Academic / professional background: Bachelor’s or Master’s degree in Linguistics, Communication, Journalism or related field. | French mastery: Exceptional written French with excellent grammar, syntax, and stylistic flexibility (formal, neutral, conversational). | LLM training experience: Hands-on experience with LLM projects, preferably in QA or lead roles. | English proficiency: Minimum C1 English for documentation, async communication, and collaboration with a global team. | Analytical & detail-oriented: You notice inconsistencies, edge cases, and guideline gaps, and are comfortable working with detailed instructions. | Leadership mindset: Comfortable working independently, giving feedback, and keeping the trainer/QA community engaged and supported.",
    "sample_interview_questions": "Why are you interested in working as a French-speaking Quality Assurance Lead in AI data training, and how does your background prepare you for this role? | When you review French content, what do you focus on first (e.g., agreement, register, spelling variants, clarity), and why? | How do you handle differences in French usage across regions (e.g., France, Canada, other francophone countries) when reviewing content for AI training? | Tell me about any experience you have with LLMs, data annotation, or AI training involving French. What were your main tasks and what did you learn from them? | If you repeatedly see issues such as anglicisms, incorrect gender/number agreement, or inconsistent register in French outputs, how would you address these with trainers and in documentation? | How would you deliver clear, constructive feedback in English to a French trainer whose work does not align with the style guide? Please describe your approach in detail. | What would you do if several French trainers become inactive at the same time? How would you prioritize outreach and what would you ask or propose to them? | If tasked with creating or updating a French style guide or FAQ, how would you structure it to help trainers quickly understand the expectations and common pitfalls? | When you face a French sentence that is technically correct but stylistically weak or potentially confusing, and the guidelines are not explicit, how do you decide whether to accept or request changes? | Describe a situation where you led or supported a team working with French content, largely independently. How did you balance your own tasks with mentoring or guiding others?",
    "source_url": "https://editor.superannotate.com/jobs/FrenchQ-25-301"
  },
  {
    "job_id": "PYTHONQ-25-601",
    "title": "Python AI Data Training QA Lead",
    "company": "SuperAnnotate",
    "status": "closed",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$50 - $100 / hour",
    "pay_min": 50,
    "pay_max": 100,
    "currency": "USD",
    "num_people": 17,
    "priority_level": 2,
    "role_description": "Are you ready to take on a pivotal role in ensuring the highest standards in AI training? We are looking for a Python Quality Assurance Lead to help ensure consistently high standards in our AI data training projects. This role combines the opportunity to work with modern AI systems with the responsibility of maintaining high-quality Python content, making it a strong fit for people who are passionate about both technology and precision.\n\nYou will play a crucial role in guaranteeing the quality and efficiency of our AI training processes, ultimately contributing to the success of our projects.",
    "keywords": "Python, Django, Coding, English, RESTful API, GraphQL, Flask, Research, Writing, Code Review",
    "key_responsibilities": "Quality: Conduct regular spot checks on Python tasks and code submissions, provide clear and constructive feedback to trainers, and escalate issues when necessary. | Communication: Keep trainers and QAs updated on Discord about new items or project changes, and respond to their questions in a timely and professional manner. | Activation management: Monitor trainer/QA activity and proactively communicate with inactive contributors to understand blockers and encourage re-engagement. | Documentation: Create, maintain, and improve key documents such as coding guidelines, style guides, trackers, FAQs, honeypots, and related documents. | Onboarding & training: Schedule, organize, and run onboarding and training calls with trainers/QAs to ensure they understand the guidelines, expectations, and Python quality standards.",
    "your_profile": "Academic / professional background: Bachelor’s or Master’s degree in Computer Science, Software Engineering, or a related field. | Python expertise: At least 7+ years of experience in Python, including real-world projects or production systems. | Frameworks & APIs: Proficiency with popular Python frameworks such as Django, Flask, or FastAPI, and experience with RESTful API and GraphQL design and implementation. | LLM training experience: Hands-on experience with LLM projects, data annotation, or AI training, preferably in QA or lead roles. | English proficiency: Minimum C1 English for documentation, async communication, and collaboration with a global team. | Analytical excellence: Strong analytical skills and the ability to review complex code and tasks systematically. | Attention to detail: Commitment to precision and thoroughness in the training and review process; ability to produce high-quality work with no supervision. | Leadership mindset: Comfortable working independently, giving feedback, and keeping the trainer/QA community engaged and supported.",
    "sample_interview_questions": "What interests you most about working as a Python Quality Assurance Lead in AI data training, and why do you think you are a strong fit for this role? | Imagine you are reviewing a Python solution from a trainer for an AI training task. How do you structure your review? What do you check first, and what are the most important aspects you focus on? | If you notice recurring problems in trainers’ Python submissions (for example, poor error handling, lack of tests, or inefficient code), how would you analyze the issue and what steps would you take to improve overall quality? | Please describe any previous experience you have with large language models, data annotation, or AI training that involved Python. What were your responsibilities, and what did you learn that is relevant for a QA Lead role? | How would you give clear and constructive written feedback in English to a trainer whose Python code does not meet the guidelines? Please outline what you would write and your tone of communication. | You notice that some Python trainers have become inactive or are not following best practices. How would you approach re-engaging them and guiding them towards the expected quality standards? | Sometimes the task requirements for are incomplete or ambiguous. Can you describe a time you dealt with unclear requirements and how you clarified them or made reasonable review decisions? | How would you handle a trainer whose submissions are consistently below the expected standard? What steps would you take to help them improve while still protecting overall quality? | If tasked with creating or updating a style guide, how would you structure it to help trainers quickly understand the expectations and common issues? | What would you do if several trainers become inactive at the same time? How would you prioritize outreach and what would you ask or propose to them?",
    "source_url": "https://editor.superannotate.com/jobs/PYTHONQ-25-601"
  },
  {
    "job_id": "JavaQ-25-116",
    "title": "Java AI Data Training Tech Quality Lead",
    "company": "SuperAnnotate",
    "status": "closed",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$50 - $100 / hour",
    "pay_min": 50,
    "pay_max": 100,
    "currency": "USD",
    "num_people": 19,
    "priority_level": 2,
    "role_description": "Are you ready to take on a pivotal role in ensuring the highest standards in AI training? We are looking for a Java Tech Quality Assurance Lead to help ensure consistently high standards in our AI data training projects. This role combines the opportunity to work with modern AI systems with the responsibility of maintaining high-quality Java content, making it a strong fit for people who are passionate about both technology and precision.\n\nYou will play a crucial role in guaranteeing the quality and efficiency of our AI training processes, ultimately contributing to the success of our projects.",
    "keywords": "Java, Technical Lead, Coding, English, Code Review, Research, Writing, Java Developer, Software Engineer, Code Refactoring",
    "key_responsibilities": "Code quality review: Conduct regular reviews of Java solutions and tasks submitted by trainers, checking for correctness, clarity, code style, structure, and maintainability; provide clear and constructive feedback, and escalate critical issues when necessary. | Technical evaluation: Assess Java code for algorithmic soundness, performance implications, and best practices (error handling, modularity, readability), and ensure alignment with project guidelines and standards. | Communication: Keep trainers and QAs updated on Discord about new items, review expectations, or project changes, and respond to their questions in a timely and professional manner. | Activation management: Monitor trainer/QA activity and proactively communicate with inactive contributors to understand blockers and encourage re-engagement. | Documentation: Create, maintain, and improve key documents such as Java coding guidelines, review checklists, example solutions, trackers, FAQs, honeypots, and related documents. | Onboarding & training: Schedule, organize, and run onboarding and training calls with trainers/QAs to walk them through Java quality standards, common issues, and review expectations.",
    "your_profile": "Academic / professional background: Bachelor’s or Master’s degree in Computer Science, Software Engineering, or a related field. | Java expertise: At least 7+ years of professional experience in Java, with a focus on code quality, code review, or technical evaluations. | Technical expertise: Strong expertise in Java programming, debugging, understanding different design and implementation approaches, and assessing code for robustness, readability, and maintainability. | LLM training experience: Hands-on experience with LLM projects, data annotation, or AI training, preferably in QA or lead roles. | English proficiency: Minimum C1 English for documentation, async communication, and collaboration with a global team. | Analytical & problem-solving skills: Exceptional analytical and problem-solving skills, with the ability to identify root causes and evaluate different solution options. | Attention to detail: Commitment to precision and thoroughness in the training and review process; ability to produce high-quality work with no supervision. | Leadership mindset: Comfortable working independently, giving feedback, and keeping the trainer/QA community engaged and supported.",
    "sample_interview_questions": "What interests you most about working as a Java Tech Quality Assurance Lead in AI data training, and why do you think you are a strong fit for this role? | When you review a Java code submission, what is your step-by-step approach? What do you look at first, and what criteria do you use to judge quality? | How do you ensure consistency in your reviews across many different Java submissions and contributors, especially when code style or approaches vary? | In AI training, task descriptions and requirements may be incomplete or ambiguous. Can you describe a situation where requirements were unclear, and how you clarified them or made a fair decision in your review? | This role relies heavily on written feedback and communication in English. How do you balance being precise and critical with staying respectful and supportive in your written comments? Please provide an example. | How would you handle a trainer whose Java submissions are consistently below the expected standard? What steps would you take to help them improve while still protecting overall quality? | If you notice that some trainers are inactive or repeatedly ignoring guidelines, how would you approach re-engaging them and guiding them toward the expected quality standards? | Please describe any previous experience you have with LLM projects, data annotation, or AI training. What were your responsibilities and what did you learn from them?",
    "source_url": "https://editor.superannotate.com/jobs/JavaQ-25-116"
  },
  {
    "job_id": "MLQ-25-109",
    "title": "ML Technical Quality Assurance Lead",
    "company": "SuperAnnotate",
    "status": "closed",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$60 - $120 / hour",
    "pay_min": 60,
    "pay_max": 120,
    "currency": "USD",
    "num_people": 19,
    "priority_level": 2,
    "role_description": "re you ready to take on a pivotal role in ensuring the highest standards in AI training? We are looking for a MLTechnical Quality Assurance Lead to help ensure consistently high standards in our AI data training projects. This role combines the opportunity to work with modern machine learning systems with the responsibility of maintaining high-quality Python and ML content, making it a strong fit for people who are passionate about both technology and precision.\n\nYou will play a crucial role in guaranteeing the technical quality and consistency of ML-related training data, code, and evaluations, ultimately contributing to the success of our projects.",
    "keywords": "ML, Machine Learning, English, Code Review, Technical Lead, Data Science, NumPy, Pandas, Scikit-learn, Data Training",
    "key_responsibilities": "Technical quality review: Review Python/ML tasks and code submissions (scripts, notebooks, experiments) for correctness, clarity, model design, and alignment with project guidelines; provide clear and constructive feedback and escalate critical issues when needed. | Communication: Keep trainers and QAs updated on Discord about new items, ML-related clarifications, or project changes, and respond to their technical and process questions in a timely and professional manner. | Activation management: Monitor trainer/QA activity and proactively communicate with inactive contributors to understand blockers and encourage re-engagement. | Documentation: Create, maintain, and improve technical documentation such as ML coding guidelines, experiment checklists, best-practice examples, trackers, FAQs, honeypots, and related documents. | Onboarding & training: Schedule, organize, and run onboarding and training calls with trainers/QAs to walk them through ML quality standards, common issues, and review expectations.",
    "your_profile": "Academic / professional background: Bachelor’s or Master’s degree in Computer Science, Machine Learning, Data Science, Statistics, or a related field | Python & ML experience: Solid professional experience with Python for machine learning, including end-to-end workflows (data preparation, model training, evaluation, and deployment or experimentation). | ML stack: Strong hands-on experience with NumPy, pandas, scikit-learn, and at least one deep learning framework such as PyTorch or TensorFlow. | Analytical & problem-solving skills: Exceptional ability to understand and evaluate ML code, experiments, and metrics, quickly identify issues, and propose clear improvements. | Written communication: Excellent written communication skills in English, able to explain technical feedback, reasoning, and recommendations clearly to a distributed team. | Managing ambiguity: Comfortable working with incomplete or evolving ML specifications and able to clarify complex technical requirements effectively. | Leadership mindset: Comfortable working independently, giving feedback, and keeping the trainer/QA community engaged, aligned with standards, and supported.",
    "sample_interview_questions": "What interests you most about working as a ML Technical Quality Assurance Lead in AI data training, and why do you think you are a strong fit for this role? | When you review a Python/ML code submission, what is your step-by-step approach? What do you look at first, and what criteria do you use to judge quality? | In AI training projects, task descriptions and model requirements can be incomplete or ambiguous. Can you describe a time when ML requirements were unclear, and how you clarified them or made reasonable decisions? | How do you ensure consistency in your technical reviews when you are evaluating many different ML submissions from different contributors? What practices or tools help you maintain a stable quality bar? | You notice that some trainers are either inactive or repeatedly ignoring best practices. How would you approach re-engaging them and guiding them toward the expected technical and quality standards? | How would you handle a trainer whose submissions are consistently below the expected standard? What steps would you take to help them improve while still protecting overall quality? | What would you do if several trainers become inactive at the same time? How would you prioritize outreach and what would you ask or propose to them? | If tasked with creating or updating a style guide, how would you structure it to help trainers quickly understand the expectations and common issues? | Please describe any previous experience you have with LLM projects, data annotation, or AI training. What were your responsibilities and what did you learn from them? | If tasked with creating or updating a style guide, how would you structure it to help trainers quickly understand the expectations and common issues?",
    "source_url": "https://editor.superannotate.com/jobs/MLQ-25-109"
  },
  {
    "job_id": "HTMLCSSQ-25-116",
    "title": "HTML/CSS Data Training Technical QA Lead",
    "company": "SuperAnnotate",
    "status": "closed",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$40 - $80 / hour",
    "pay_min": 40,
    "pay_max": 80,
    "currency": "USD",
    "num_people": 20,
    "priority_level": 2,
    "role_description": "Are you ready to take on a pivotal role in ensuring the highest standards in AI training? We are looking for an HTML/CSS Technical Quality Assurance Lead to help ensure consistently high standards in our front-end–focused AI data training projects. This role combines the opportunity to work with modern web technologies with the responsibility of maintaining high-quality HTML/CSS content, making it a strong fit for people who are passionate about both technology and precision.\n\nYou will play a crucial role in guaranteeing the quality and consistency of front-end code and web-related training data, ultimately contributing to the success of our projects.",
    "keywords": "HTML/CSS, Javascript, Coding, English, JS, Research, Writing, Code Review, CSS, HTML",
    "key_responsibilities": "Code quality review: Review HTML/CSS tasks and code submissions for correctness, semantic markup, responsiveness, readability, and alignment with project guidelines; provide clear and constructive feedback and escalate critical issues when needed. | Front-end evaluation: Assess layout structure, CSS organization, component structure, and basic accessibility and performance considerations to ensure high-quality, realistic front-end examples. | Communication: Keep trainers and QAs updated on Discord about new items, clarifications, or project changes, and respond to their questions in a timely and professional manner. | Activation management: Monitor trainer/QA activity and proactively communicate with inactive contributors to understand blockers and encourage re-engagement. | Documentation: Create, maintain, and improve documentation such as HTML/CSS style guides, component examples, review checklists, trackers, FAQs, honeypots, and related documents. | Onboarding & training: Schedule, organize, and run onboarding and training calls with trainers/QAs to walk them through front-end quality standards, common issues, and review expectations.",
    "your_profile": "Academic / professional background: Bachelor’s or Master’s degree in Computer Science, Web Development, or a related field. | Front-end experience: At least 5+ years of experience in HTML/CSS, building and maintaining real-world user interfaces or web pages. | JavaScript & frameworks: Experience with JavaScript, CSS, and front-end frameworks/libraries such as React, Vue, or Angular. | Front-end best practices: Strong understanding of semantic HTML, responsive design, CSS structuring, and basic accessibility and performance considerations. | English proficiency: Minimum C1 English for documentation, async communication, and collaboration with a global team. | Analytical excellence: Strong analytical skills and the ability to review complex code and tasks systematically. | Attention to detail: Commitment to precision and thoroughness in the training and review process; ability to produce high-quality work with no supervision. | Leadership mindset: Comfortable working independently, giving feedback, and keeping the trainer/QA community engaged and supported.",
    "sample_interview_questions": "What interests you most about working as an HTML/CSS Technical Quality Assurance Lead in AI data training, and why do you think you are a strong fit for this role? | Can you describe the types of front-end projects you’ve worked on and which areas of HTML/CSS you feel most confident in (e.g., layout, semantics, responsive design)? | When you review an HTML/CSS submission from a trainer, what is your step-by-step approach? What do you look at first, and what criteria do you use to evaluate quality? | Sometimes design or task requirements for front-end examples are incomplete or ambiguous. Can you describe a time you dealt with unclear requirements and how you clarified them or made reasonable review decisions? | This role requires giving written feedback in English to contributors. How would you structure feedback for a trainer whose HTML/CSS works but is overly complex and hard to maintain? Please describe your tone and main points. | If you notice that some front-end trainers are inactive or repeatedly ignoring guidelines, how would you approach re-engaging them and guiding them toward the expected quality standards? | Please describe any previous experience you have with LLM projects, data annotation, or AI training. What were your responsibilities and what did you learn from them? | Sometimes the task requirements for are incomplete or ambiguous. Can you describe a time you dealt with unclear requirements and how you clarified them or made reasonable review decisions? | If tasked with creating or updating a style guide, how would you structure it to help trainers quickly understand the expectations and common issues? | What would you do if several trainers become inactive at the same time? How would you prioritize outreach and what would you ask or propose to them?",
    "source_url": "https://editor.superannotate.com/jobs/HTMLCSSQ-25-116"
  },
  {
    "job_id": "ArmenianQ-25-356",
    "title": "Armenian - Linguistic Lead",
    "company": "SuperAnnotate",
    "status": "closed",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Armenia",
    "pay_rate": "$10 - $20 / hour",
    "pay_min": 10,
    "pay_max": 20,
    "currency": "USD",
    "num_people": 5,
    "priority_level": 0,
    "role_description": "Are you ready to take on a pivotal role in ensuring the highest standards in AI training? We are looking for an Armenian-speaking Linguistic Quality Assurance Lead to help ensure consistently high standards in our AI data training projects. This role combines the opportunity to work with modern AI systems with the responsibility of maintaining high-quality Armenian content, making it a strong fit for people who are passionate about both technology and precision.\n\nYou will play a crucial role in guaranteeing the quality and efficiency of our AI training processes, ultimately contributing to the success of our projects.",
    "keywords": "English, Armenian, Writing, Research, Documentation, Philology Communication, Leading, Proofreading, Editing",
    "key_responsibilities": "Quality: Conduct regular spot checks on items, provide clear and constructive feedback to trainers, and escalate issues when necessary. | Communication: Keep trainers and QAs updated on Discord about new items or project changes, and respond to their questions in a timely and professional manner. | Activation management: Monitor trainer/QA activity and proactively reach out to inactive contributors to understand blockers and encourage re-engagement. | Documentation: Create, maintain, and improve key documents such as style guides, trackers, FAQs, honeypots, and related documents. | Onboarding & training: Schedule, organize, and run onboarding and training calls with trainers/QAs to ensure they understand the guidelines and expectations.",
    "your_profile": "Academic / professional background: Bachelor’s or Master’s degree in Linguistics, Communication, Journalism or related field. | Armenian mastery: Exceptional written Armenian with excellent grammar, syntax, and stylistic flexibility (formal, neutral, conversational). | LLM training experience is preferred: Hands-on experience with LLM projects, preferably in QA or lead roles. | English proficiency: Minimum C1 English for documentation, async communication, and collaboration with a global team. | Analytical & detail-oriented: You notice inconsistencies, edge cases, and guideline gaps, and are comfortable working with detailed instructions. | Leadership mindset: Comfortable working independently, giving feedback, and keeping the trainer/QA community engaged and supported.",
    "sample_interview_questions": "Why are you interested in working as an Armenian-speaking Quality Assurance Lead in AI data training, and how does your background prepare you for this role? | When you review Armenian content, what do you focus on first (e.g., agreement, register, spelling variants, clarity), and why? | How do you handle differences in Armenian usage across regions when reviewing content for AI training? | Tell me about any experience you have with LLMs, data annotation, or AI training involving French. What were your main tasks and what did you learn from them? | If you repeatedly see issues such as anglicisms, incorrect gender/number agreement, or inconsistent register in French outputs, how would you address these with trainers and in documentation? | How would you deliver clear, constructive feedback to an Armenian trainer whose work does not align with the style guide? Please describe your approach in detail. | What would you do if several Armenian trainers become inactive at the same time? How would you prioritize outreach and what would you ask or propose to them? | If tasked with creating or updating a Armenian style guide or FAQ, how would you structure it to help trainers quickly understand the expectations and common pitfalls? | When you face a Armenian sentence that is technically correct but stylistically weak or potentially confusing, and the guidelines are not explicit, how do you decide whether to accept or request changes? | Describe a situation where you led or supported a team working with Armenian content, largely independently. How did you balance your own tasks with mentoring or guiding others?",
    "source_url": "https://editor.superannotate.com/jobs/ArmenianQ-25-356"
  },
  {
    "job_id": "Python-25-656",
    "title": "Senior Python Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $48 / hour",
    "pay_min": -1,
    "pay_max": 48,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 2,
    "role_description": "As a Senior Python Engineer, you will work remotely on an hourly paid basis to review AI-generated Python solutions and technical explanations, as well as generate high-quality reference content that demonstrates strong software engineering practices. You will assess solutions for correctness, code quality, performance, and adherence to the prompt; identify errors in logic, architecture, or methodology; fact-check technical information; craft clear, step-by-step explanations and exemplary solutions; and rate and compare multiple AI responses based on their reasoning quality and implementation soundness. \n\nThis is a fully remote, hourly paid contractor role with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs, where your Python expertise will directly help improve the world’s premier AI models.",
    "keywords": "Python, Backend Engineering, API, Data Structures, Algorithms, Unit Testing, Code Review, Distributed Systems, English, Research",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Software Engineering, Mathematics, or a closely related technical field. | 7+ years of professional software engineering experience, with significant hands-on work in Python on production systems. | Deep knowledge of Python language features, the standard library, and common ecosystems such as web frameworks, data tooling, or backend services. | Strong understanding of algorithms, data structures, system design, and performance optimization in Python applications. | Proven experience designing, reviewing, and improving high-quality Python codebases, including code review and mentoring responsibilities. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Comfort working with modern tooling such as version control systems, CI/CD pipelines, testing frameworks, and containerization technologies. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented, with a structured and methodical approach to evaluating reasoning quality and identifying subtle errors in technical solutions.",
    "sample_interview_questions": "Why do mutable default arguments persist across function calls in Python, and how are default values stored internally? | What is the difference between object identity and equality in Python, and what bugs can result from confusing is and ==? | How does Python’s reference counting work, and why can it fail to reclaim memory in certain cases? | Why can defining a __del__ method lead to unpredictable behavior or memory leaks? | What happens internally when a mutable object is passed to a function and modified versus when it is rebound? | What is late binding in Python closures, and why does it often cause bugs in loops? | How do decorators work under the hood, and how can they break function signatures or introspection? | What does Python actually mean by “pass by object reference,” and why is this term misleading? | How do positional-only and keyword-only arguments affect function APIs and backward compatibility? | How does exception propagation interact with try, except, finally, and return statements? | What is exception masking, and why is catching Exception broadly considered dangerous? | How do context managers work at the protocol level, and how does __exit__ control exception handling? | Why can reusing iterators cause subtle bugs, and how do generators differ in behavior? | How does Python determine truthiness for custom objects, and what risks come with overriding __bool__? | How does Python’s import system resolve modules, and why can circular imports sometimes succeed and sometimes fail? | Why can import-time side effects cause non-deterministic behavior in large Python applications? | Why does the Global Interpreter Lock exist, and under what workloads does it become a bottleneck? | How does Python manage stack frames and recursion limits, and why is tail-call optimization not implemented? | What optimizations does CPython perform at the bytecode level, and which ones does it intentionally avoid? | Under what conditions can both shallow and deep copies still result in shared mutable state?",
    "source_url": "https://editor.superannotate.com/jobs/Python-25-656"
  },
  {
    "job_id": "DataScientist-25-381",
    "title": "Senior Data Scientist",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $98 / hour",
    "pay_min": -1,
    "pay_max": 98,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 2,
    "role_description": "As a Senior Data Scientist, you will work remotely on an hourly paid basis to review AI-generated analytical reasoning, code, and model outputs, as well as generate high-quality reference solutions and explanations for complex data problems. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology, modeling choices, or statistical reasoning; fact-check quantitative claims; write clear, step-by-step explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis is a fully remote, hourly paid contractor role with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs. Your data science expertise will directly contribute to improving the world’s premier AI models used across products and industries.",
    "keywords": "Data science, Machine learning, Python, Statistics, SQL, Scikit-learn, Model evaluation, NumPy, English, Research",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Data Science, Computer Science, Statistics, Mathematics, or a closely related quantitative field. | 5+ years of professional experience as a Data Scientist or in a closely related analytical role, working on end-to-end machine learning projects. | Strong proficiency in Python for data analysis and machine learning, including libraries such as pandas, NumPy, scikit-learn, and related tooling. | Solid background in statistics, experimental design, and applied probability, with experience designing and analyzing controlled experiments. | Hands-on experience building, evaluating, and deploying machine learning models in real-world business or product environments. | Advanced SQL skills and comfort working with large, complex datasets from data warehouses or data lakes. | Minimum C1 English proficiency (written and spoken), with the ability to write clear quantitative explanations and follow detailed English-language guidelines. | Experience with data visualization tools or dashboards and the ability to present analytical findings to stakeholders. | Previous experience with AI data training, annotation, or reviewing AI-generated analytical content is a strong plus. | Highly detail-oriented and systematic, with a strong focus on quality, reproducibility, and careful evaluation of reasoning steps.",
    "sample_interview_questions": "How do you typically frame a business question into a data science problem, and what information do you need at the outset? | Can you explain the bias–variance tradeoff and how it influences your model selection and tuning decisions? | When building a binary classification model with severe class imbalance, which techniques would you consider and why? | How do you decide between using a simpler model, such as logistic regression, and a more complex model, such as gradient boosting, for a production use case? | Walk through your typical end-to-end workflow for a supervised machine learning project, from data acquisition to deployment and monitoring. | How do you handle missing data and outliers in a dataset, and what factors guide your choice of technique? | Describe how you would design and interpret an A/B test for a new product feature, including key metrics and common pitfalls. | What approaches do you use to detect data leakage during model development, and why is it harmful? | How do you evaluate model performance beyond a single metric, and when might you favor precision, recall, F1, or ROC-AUC? | Can you explain L1 and L2 regularization and how they impact model coefficients and feature selection? | How do you ensure that your models remain robust over time in the presence of concept drift or changing data distributions? | Describe a time you had to explain a complex model or analysis to non-technical stakeholders; how did you adapt your communication? | What is your approach to feature engineering for tabular business data, and how do you avoid overfitting in this step? | How do you use SQL in your day-to-day work as a data scientist, and what kinds of queries do you most frequently write? | When working with large datasets that do not fit in memory, what tools or techniques do you use to scale your analysis? | How do you incorporate uncertainty estimation or confidence intervals into your analyses or model outputs? | What steps do you take to validate that your data sources are trustworthy and that your training labels are of high quality? | How would you compare and choose between several competing models that all show similar validation performance? | Describe how you would design a logging and monitoring strategy to track a model’s performance after deployment? | How do you prioritize and manage your time when working on multiple concurrent data science projects with competing deadlines?",
    "source_url": "https://editor.superannotate.com/jobs/DataScientist-25-381"
  },
  {
    "job_id": "Java-25-742",
    "title": "Senior Java Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 2,
    "role_description": "As a Senior Java Engineer, you will work remotely on an hourly paid basis to review AI-generated Java code, architectural solutions, and technical explanations, as well as generate high-quality reference implementations and reasoning steps. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in logic, performance, or design; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs, where your Java expertise will directly help improve the world’s premier AI models.",
    "keywords": "Java, Spring Boot, Microservices, Distributed systems, Backend Engineering, Code Review, Unit Testing, Performance Optimization, English, Remote contractor",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Software Engineering, or a closely related technical field. | 5+ years of professional software engineering experience, with significant hands-on work in Java on production systems. | Deep understanding of core Java,  concurrency primitives, and the Java Collections Framework. | Extensive experience building and maintaining backend services using Spring or Spring Boot, REST APIs, and microservice architectures. | Strong grasp of software design principles, system design, and performance optimization in distributed Java systems. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Comfort working with modern engineering tooling such as Git, CI/CD pipelines, automated testing frameworks, and containerization technologies. | Demonstrated experience conducting thorough code reviews and providing structured, constructive feedback to other engineers. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in Java solutions.",
    "sample_interview_questions": "How would you design the architecture of a high-throughput Java microservice that needs to remain maintainable over several years? | Can you explain the differences between the main collections in the Java Collections Framework and when you would choose one over another in practice? | How does the Java memory model influence the way you write multithreaded code, particularly around visibility and ordering of operations? | Describe how the JVM manages memory, including the heap, stack, and garbage collection, and how this affects application tuning. | What are common causes of thread contention and deadlocks in Java applications, and how would you detect and resolve them? | How do you use Spring Boot to build and configure RESTful services, and what patterns do you follow for error handling and validation? | When diagnosing a production performance issue in a Java service, what tools and step-by-step approach do you typically use? | How do you design and structure unit, integration, and end-to-end tests for a Java-based backend system? | What strategies do you use to handle configuration management and secrets securely in Java applications across multiple environments? | Explain how you would design an API contract between Java microservices to minimize coupling and ease future changes. | How do you approach schema versioning and data migrations when a Java service relies on a relational database? | What are the trade-offs between using blocking I/O (e.g., traditional servlet model) and non-blocking or reactive approaches in Java? | How do you review another engineer’s Java code for readability, correctness, and long-term maintainability? | Describe how you would structure exception handling in a layered Java application to keep error propagation clear and consistent. | What techniques do you use to ensure backward compatibility when evolving public Java APIs or library code? | How do you manage dependencies and modularization in large Java projects, and what build tools and conventions do you prefer? | Explain how you would set up logging, metrics, and tracing in a Java service to support effective observability and debugging. | How do you assess the security posture of a Java web application, and what common vulnerabilities do you specifically check for? | When comparing two alternative Java implementations of the same feature, what concrete criteria do you use to decide which one to recommend? | Describe a complex Java-based system you have worked on, focusing on the design decisions you made and how you validated they were sound.",
    "source_url": "https://editor.superannotate.com/jobs/Java-25-742"
  },
  {
    "job_id": "SQL-25-248",
    "title": "Senior SQL Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $48 / hour",
    "pay_min": -1,
    "pay_max": 48,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 2,
    "role_description": "As a Senior SQL Engineer, you will work remotely on an hourly paid basis to review AI-generated SQL queries, database designs, and data-processing logic, as well as generate high-quality reference solutions and explanations for complex data problems. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology, query structure, indexing strategy, or conceptual understanding; fact-check data-related claims; write clear, step-by-step explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness, performance, and reasoning quality. \n\nThis is a fully remote, hourly paid contractor role with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs. Your SQL and database engineering expertise will directly contribute to improving the world’s premier AI models used across industries.",
    "keywords": "SQL, Query Optimization, Database Performance Tuning, Data warehousing, ETL pipelines, Relational databases, Data modeling, Cloud data platforms, English, Remote contractor",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Information Systems, Engineering, Mathematics, or a closely related technical field. | 7+ years of professional experience working with SQL in production environments as a Database Engineer, SQL Developer, Data Engineer, or similar role. | Expert-level proficiency in SQL, including complex joins, window functions, aggregation strategies, and query plan analysis. | Demonstrated experience with database performance tuning, indexing strategies, and troubleshooting concurrency or locking issues. | Hands-on experience designing relational data models, schemas, and data warehouse structures (star and snowflake schemas). | Strong experience with ETL/ELT processes and tooling, as well as working with large-scale data in modern cloud or on-prem data platforms. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Comfort with version control, CI/CD practices, and collaborative workflows for managing database changes. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle data or query issues.",
    "sample_interview_questions": "How do you approach analyzing and improving the performance of a slow-running SQL query without immediately changing indexes? | Can you explain database normalization up to at least third normal form and describe when you might intentionally denormalize a schema? | What factors do you consider when designing clustered and non-clustered indexes for a heavily used transactional table? | How would you investigate and resolve blocking and deadlocking issues in a high-concurrency OLTP database? | Describe how you would design a partitioning strategy for a very large fact table in a data warehouse. | What are the trade-offs between using stored procedures, views, and application-side logic for implementing business rules? | How do different transaction isolation levels affect concurrency and data consistency, and when would you choose each level? | Explain your process for designing a star schema versus a snowflake schema and when each is appropriate. | How do you ensure that ETL or ELT pipelines are reliable, idempotent, and easy to debug when issues arise? | What techniques do you use to detect and handle slowly changing dimensions in a warehouse environment? | How do you validate that query plans remain optimal after schema or index changes, and what tools do you rely on? | Describe how you would design a backup, restore, and disaster recovery strategy for a mission-critical production database. | What is your approach to controlling data access and permissions in SQL systems to balance security with developer productivity? | How do you review another engineer’s SQL code or data model for correctness, clarity, and scalability? | What are common anti-patterns you have seen in SQL schema design or query writing, and how do you address them? | How would you design a logging and monitoring setup to track database performance, resource usage, and query health over time? | Describe how you would structure a migration plan for a major schema change with minimal downtime and risk. | How do you work with product and analytics teams to translate reporting or data needs into robust SQL-based solutions? | When comparing two alternative SQL-based designs for the same requirement, what concrete criteria do you use to decide which is better? | Tell me about a complex SQL or database engineering project you led, and how you ensured the technical solution was well-reasoned and reliable.",
    "source_url": "https://editor.superannotate.com/jobs/SQL-25-248"
  },
  {
    "job_id": "C-25-417",
    "title": "Senior C Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 3,
    "role_description": "As a Senior C Engineer, you will work remotely on an hourly paid basis to review AI-generated C code, low-level systems designs, and technical explanations, as well as generate high-quality reference implementations and step-by-step reasoning for complex engineering problems. You will assess solutions for accuracy, clarity, safety, and adherence to the prompt; identify errors in logic, memory handling, concurrency, or performance; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality.\n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs. Your systems-level C expertise will directly help improve the world’s premier AI models used by millions of developers worldwide.",
    "keywords": "C programming, C, Systems programming, Embedded development, Memory management, Concurrency, Debugging, Performance optimization, Code review, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Electrical/Computer Engineering, or a closely related technical field. | 4+ years of professional experience developing production software in C for systems, embedded, or performance-critical applications. | Expert-level proficiency in C, including deep understanding of pointers, memory management, undefined behavior, and low-level debugging. | Strong background in systems programming concepts such as concurrency, operating systems, hardware interaction, and performance optimization. | Hands-on experience with build systems, compilers, linkers, and debugging tools commonly used in C development. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Proven experience conducting detailed code reviews and enforcing coding standards for safety and maintainability in C codebases. | Comfort working with version control, CI/CD workflows, and automated testing for C projects. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in low-level code.",
    "sample_interview_questions": "How do you structure and organize a large C codebase so that it remains maintainable and testable over time? | Can you explain how memory allocation works in C, including the differences between stack and heap allocation and common pitfalls? | What techniques do you use to prevent and detect buffer overflows and other memory safety issues in C programs? | How does undefined behavior in C affect the way you write, review, and reason about low-level code? | Describe how you would design and document a clear interface in C using header files and opaque data types. | What is your approach to debugging hard-to-reproduce bugs in a C application running on a target system you cannot easily instrument? | How do you use tools such as static analyzers, sanitizers, or valgrind to improve code quality in C projects? | Explain how you would design a thread-safe C module and what synchronization primitives or patterns you might use. | How do you reason about and measure the performance of a C function that is on the critical path of a system? | What considerations are important when working directly with hardware registers, memory-mapped I/O, or other low-level constructs in C? | How do you manage error handling in C in a way that keeps control flow clear and avoids resource leaks? | Describe how you would design and implement a logging or tracing approach for a C system with tight resource constraints. | What are common portability issues you watch for when writing C code intended to run on multiple platforms or compilers? | How do you approach code review for C projects, and what specific issues do you pay the most attention to? | What strategies do you use to organize unit tests and integration tests for C code, especially when hardware dependencies are involved? | How do you design data structures in C to balance memory footprint, cache behavior, and ease of use? | Explain how the C build toolchain (compiler options, linker scripts, build systems) influences your development and debugging workflow. | How do you communicate complex low-level design decisions to less experienced engineers so that they can follow your reasoning? | When comparing two alternative C implementations of the same subsystem, what concrete criteria do you use to decide which is better? | Tell me about a complex C-based system you have worked on, and how you ensured the overall design and implementation were robust and well-reasoned.",
    "source_url": "https://editor.superannotate.com/jobs/C-25-417"
  },
  {
    "job_id": "Cpp-25-593",
    "title": "Senior C++ Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Bangladesh",
    "pay_rate": "$-1 - $22 / hour",
    "pay_min": -1,
    "pay_max": 22,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 3,
    "role_description": "As a Senior C++ Engineer, you will work remotely on an hourly paid basis to review AI-generated C++ code, systems designs, and technical explanations, as well as generate high-quality reference implementations and step-by-step reasoning for complex engineering problems. You will assess solutions for accuracy, clarity, safety, and adherence to the prompt; identify errors in logic, memory handling, concurrency, or performance; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs, where your C++ and systems expertise will directly help improve the world’s premier AI models used by millions of developers worldwide.",
    "keywords": "C++ programming, Systems programming, Low-latency development, Memory management, Concurrency, Code review, Performance optimization, Debugging, English, Remote contractor",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Electrical/Computer Engineering, or a closely related technical field. | 4+ years of professional experience developing production software in C++ for systems, performance-critical, or large-scale applications. | Expert-level proficiency in modern C++ (C++11 and later), including templates, move semantics, smart pointers, and the STL. | Strong background in systems programming concepts such as concurrency, operating systems, low-level performance optimization, and memory management. | Hands-on experience with C++ build systems, compilers, linkers, debuggers, and profiling or analysis tools. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Proven experience conducting detailed C++ code reviews and enforcing coding standards for safety, performance, and maintainability. | Comfort working with version control, CI/CD workflows, and automated testing frameworks in modern C++ projects. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in complex C++ code.",
    "sample_interview_questions": "How do you structure and organize a large C++ codebase so that it remains modular, testable, and maintainable over time? | Can you explain the differences between stack and heap allocation in C++ and how RAII helps manage resources safely? | What are the main differences between modern C++ (C++11 and later) and earlier standards that you rely on in day-to-day development? | How do move semantics and rvalue references work in C++, and when would you explicitly define move constructors or move assignment operators? | Explain how smart pointers (unique_ptr, shared_ptr, weak_ptr) differ and when you would choose each in a real system. | How do you diagnose and prevent common memory issues such as leaks, dangling pointers, and use-after-free bugs in C++? | What is your approach to designing exception-safe C++ code, and how do you handle error propagation in performance-critical paths? | How do you use templates and generic programming in C++ without sacrificing readability and compile times? | Describe how you would design and implement a thread-safe C++ component and what concurrency primitives or libraries you would use. | What techniques do you use to reason about and optimize the performance of C++ code on the critical path, including cache behavior and allocations? | How do you approach interoperability between C++ and C or other languages, and what pitfalls do you watch out for? | Explain your strategy for organizing unit tests and integration tests for C++ code, especially when hardware or OS-specific behavior is involved. | What are common pitfalls you look for during C++ code reviews, particularly around lifetime management and ownership semantics? | How do you use tools such as sanitizers, profilers, and static analyzers to improve the quality of C++ codebases? | Describe how you would structure the build system and dependency management for a large C++ project using CMake or a similar tool. | What considerations are important when writing cross-platform C++ code intended to run on multiple operating systems and architectures? | How do you document complex C++ APIs or subsystems so that other engineers can understand the design decisions and usage patterns? | When comparing two alternative C++ implementations of the same subsystem, what concrete criteria do you use to decide which one is better? | Describe a challenging C++ performance or correctness issue you have solved and how you systematically narrowed down the root cause? | How would you explain advanced C++ concepts such as SFINAE or template metaprogramming to a less experienced engineer in a clear, step-by-step way?",
    "source_url": "https://editor.superannotate.com/jobs/Cpp-25-593"
  },
  {
    "job_id": "TypeScript-25-652",
    "title": "TypeScript Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $48 / hour",
    "pay_min": -1,
    "pay_max": 48,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 2,
    "role_description": "As a TypeScript Engineer, you will work remotely on an hourly paid basis reviewing AI-generated TypeScript code snippets, design proposals, and technical explanations, as well as generating your own high-quality content and reference solutions. You will evaluate step-by-step reasoning and problem-solving, checking solutions for correctness, clarity, and adherence to the prompt; identify errors in logic, type modeling, or architectural thinking; fact-check technical information; craft clear explanations and model solutions that demonstrate correct approaches; and rate and compare multiple AI responses based on their reasoning quality and implementation soundness. \n\nThis fully remote, hourly contractor position is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs. In this role, your TypeScript expertise directly helps improve the world’s premier AI models used by engineers and developers everywhere.",
    "keywords": "TypeScript, Advanced typing, React, Angular, OOP, Git Code review, Static typing, English, vue.js, Computer Science",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years professional TypeScript experience. | Expertise in TypeScript features, including advanced type manipulation, generics, interfaces, and utility types. | Strong proficiency with a major frontend framework (e.g., React, Angular, or Vue.js) or backend runtime (e.g., Node.js with Express or NestJS). | Solid understanding of object-oriented programming (OOP), functional programming concepts, and common software design patterns. | Experience with Git and standard version control workflows (branching, merging, pull requests). | Familiarity with modern build tools (e.g., Webpack, Vite, Babel). | Minimum Bachelor’s degree in Computer Science or a closely related field. | Minimum C1 English proficiency (written and spoken), with strong ability to follow detailed technical guidelines. | Good to have: Merged PRs in TypeScript-based open-source projects. | Good to have: Competitive programming experience, experience reviewing typed API layers, DTOs, or schema-driven designs, and previous exposure to AI data training or annotation projects.",
    "sample_interview_questions": "How do you explain the benefits of TypeScript over plain JavaScript to a team that is hesitant to adopt static typing? | Can you walk through how TypeScript’s type inference works and when you would choose to add explicit type annotations anyway? | How do you design and organize TypeScript interfaces and types to keep a growing codebase consistent and easy to evolve? | What are some practical use cases for advanced TypeScript features such as conditional types, mapped types, or utility types? | How do generics in TypeScript help you build reusable components or functions, and what pitfalls have you encountered when using them? | Describe how you would structure a medium-sized frontend application in React or another framework using TypeScript to keep concerns separated. | How do you typically model API responses and request payloads in TypeScript, and how do you ensure those types stay in sync with backend changes? | What strategies do you use to gradually introduce TypeScript into an existing JavaScript codebase without blocking delivery? | How do you reason about and enforce strict null checking and optional properties in TypeScript to avoid runtime errors? | What is your approach to organizing and naming TypeScript modules, files, and namespaces in a way that scales as the project grows? | How do you design a TypeScript-based component or service so that its public surface area is stable while internal details can change safely? | What patterns do you follow when handling errors and exceptions in a TypeScript application, both on the frontend and in Node.js backends? | How do you review another engineer’s TypeScript code, and what do you focus on when assessing correctness, readability, and type safety? | Describe how you would set up and maintain a TypeScript configuration (tsconfig) for a monorepo or multi-package project. | How do modern build tools like Webpack or Vite integrate with TypeScript, and what kinds of configuration issues have you had to solve? | What is your approach to writing and organizing tests for TypeScript code, and how do you ensure type definitions and tests reinforce each other? | How do you collaborate with designers or product managers to translate UI or API requirements into well-typed TypeScript models and components? | When working with third-party JavaScript libraries that lack complete type definitions, how do you create or refine .d.ts typings safely? | What concrete criteria do you use to compare two alternative TypeScript implementations of the same feature and decide which one to recommend? | Tell me about a challenging TypeScript-related issue you have solved, and how you reasoned through the problem from symptoms to root cause.",
    "source_url": "https://editor.superannotate.com/jobs/TypeScript-25-652"
  },
  {
    "job_id": "JS-25-112",
    "title": "JavaScript Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $48 / hour",
    "pay_min": -1,
    "pay_max": 48,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 2,
    "role_description": "As an hourly paid, fully remote JavaScript Engineer for AI Data Training, you will review complex AI-generated code and explanations or generate new ones, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt, identify errors in methodology or conceptual understanding, fact-check information, write high-quality explanations and model solutions that demonstrate correct methods, and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs. Your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "JavaScript, ES6, Node.js, React, TypeScript, Async programming, Code review, Frontend development, Backend development, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Minimum 2+ years of professional JavaScript experience in production environments. | Strong understanding of closures, async/await, prototype-based inheritance, and JavaScript event loop behavior, with deep proficiency in modern Vanilla JavaScript (ES6/ES2015+). | Proven ability to evaluate correctness, readability, and performance of JavaScript code, including identifying subtle logic and edge-case issues. | Hands-on work experience in software engineering, preferably in roles involving code review, mentoring, or technical design discussions. | Significant experience using LLMs or AI coding assistants while programming, combined with a critical mindset for validating their output. | Excellent English writing skills, capable of producing clear, concise, and pedagogical technical explanations. | Strong proficiency with at least one major frontend framework (such as React, Angular, or Vue.js) or backend runtime (such as Node.js with Express or NestJS). | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, data annotation, or evaluation of AI-generated technical content is a strong plus; Minimum C1 English proficiency and a highly detail-oriented working style are required. | Preferred: contributions or merged PRs in JavaScript or TypeScript open-source projects, familiarity with bundlers and tooling (Webpack, Vite, Rollup), frontend–backend integration and Node.js runtime behavior, reviewing typed API layers, DTOs, or schema-driven designs, and experience evaluating frontend logic, async patterns, and state management.",
    "sample_interview_questions": "Can you explain how the JavaScript event loop works, including the roles of the call stack, callback queue, and microtask queue? | How do closures work in JavaScript, and how have you used them in real projects to encapsulate state or build abstractions? | Describe a situation where async/await significantly improved the readability or reliability of your asynchronous code. | How would you assess the correctness and edge-case coverage of a non-trivial JavaScript function that manipulates arrays or objects? | When reviewing JavaScript code, what specific indicators help you judge its readability and long-term maintainability? | Can you walk through how JavaScript’s prototype chain works and how it affects property lookup and method overriding? | How do you approach spotting and fixing performance bottlenecks in a JavaScript-heavy application, either in the browser or in Node.js? | What are common pitfalls you watch for when dealing with promises, and how do you avoid subtle bugs like unhandled rejections or race conditions? | How do you decide whether a piece of logic should live on the frontend or backend when working in a full-stack JavaScript environment? | Describe how you would review frontend state management logic (for example, in React or Vue) to ensure it is predictable, testable, and easy to debug. | How do you evaluate whether JavaScript code is structured in a way that makes it easy to extend new features without introducing regressions? | What strategies do you use to validate that code handling user input and API responses is robust against errors, null values, and unexpected shapes? | Can you explain how you reason about time complexity and space complexity in JavaScript, and how that influences your choice of data structures? | How do you verify the correctness of code that uses debouncing, throttling, or other timing-related patterns in the browser? | Describe how you would review the integration between a frontend JavaScript client and a backend API for correctness, failure handling, and data consistency. | What is your approach to evaluating the safety and reliability of third-party JavaScript libraries before adopting them in a codebase? | How do you critically review TypeScript or JSDoc-style typings to ensure they accurately model the data and prevent runtime errors? | When using code assistants or LLMs to help you while coding, how do you verify that their suggestions are correct, efficient, and secure before accepting them? | Can you describe a detailed example where you refactored existing JavaScript code to improve structure, reduce duplication, or clarify logic? | How do you document complex JavaScript behavior so that future reviewers and collaborators can easily understand the intent and constraints?",
    "source_url": "https://editor.superannotate.com/jobs/JS-25-112"
  },
  {
    "job_id": "Rust-25-804",
    "title": "Rust Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Rust Engineer for AI Data Training, you will review AI-generated Rust code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, safety, and adherence to the prompt; identify errors in ownership, borrowing, lifetimes, or algorithmic reasoning; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Rust patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs. Your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Rust, Lifetimes, Async Rust, Tokio, Actix, Serde, Systems programming, Backend development, English, Research",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Rust development experience in backend, CLI, or systems-focused projects. | Strong understanding of Rust’s ownership, borrowing, and lifetime model, with the ability to reason clearly about aliasing and data races. | Solid software engineering experience in at least one of backend services, command-line tools, or systems programming using Rust. | Ability to evaluate safe, idiomatic Rust code, including appropriate use of traits, generics, pattern matching, and error handling. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: experience with Tokio, Actix, Serde, and async Rust patterns in production or advanced side projects. | Preferred: competitive programming experience, contributions to Rust open-source ecosystems, and demonstrated ability to reason about performance, concurrency, and correctness in Rust code.",
    "sample_interview_questions": "How would you explain Rust’s ownership model to another engineer, and why is it central to memory safety in Rust? | Can you describe the difference between borrowing by reference and borrowing mutably in Rust, and how the borrow checker enforces these rules? | When reviewing Rust code, what patterns or signals tell you that lifetimes are being handled correctly versus only compiling by coincidence? | How do you reason about choosing between Box, Rc, Arc, and borrowing in a design that involves shared data structures? | What are some examples of code that may compile but still be considered non-idiomatic or unsafe in practice, and how do you identify them during review? | How do you compare Rust’s error handling approaches (Result, ? operator, custom error types) and decide which pattern fits a particular API? | Can you walk through how you would evaluate the correctness and edge-case handling of a function manipulating collections in Rust, such as using iterators and combinators? | When you encounter a complex lifetime error message, what is your systematic process for understanding and resolving it? | How do you assess the trade-offs between using synchronous Rust and async Rust in the context of backend, CLI, or systems tasks? | What factors do you consider when reviewing async Rust code that uses Tokio or Actix, especially around cancellation, backpressure, and resource leaks? | How do you evaluate the design of a trait-based abstraction in Rust for clarity, extensibility, and minimal coupling? | What steps do you take to assess whether Serde-based serialization and deserialization code is robust to schema changes and malformed input? | Can you discuss how you would review a Rust API for ergonomics, including naming, use of generics, and the balance between flexibility and simplicity? | How do you approach reasoning about performance characteristics in Rust, such as allocations, copies, and use of iterators versus indexing? | When examining unsafe blocks in Rust, what specific criteria do you apply to decide whether the usage is justified and correctly encapsulated? | How do you validate that a Rust CLI tool handles errors, logging, and exit codes in a way that is predictable and friendly for users and scripts? | What is your approach to reviewing module and crate structure in a Rust project for cohesion, clear boundaries, and maintainable dependencies? | How do you critically evaluate suggestions from LLMs or code assistants for Rust code, and what kinds of mistakes do you most frequently catch? | Can you describe an example where you refactored existing Rust code to improve safety, clarity, or performance, and how you measured the improvement? | When looking at contributions to a Rust open-source project, what aspects of the code and discussions do you examine to judge quality and alignment with project standards?",
    "source_url": "https://editor.superannotate.com/jobs/Rust-25-804"
  },
  {
    "job_id": "Go-25-319",
    "title": "Go Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Go Engineer for AI Data Training, you will review AI-generated Go code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, readability, and adherence to backend-style best practices; identify errors in concurrency, error handling, or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Go; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Go, Golang, Concurrency, Backend development, Microservices, gRPC, Cloud-native, Error handling, Profiling, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Go programming experience, ideally focused on backend services, APIs, or systems work. | Strong understanding of Go idioms, including clear error handling, composition over inheritance, and simple, readable interfaces. | Hands-on software engineering experience building and maintaining backend-style Go applications in production environments. | Ability to evaluate correctness and readability in backend-oriented Go code, including concurrency patterns, error propagation, and API design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: hands-on experience with Go frameworks and libraries such as Gin, gRPC, or Echo in real projects. | Preferred: merged PRs in Go open-source projects, competitive programming experience, cloud-native development exposure, and familiarity with Go performance profiling tools and techniques.",
    "sample_interview_questions": "How would you describe Go’s philosophy and core idioms to a developer coming from an object-oriented language like Java or C#? | Can you explain how goroutines and channels work together to implement concurrency in Go, and when you would favor one pattern over another? | When reviewing Go code that uses shared state, what signs tell you that data races or concurrency issues may be present? | How do you typically structure error handling in Go, and what patterns do you consider idiomatic versus overly complex? | What factors do you evaluate to judge whether a Go function or method is sufficiently readable and maintainable for a production codebase? | Can you walk through how the Go scheduler works at a high level and how that influences your design of concurrent systems? | How do you approach designing and reviewing HTTP or gRPC handlers in Go for clarity, separation of concerns, and testability? | What strategies do you use to organize Go packages and modules so that dependencies remain clear and the architecture stays modular? | How do you evaluate the trade-offs between using interfaces and concrete types in Go APIs, and what are common mistakes you watch for? | When reading Go code that interacts with context.Context, what patterns indicate that timeouts, cancellations, and deadlines are being handled correctly? | How do you reason about performance in Go, and what kinds of issues do you look for before resorting to profiling tools? | What is your process for using Go’s profiling tools (pprof, trace) to diagnose bottlenecks or memory leaks in a service? | How do you review Go code that uses third-party libraries or frameworks like Gin or Echo to ensure they are integrated safely and consistently? | When evaluating persistence-related code in Go (for example, database access), what patterns help ensure correctness, transaction safety, and clear error propagation? | How do you decide whether to use channels versus other synchronization primitives like mutexes or wait groups in a given design? | What are some common anti-patterns you have seen in Go projects, and how do you recommend refactoring them toward more idiomatic solutions? | How do you ensure that logging, metrics, and observability concerns are handled cleanly within Go services without polluting business logic? | When reviewing a Go-based microservice, what aspects of configuration, deployment, and cloud-native integration do you pay attention to? | How do you critically evaluate suggestions from LLMs or code assistants for Go, and what types of errors or non-idiomatic patterns do you most frequently catch? | Can you describe a concrete example where you improved an existing Go codebase for correctness, concurrency behavior, or readability, and what steps you took to get there?",
    "source_url": "https://editor.superannotate.com/jobs/Go-25-319"
  },
  {
    "job_id": "Ruby-25-982",
    "title": "Ruby Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Ruby Engineer for AI Data Training, you will review AI-generated Ruby and Rails code or generate your own solutions, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for readability, maintainability, and correctness; identify errors in MVC structure, domain modeling, or control flow; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Ruby patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Ruby, Ruby on Rails, MVC, Backend development, Web applications, RSpec, Metaprogramming, Refactoring, Code review, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Ruby programming experience in production environments. | Strong understanding of Ruby on Rails, MVC architecture, and idiomatic Ruby patterns for structuring application code. | Hands-on software engineering experience building and maintaining Ruby or Rails applications, with exposure to real-world production constraints. | Ability to evaluate readability, maintainability, and logical correctness in Ruby and Rails code, including model, controller, and service-layer design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating and reviewing their output. | Excellent English writing skills, capable of producing clear, structured, and pedagogical technical explanations and code reviews. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, large-scale code review, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: experience with frameworks and tools such as Ruby on Rails, Sinatra, and RSpec in real projects. | Preferred: merged PRs in Ruby open-source projects, experience with metaprogramming or test-heavy systems, and competitive programming exposure.",
    "sample_interview_questions": "How would you describe the Ruby language’s philosophy and core idioms, and how does that influence how you structure code in real projects? | Can you explain the MVC pattern in the context of Ruby on Rails and how responsibilities should be divided between models, views, and controllers? | When reviewing a Rails controller, what are the main indicators that too much business logic has leaked into the controller layer? | How do you decide when to extract code into service objects, concerns, or modules in a Rails application to improve maintainability? | What are some common Ruby idioms you look for when evaluating whether code is idiomatic versus simply functional but harder to read? | How do you approach reasoning about callbacks, validations, and associations in Active Record models to keep them understandable and testable? | When you encounter metaprogramming in a Ruby codebase, what criteria do you use to decide whether it is justified or overcomplicating the design? | How do you evaluate the readability and maintainability of a complex Ruby method that chains multiple enumerable operations or uses blocks heavily? | What strategies do you use to ensure that Rails routing and controller structure remain coherent as an application grows in size and features? | How do you think about error handling and exception usage in Ruby and Rails applications, and what patterns help keep this logic clear? | Can you describe how you would review an RSpec test suite for clarity, coverage, and resilience to refactoring? | When looking at database queries generated by Active Record, what do you look for to identify potential performance or N+1 query problems? | How do you evaluate whether a Rails application’s configuration, environment handling, and secrets management follow best practices for security and maintainability? | What is your approach to reviewing code that heavily uses Ruby blocks, procs, or lambdas, and how do you judge whether the abstraction is appropriate? | How do you ensure that view templates or view components remain simple, testable, and free from excessive business logic? | When reviewing a pull request in a Ruby or Rails project, what is your personal checklist for readability, logic correctness, and alignment with style guidelines? | How do you assess the impact of a proposed refactor on an existing Ruby codebase, especially in systems with a large and critical test suite? | What aspects of a Ruby on Rails application do you pay attention to when evaluating for long-term maintainability, such as naming, directory structure, or dependency management? | How do you critically evaluate suggestions from LLMs or code assistants for Ruby or Rails code, and what types of mistakes do you most often catch? | Can you describe an instance where you improved an existing Ruby or Rails codebase for clarity, maintainability, or testability, and what concrete steps you took?",
    "source_url": "https://editor.superannotate.com/jobs/Ruby-25-982"
  },
  {
    "job_id": "Kotlin-25-582",
    "title": "Kotlin Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Kotlin Engineer, you will review AI-generated responses and generate high-quality Kotlin-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical information; craft expert-level explanations and model solutions that demonstrate correct approaches; and rate and compare multiple AI responses based on correctness and reasoning quality. T\n\nhis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, helping to improve the world’s premier AI models.",
    "keywords": "Kotlin, Android development, Backend engineering, Coroutines, Null safety, Ktor, Android Jetpack, Software architecture, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on Kotlin development experience. | Strong understanding of Kotlin null safety, coroutines, and functional idioms. | Professional software engineering experience using Kotlin for Android or backend services. | Minimum Bachelor’s degree in Computer Science or a closely related field. | Ability to critically evaluate idiomatic Kotlin style, design decisions, and architecture trade-offs. | Significant experience using large language models (LLMs) to assist with coding, debugging, and code review. | Excellent English writing skills with the ability to articulate complex technical concepts clearly. | Highly preferred: Experience with Ktor, Android Jetpack, and advanced coroutine usage in production systems. | Preferred: Merged pull requests in Kotlin open-source projects, competitive programming experience, or similar demonstration of strong problem-solving skills. | Experience reviewing app logic or mobile/backend architecture, with previous AI data training or model evaluation work strongly preferred and Minimum C1 English proficiency.",
    "sample_interview_questions": "Can you explain how Kotlin’s null safety system works and how it helps prevent common runtime errors compared to Java? | How do you decide when to use coroutines versus other concurrency mechanisms in a Kotlin-based system? | Describe a situation where you refactored Kotlin code to be more idiomatic. What changes did you make and why? | What are the key differences between suspend functions and regular functions in Kotlin, and how do you avoid blocking the main thread when using them? | How do you structure a multi-module Kotlin project to keep responsibilities clear and maintainable over time? | Can you walk through how you would analyze and improve the performance of a slow Kotlin API or Android feature? | What architectural patterns (for example, MVVM, Clean Architecture, hexagonal) have you used with Kotlin, and what trade-offs did you observe? | How do you handle error propagation and exception handling in coroutine-based code to keep logic clear and robust? | In your experience, what are some common anti-patterns or code smells you see in Kotlin codebases, and how do you address them? | How would you evaluate whether a piece of Kotlin code is truly idiomatic, beyond just compiling and working correctly? | Describe how you have used Kotlin features like extension functions, data classes, and sealed classes to model domain logic effectively. | If you have used Ktor, how would you design and organize routes, middleware, and configuration for a medium-sized backend service? | How do you approach reviewing another engineer’s Android or backend Kotlin code for correctness, readability, and architectural soundness? | What strategies do you use to ensure that asynchronous operations in an Android app (such as network calls) remain lifecycle-aware and safe? | Can you explain how you would reason about and debug a race condition or concurrency bug in coroutine-based code? | How do you typically leverage large language models while coding, and what are your criteria for trusting or questioning their suggestions? | Describe how you would validate that a proposed Kotlin implementation correctly fulfills a product requirement or user story. | How do you think about test strategy in Kotlin projects, including unit tests and integration tests, to ensure code reliability? | What factors do you consider when choosing between different dependency injection approaches in a Kotlin project (for example, Dagger/Hilt versus manual DI)? | Tell me about a time you had to evaluate or improve the architecture of a mobile or backend system built with Kotlin. What steps did you take and what was the outcome?",
    "source_url": "https://editor.superannotate.com/jobs/Kotlin-25-582"
  },
  {
    "job_id": "Swift25-913",
    "title": "Swift Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Swift Engineer, you will review AI-generated responses and generate high-quality Swift and iOS-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical details; write expert-level explanations and model solutions that demonstrate correct approaches in Swift and iOS; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, helping to improve the world’s premier AI models.",
    "keywords": "Swift, iOS development, SwiftUI, UIKit, Combine, Mobile architecture, ARC, UI logic review, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on Swift/iOS development experience. | Strong understanding of core Swift fundamentals, memory management, and ARC. | Professional iOS engineering experience shipping or maintaining production mobile applications. | Minimum Bachelor’s degree in Computer Science or a closely related field. | Ability to evaluate code for correctness, maintainability, performance, and UI logic quality. | Significant experience using large language models (LLMs) to assist with coding, debugging, and code review. | Excellent English writing skills with the ability to articulate complex technical concepts clearly. | Highly preferred: Experience with SwiftUI, UIKit, and Combine in modern iOS architectures such as MVVM. | Preferred: Merged pull requests in Swift/iOS open-source projects, competitive programming exposure, or similar evidence of strong algorithmic problem-solving skills. | Experience reviewing app logic or mobile architecture, with previous AI data training or model evaluation work strongly preferred, and Minimum C1 English proficiency.",
    "sample_interview_questions": "How does Automatic Reference Counting (ARC) work in Swift, and what are strong, weak, and unowned references used for? | Can you describe a situation where a retain cycle might occur in an iOS app and how you would detect and resolve it? | When would you choose a struct versus a class in Swift, and what practical implications does that choice have for iOS app design? | How do value types and reference types behave differently when passed into functions or stored in collections in Swift? | Explain how you would design a networking layer for an iOS app using URLSession or a similar abstraction. | What are the key differences between UIKit and SwiftUI, and when might you prefer one over the other in a new feature? | How do you approach organizing view and business logic when using MVVM or another modern iOS architecture pattern? | Can you walk through how Combine or another reactive framework can be used to manage asynchronous data flows in an iOS app? | What strategies do you use to keep view controllers lean and maintainable in a UIKit-based application? | How would you analyze and improve the performance of a scrolling list that feels laggy on certain devices? | What are some common pitfalls you have seen in iOS concurrency, and how do you avoid issues when working with the main thread? | How do you approach reviewing another engineer’s Swift code for readability, correctness, and long-term maintainability? | Describe how you would validate that a complex UI flow behaves correctly given different user inputs and edge cases. | What techniques do you use to debug tricky layout or Auto Layout issues in iOS interfaces? | How do you typically structure unit tests and UI tests for an iOS feature to ensure it is well covered? | In what cases would you use property wrappers in Swift, and how can they help keep code expressive and reusable? | How do you reason about and handle error propagation in a Swift codebase that uses Result types or async/await? | Describe how you have used large language models while coding and how you decide whether to accept or modify their suggestions. | When evaluating an existing iOS codebase, what signals tell you that the architecture is healthy or that it needs refactoring? | How do you balance short-term delivery pressure with the need to maintain a clean and scalable iOS codebase over time?",
    "source_url": "https://editor.superannotate.com/jobs/Swift25-913"
  },
  {
    "job_id": "Bash-25-913",
    "title": "Bash/PowerShell Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Bash/PowerShell Engineer, you will review AI-generated responses and generate high-quality automation-focused content, evaluating the reasoning quality and step-by-step problem-solving behind scripts and troubleshooting workflows. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in logic, safety, or portability; fact-check technical details; write expert-level explanations and model solutions that demonstrate correct scripting patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, directly helping to improve the world’s premier AI models.",
    "keywords": "Bash, PowerShell, Shell scripting, DevOps automation, Infrastructure scripting, CI/CD pipelines, Azure PowerShell, awk sed grep, LLM-assisted coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Expert-level proficiency in Bash and/or PowerShell scripting. | 2–3+ years of hands-on experience in at least one of Bash or PowerShell. | Strong experience writing, debugging, and maintaining shell or object-based automation scripts. | Ability to evaluate scripts for correctness, readability, safety, performance, and portability across environments. | Professional experience in software engineering, DevOps, IT automation, or infrastructure-focused roles. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Significant experience using large language models (LLMs) while coding, troubleshooting, and reviewing scripts. | Excellent English writing skills with the ability to document and explain complex automation clearly. | Highly preferred: For Bash, strong comfort with awk, sed, and grep; for PowerShell, experience with PowerShell Core, DSC, and Azure PowerShell modules. | Preferred: Merged PRs in CLI, DevOps, or infrastructure-related open-source projects; familiarity with CI pipelines, automation workflows, or cloud scripting; previous AI data training or model evaluation work strongly preferred and Minimum C1 English proficiency.",
    "sample_interview_questions": "How do you typically structure and organize a larger Bash or PowerShell script to keep it readable and maintainable over time? | Can you explain the main differences between Bash and PowerShell in terms of how they handle data, pipelines, and error handling? | When debugging a failing script in a production-like environment, what is your step-by-step approach to identifying and isolating the root cause? | How do you evaluate whether a script is safe to run on multiple environments, and what practices do you follow to avoid destructive operations? | What techniques do you use in Bash or PowerShell to make scripts idempotent and reliable when used in automation workflows? | Can you describe how you manage configuration, secrets, and environment-specific values within automation scripts? | How do you design scripts so they fail gracefully and provide useful diagnostic information when something goes wrong? | In Bash, how do tools like awk, sed, and grep fit into your typical data-processing or log-parsing workflows? | In PowerShell, how do you take advantage of objects, pipelines, and built-in cmdlets to manipulate and query data effectively? | What factors do you consider when deciding whether a task should be implemented as a one-off script, a reusable module, or part of a larger automation framework? | How do you approach reviewing another engineer’s shell or PowerShell script for correctness, readability, and long-term maintainability? | Can you describe an example where you improved the performance or robustness of an existing automation script and what changes you made? | How do you ensure your scripts are portable across different operating systems, shells, or PowerShell editions when needed? | What is your approach to logging, tracing, and monitoring within scripts that run as part of CI/CD pipelines or scheduled jobs? | How do you think about security concerns such as command injection, unsafe input handling, or misuse of elevated privileges in scripts? | When working with cloud platforms or remote infrastructure, how do you structure scripts that interact with APIs, CLIs, or remote sessions? | How have you used large language models while writing or reviewing scripts, and what criteria do you use to validate their suggestions? | What strategies do you follow to keep a growing collection of automation scripts discoverable, documented, and consistent across a team? | How do you test and validate changes to important automation scripts before they are used in production environments? | When examining an unfamiliar script, what aspects do you look at first to assess its quality, safety, and alignment with best practices?",
    "source_url": "https://editor.superannotate.com/jobs/Bash-25-913"
  },
  {
    "job_id": "R-25-613",
    "title": "R Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid R Engineer, you will review AI-generated responses and generate high-quality R and data-analysis-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in statistical methodology or data-wrangling workflows; fact-check analytical results; write expert-level explanations and model solutions that demonstrate correct use of R; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, directly helping to improve the world’s premier AI models.",
    "keywords": "R programming, Statistical modeling, Data analysis, Data visualization, Reproducible research, Data cleaning, Time series analysis, Machine learning in R, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on experience using R for data analysis, statistics, or data science work. | Strong proficiency in R programming, including data wrangling, functional programming patterns, and writing reusable functions or packages. | Solid grounding in applied statistics, including regression, inference, and model validation, with practical experience implementing these methods in R. | Experience building end-to-end analyses in R that include data cleaning, exploratory analysis, modeling, and visualization. | Familiarity with common R ecosystems such as tidyverse, data.table, and ggplot2, and the ability to choose appropriate tools for a given task. | Professional experience in a data-focused role such as data scientist, statistician, quantitative analyst, or similar. | Minimum Bachelor’s degree in Statistics, Mathematics, Computer Science, or a closely related quantitative field. | Significant experience using large language models (LLMs) to assist with coding, analysis design, and code review in R. | Excellent English writing skills with the ability to document analyses and explain complex statistical ideas clearly to non-experts. | Previous experience with AI data training or model evaluation is strongly preferred, and Minimum C1 English proficiency is required.",
    "sample_interview_questions": "How do you typically structure an R analysis project to keep data loading, cleaning, modeling, and reporting clear and organized? | When would you choose base R functions versus tidyverse tools such as dplyr or purrr for data manipulation, and why? | Can you explain the differences between data.frame, tibble, and data.table objects and when you might prefer each in practice? | How do you approach handling missing data in R, and how do you decide between methods such as deletion, imputation, or modeling-based approaches? | What are the advantages of vectorized operations in R compared with explicit loops, and how do you identify code that would benefit from vectorization? | How do you design an analysis pipeline in R to be reproducible across machines and over time, including package versions and random seeds? | Describe your process for debugging a failing R script or notebook, including the tools and functions you rely on most often. | How do you evaluate whether an R script or function is readable and maintainable for other analysts or engineers on your team? | Can you walk through how you would choose and fit an appropriate statistical model in R for a regression problem with both numeric and categorical predictors? | How do you typically validate and compare models in R, for example using cross-validation, train/test splits, or information criteria? | Describe how you use ggplot2 or another visualization library to explore data, and how you decide which visual encodings best communicate specific relationships. | What strategies do you use in R to work efficiently with large datasets that do not fit comfortably into memory? | How would you approach building a small internal R package to share reusable functions, including documentation and testing? | What are common pitfalls you have seen when working with factors or categorical variables in R, and how do you avoid them? | How do you connect R to external data sources such as relational databases or APIs, and what best practices do you follow when doing so? | Describe your approach to writing unit tests and regression tests for R code, and how you decide what needs to be tested. | When reviewing another person’s R code for an analysis, what aspects do you focus on to judge correctness, clarity, and statistical soundness? | Can you give an example of a complex analysis you completed in R, including how you structured the work and communicated the results? | How do you collaborate with others on R-based projects using version control and shared environments or notebooks? | How have you used large language models while working in R, and what checks do you perform to confirm that their suggestions are statistically and technically sound?",
    "source_url": "https://editor.superannotate.com/jobs/R-25-613"
  },
  {
    "job_id": "Mechanical-25-777",
    "title": "Mechanical Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $69 / hour",
    "pay_min": -1,
    "pay_max": 69,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated engineering solutions and/or generate expert Mechanical Engineering content, evaluating reasoning quality and step-by-step problem-solving while providing crisp written feedback. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify methodological or conceptual errors; fact-check assumptions; write high-quality explanations and model solutions; and rate and compare multiple responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate, delivering training data for many of the world’s largest AI companies and foundation-model labs. Your mechanical engineering expertise directly helps improve the world’s premier AI models by making their technical reasoning more accurate, reliable, and clearly explained.",
    "keywords": "Mechanical Engineering, Thermodynamics, Heat Transfer, Fluid Mechanics, Statics, Dynamics, Differential Equations, System Modeling, Design Principles, English (C1+)",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Mechanical Engineering, Aerospace Engineering, or Engineering Physics (Thermo/Heat Transfer/Fluids/Statics-Dynamics/Differential Equations). | Strong, confident fundamentals in mechanics, thermodynamics, fluid mechanics, applied physics, engineering math, and system modeling. | Comfortable evaluating technical solutions for correctness, methodology, units/dimensional consistency, assumptions, and physical plausibility with high attention to detail. | Able to clearly explain reasoning and corrections in writing; Minimum C1 English proficiency. | Experience applying basic design principles (requirements, constraints, margins, trade-offs) in real engineering contexts. | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation or technical reviewing is strongly preferred.",
    "sample_interview_questions": "A transient conduction model predicts faster cool-down than observed. Without re-running anything, what hierarchy of checks do you apply (units, properties, boundary conditions, contact resistances, radiation, internal generation) to identify the most likely root cause? | In a compressible duct with heat addition (Rayleigh flow), what qualitative trends in Mach number and stagnation temperature would you expect as heat is added in subsonic vs supersonic regimes, and how can those expectations catch a flawed solution? | A CFD report shows turbulent pipe flow with y+ ~ 200 but claims wall-resolved accuracy. What questions would you ask to determine whether the wall treatment is appropriate and whether friction factor/ΔP should be trusted? | You see an analysis that uses constant viscosity and density for a thermal-fluid problem with large temperature rise. What dimensionless groups or sensitivity arguments would you use to judge whether variable-property effects materially change the conclusion? | A mechanism’s static equilibrium solution gives plausible reactions, but its predicted actuator torque is far too low. What are the most common modeling omissions (friction, moment arms, constraint forces, gravity distribution) that create this specific symptom? | A rotor-bearing system shows a critical speed near the operating range. What evidence would make you model gyroscopic effects and cross-coupled stiffness, and what failure modes do those effects tend to introduce if ignored? | An engineer estimates convective coefficients using a textbook correlation outside its stated range (Re, Pr, geometry). What ‘red flags’ tell you extrapolation is unsafe, and what alternative bounding approach would you use to sanity-check results? | A first-principles system model of a closed volume produces negative absolute pressure during a rapid valve event. What does that imply about compressibility assumptions, numerical integration stability, or state variable choice? | A lumped-capacitance model passes a Biot-number check, yet measured temperature gradients persist. What physical mechanisms (internal heat generation distribution, anisotropy, contact resistance, phase change, radiation) can invalidate the practical lumped assumption? | Two engineers disagree on whether buoyancy matters in a forced-convection enclosure flow. What scaling analysis (e.g., Richardson/Grashof vs Reynolds) would you apply, and how would you interpret borderline values? | A thin-wall pressure vessel calculation looks fine globally, but a nozzle penetration is present. What specific stress components and local effects would you expect to dominate near the discontinuity, and what would you look for in a credible solution narrative? | A statics solution ‘works’ numerically, but the structure exhibits unexpected motion in reality. What stability checks distinguish a solvable equilibrium set from an actually stable configuration (mechanism vs structure)? | In a boiling heat transfer loop, outlet quality and wall superheat rise sharply at similar heat flux. What tells you the system is approaching CHF versus experiencing flow instabilities or dryout due to maldistribution? | A finite element result shows extremely high stress at a sharp re-entrant corner; the mesh refinement increases the peak without convergence. How do you distinguish a mathematical singularity from a design-critical hotspot, and what quantity do you report instead? | A dynamic model linearized at an operating point matches small-signal response but fails under moderate step inputs. What features (nonlinear friction, saturation, regime switching, property variation) would you suspect first, and how would they manifest in residuals? | Given multiaxial, nonproportional loading on a ductile part, what limitations of von Mises/Tresca would you consider, and what additional evidence would you seek before accepting a single scalar margin? | Pump + system curves predict an operating point, but field measurements show higher head at lower flow. What hidden assumptions (NPSH, cavitation, instrumentation bias, viscosity correction, recirculation, control valve characteristics) are most diagnostic to check? | A radiation model assumes gray, diffuse surfaces and uses view factors, yet predicts net heat flow in the wrong direction. What quick consistency checks (energy balance, reciprocity, enclosure constraints, emissivity bounds) can reveal the modeling error? | When reviewing an engineering solution, what are your top three ‘physics sanity tests’ (order-of-magnitude bounds, limiting cases, conservation checks) and how do they change across fluids, thermo, and dynamics problems? | You’re comparing two solution approaches: one uses aggressive simplifying assumptions but is elegantly consistent; the other is more general but internally messy. What criteria do you use to judge correctness, robustness across edge cases, and decision risk?",
    "source_url": "https://editor.superannotate.com/jobs/Mechanical-25-777"
  },
  {
    "job_id": "Finance-25-141",
    "title": "Senior Finance Specialist",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$35 - $90 / hour",
    "pay_min": 35,
    "pay_max": 90,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated finance analyses and/or generate expert finance content, evaluating reasoning quality and step-by-step problem-solving while providing precise written feedback. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify methodological or conceptual errors; fact-check financial claims and assumptions; write high-quality explanations and model solutions; and rate and compare multiple responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate, delivering training data for many of the world’s largest AI companies and foundation-model labs. Your finance expertise directly helps improve the world’s premier AI models by making their financial reasoning more accurate, reliable, and clearly explained.",
    "keywords": "Financial Statement Analysis, Valuation, Corporate Finance, Investments, Risk/Return, Financial Modeling, Econometrics, Macroeconomics, Accounting, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Finance, Economics, Accounting, Business, Financial Engineering, or Quantitative Finance (including coursework in Micro/Macro, Corporate Finance, Investments, Econometrics, and Accounting). | 5+ years of professional experience in Finance, Economics, Accounting, Business, Financial Engineering, or Quantitative Finance. | Strong command of financial statements, valuation (DCF and multiples), time value of money, and markets/instruments. | Confident in risk/return reasoning, basic macro & microeconomic intuition, and financial modeling logic (assumptions, drivers, sensitivities, consistency checks). | Able to rigorously review and explain reasoning, identify methodological errors, and fact-check claims with high attention to detail; Minimum C1 English proficiency. | Comfortable applying structured sanity checks (conservation-style checks for finance: reconciliation, sign/units consistency, boundary cases, and plausibility bounds). | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation, expert review, or editorial QA is strongly preferred.",
    "sample_interview_questions": "You’re reviewing a DCF where FCF grows faster than revenue for 10 years. What specific reconciliation checks would you apply to determine whether the margin expansion and reinvestment assumptions are internally consistent? | An analysis uses WACC with a beta from a different sector and a single-point ERP assumption. What evidence would you require to accept the beta and ERP choices, and what failure modes do you watch for in the resulting valuation? | A model treats stock-based compensation as a non-cash add-back with no dilution impact. What are the most common ways this misstates value, and what would you look for to correct it? | A company reports rising EBITDA but consistently negative operating cash flow. What are the highest-signal drivers you would investigate first, and what patterns would differentiate working-capital timing from earnings quality issues? | A leveraged buyout case shows a very high IRR despite modest multiple expansion. What are the likely hidden assumptions that can create this outcome, and how would you stress-test them conceptually? | You’re given two comparable-company sets: one yields a much higher multiple range than the other. What selection biases or capital-structure effects would you check before concluding the target is mispriced? | A model forecasts constant gross margin while input costs are volatile and partially hedged. What questions would you ask to assess whether hedging policy and pass-through dynamics are modeled credibly? | An analyst uses CAPM for a firm with meaningful country risk and concentrated customer exposure. What adjustments or alternative frameworks would you consider, and what decision criteria guide whether to apply them? | You suspect a model double-counts growth by combining high terminal growth with aggressive near-term share gains. What are the telltale signs of double-counting, and how would you reframe the growth story to avoid it? | A financial statement review shows capitalized costs rising rapidly. What accounting and cash-flow impacts would you expect, and what would you ask to determine whether capitalization is appropriate versus earnings management? | An NPV analysis for a project uses nominal cash flows but a real discount rate. What symptoms would appear in the output, and what quick checks would you use to catch this mismatch? | A bank model assumes deposit costs remain flat even as benchmark rates rise. What balance-sheet and competitive dynamics would you examine to judge whether the assumption is defensible? | A credit memo claims strong interest coverage, but most cash generation is from non-recurring working-capital releases. How would you re-assess debt service capacity and what indicators would you prioritize? | Two valuation methods disagree materially: DCF suggests upside while multiples imply downside. What structured diagnostic steps would you take to isolate whether the divergence is driven by cash-flow forecast, discounting, or comp selection? | A portfolio optimization write-up claims higher expected return with lower risk after adding a new asset. What assumptions about correlations and estimation error would you challenge, and why? | A risk analysis uses VaR without discussing fat tails or liquidity horizons. What scenario-based critiques would you raise, and what alternative risk views would you insist on before trusting the conclusion? | A model uses ‘plug’ items to balance the balance sheet. Under what circumstances can plugs be acceptable, and what are the most dangerous plugs that typically conceal major errors? | A merger model shows EPS accretion, but you suspect value destruction. What conceptual checks would you use to distinguish accounting accretion from economic value creation? | You find a sensitivity table that varies only revenue growth and discount rate. What missing sensitivities are usually most decision-relevant for this type of valuation, and what tells you which to prioritize? | When comparing two competing analyses, one is numerically polished but assumption-light; the other is assumption-rich but messier. What criteria do you use to judge which is more credible and decision-safe?",
    "source_url": "https://editor.superannotate.com/jobs/Finance-25-141"
  },
  {
    "job_id": "Legal-25-177",
    "title": "Legal Expert",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$40 - $115 / hour",
    "pay_min": 40,
    "pay_max": 115,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated legal analyses and/or generate expert legal content, evaluating reasoning quality and step-by-step issue-spotting while providing precise written feedback. You will assess answers for accuracy, clarity, and adherence to the prompt; identify errors in legal methodology or doctrine; fact-check citations and stated rules; write high-quality explanations and model solutions; and rate and compare multiple responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate, delivering training data for many of the world’s largest AI companies and foundation-model labs. Your legal expertise directly helps improve the world’s premier AI models by making their legal reasoning more accurate, reliable, and clearly explained.",
    "keywords": "Legal Reasoning, Statutory Interpretation, Case Law Analysis, Contracts, Torts, Civil Procedure, Criminal Law, Regulatory & Administrative Law, Compliance, English (C1+)",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Law (JD/LLB), Legal Studies, Public Policy, or Political Science, with strong grounding in Constitutional Law, Contracts, Torts, Criminal Law, Civil Procedure, and Regulatory/Administrative Law. | 5+ years of professional experience in Law, Legal Studies, Public Policy, or Political Science. | Strong legal reasoning and issue-spotting skills, including statutory interpretation, case analysis, and translating facts into elements and defenses. | Fluent in compliance concepts, rights & obligations analysis, and policy frameworks; able to identify missing facts and assumptions that change outcomes. | Exceptional attention to detail when checking rule statements, citations, jurisdictional relevance, procedural posture, and logical consistency; Minimum C1 English proficiency. | Able to write clear, structured feedback that explains errors and the correct reasoning path without unnecessary verbosity. | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation, expert review, legal editing, or QA is strongly preferred.",
    "sample_interview_questions": "You’re reviewing a memo that treats statutory silence as authorization. What interpretive steps would you expect before accepting that conclusion (text, structure, canons, context, legislative history, purpose), and what red flags indicate overreach? | A proposed rule includes a broad definition that appears to exceed the agency’s enabling statute. What analytical checkpoints distinguish a permissible interpretation from an ultra vires expansion? | A contract dispute analysis concludes ‘no breach’ because damages are hard to quantify. What doctrinal mistakes might that reflect, and what facts would you probe to assess breach, causation, and remedies separately? | A force majeure clause is invoked after a supply disruption. What is your framework for evaluating whether the event qualifies, whether performance is excused or delayed, and what notice/mitigation duties typically matter most? | Two cases are cited as controlling precedent, but they are from different jurisdictions and procedural postures. What questions do you ask to determine their binding weight and relevance to the issue presented? | A tort analysis assumes foreseeability is satisfied because harm occurred. What nuanced distinctions in duty/breach/proximate cause would you expect to see addressed to avoid outcome-driven reasoning? | A negligence memo cites res ipsa loquitur as shifting the burden of proof universally. What clarifying conditions and jurisdictional variations would you look for before accepting that statement? | A criminal law analysis treats mens rea as interchangeable across offenses. What issue-spotting steps ensure the correct mental state is matched to each element, and what common pitfalls should be caught? | A Fourth Amendment discussion assumes any warrantless search is unconstitutional. What structured exceptions and threshold questions must be considered to evaluate reasonableness properly? | A civil procedure answer asserts personal jurisdiction based solely on internet accessibility. What facts and doctrinal tests would you require to evaluate minimum contacts and purposeful availment credibly? | A removal analysis ignores timing and the forum-defendant rule. What procedural failure points do you check first when reviewing removal/ remand arguments? | A summary judgment discussion argues that credibility disputes can be resolved by the judge. What is the correct standard, and what indicators tell you the writer misunderstood the role of inferences and fact disputes? | A compliance assessment concludes ‘no risk’ because the policy exists. What evidence would you require to evaluate program effectiveness (controls, training, monitoring, enforcement) rather than paper compliance? | A regulatory memo cites a guidance document as if it were binding law. What questions distinguish legislative rules, interpretive rules, policy statements, and guidance—and why does it matter for enforcement risk? | An analysis claims a private right of action under a federal statute without addressing the test for implied rights. What doctrinal pathway must be analyzed, and what signals suggest the claim is weak? | A constitutional law answer applies strict scrutiny to a regulation without identifying the right or classification precisely. What triage approach do you use to select the correct level of scrutiny? | A due process argument focuses only on fairness, ignoring property/liberty interest and procedures due. What elements must be established before balancing tests become relevant? | A contracts analysis treats an ‘agreement in principle’ as fully enforceable. What formation issues (intent, definiteness, consideration, conditions precedent) do you examine to determine enforceability? | You are comparing two legal analyses: one is very confident but cites no authority; the other cites authority but uses conclusory reasoning. What criteria do you use to judge which is more trustworthy and where each fails? | When reviewing a legal answer for correctness, what are your top three ‘sanity checks’ (e.g., rule statement precision, element-by-element application, procedural posture fit), and what specific defects trigger an immediate rewrite?",
    "source_url": "https://editor.superannotate.com/jobs/Legal-25-177"
  },
  {
    "job_id": "Medical-25-104",
    "title": "Medical General Expert",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$40 - $90 / hour",
    "pay_min": 40,
    "pay_max": 90,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated medical responses and/or generate expert healthcare content, evaluating reasoning quality and step-by-step clinical problem-solving while providing precise written feedback. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in clinical methodology or conceptual understanding; fact-check medical information; write high-quality explanations and model solutions that demonstrate correct reasoning; and rate and compare multiple responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate, delivering training data for many of the world’s largest AI companies and foundation-model labs. Your healthcare expertise directly helps improve the world’s premier AI models by making their clinical and public-health reasoning more accurate, reliable, and clearly explained.",
    "keywords": "Clinical Reasoning, Disease Processes, Patient Care, Epidemiology, Public Health, Healthcare Systems, Medical Terminology, Risk Stratification, Evidence-Based Medicine, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Medicine (MD/DO), Nursing, Public Health (MPH), Health Sciences, or Allied Health, with strong grounding in Epidemiology, Clinical Medicine, Healthcare Systems, and Patient Care. | 5+ years of professional experience in Nursing, Public Health, Health Sciences, or Allied Health. | Confident in clinical reasoning (differential diagnosis, risk stratification, red-flag recognition) and explaining why a conclusion follows from the evidence. | Strong understanding of disease processes, patient care concepts, public health principles, healthcare systems, and medical terminology. | Exceptional attention to detail when fact-checking medical content and identifying unsafe assumptions, missing contraindications, or misinterpretation of tests; Minimum C1 English proficiency. | Comfortable evaluating answers for internal consistency (timelines, physiology, dosing logic), appropriateness for setting (ED vs outpatient), and patient safety implications. | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation, clinical documentation review, utilization review, or healthcare editorial QA is strongly preferred.",
    "sample_interview_questions": "A patient has chest pain with a nondiagnostic initial ECG and a borderline troponin. What clinical features and risk-stratification considerations most strongly influence whether you treat this as ACS, and what common reasoning errors cause unsafe reassurance? | A workup plan proposes broad-spectrum antibiotics for suspected pneumonia without assessing severity or likely pathogen. What decision points should be used to justify outpatient vs inpatient care and narrow vs broad coverage in principle? | You’re reviewing a differential diagnosis that lists many conditions but fails to prioritize. What criteria do you use to rank likely vs dangerous causes, and how do you identify when the differential is missing a must-not-miss diagnosis? | A note attributes dyspnea to anxiety after a normal chest X-ray. What additional history, exam, and basic test patterns would you expect to be considered before accepting a functional diagnosis? | A clinical summary treats ‘normal vital signs’ as excluding sepsis. What physiologic and timing considerations make this reasoning unreliable, and what alternative framing is more clinically sound? | An interpretation claims a normal anion gap rules out metabolic acidosis. What conceptual checks (mixed disorders, albumin effect, respiratory compensation) would you apply to validate acid–base conclusions? | A diabetes management answer recommends intensifying insulin based only on a single high glucose. What factors should be assessed first (measurement context, hypoglycemia risk, adherence, renal function, intercurrent illness) to avoid harm? | A medication safety review misses a high-risk drug–drug interaction. What systematic approach do you use to surface the most clinically significant interactions (severity, mechanism, timing, patient factors)? | A patient-care rationale uses relative risk reduction to argue a therapy is ‘highly effective’ without absolute risk. What questions would you ask to translate that into absolute benefit and number needed to treat for decision-making? | A screening recommendation is made without considering pretest probability and harms. What principles guide when screening is net-beneficial, and what red flags suggest over-screening? | A pediatric fever case is managed as viral illness with no safety-netting. What clinical reasoning elements justify no immediate testing, and what return precautions are most essential conceptually? | An analysis concludes that a positive test confirms disease without considering base rates. How do you explain, at a clinical level, why PPV changes with prevalence and what that implies for interpreting results? | A public-health summary claims an intervention ‘caused’ a mortality decrease based on a before/after comparison. What alternative explanations (confounding, secular trends, regression to the mean) must be assessed before causal language is acceptable? | An outbreak investigation proposal selects a convenient sample and then generalizes findings. What bias risks does this create, and what design choices would make conclusions more credible? | A guideline-based care answer copies recommendations but ignores contraindications and patient preferences. What checks ensure applicability to the individual patient and avoid guideline misapplication? | A triage recommendation underestimates risk in an elderly patient with atypical symptoms. What age-related presentation differences and baseline risks should alter threshold decisions? | A case analysis cites ‘normal labs’ as excluding appendicitis. What is the correct reasoning about sensitivity/specificity of common tests here, and how do you avoid test-anchoring? | A discharge plan fails to address social determinants that affect adherence. What key nonclinical factors most predict failure of outpatient management, and how should they alter disposition reasoning? | A clinician assumes oxygen saturation is a complete measure of respiratory status. What physiologic limitations make that incomplete, and what additional indicators help avoid missed deterioration? | When comparing two competing clinical explanations, one is elegant but ignores edge cases, and the other is more complex but safer. What criteria do you use to judge which reasoning is more defensible for patient safety?",
    "source_url": "https://editor.superannotate.com/jobs/Medical-25-104"
  },
  {
    "job_id": "English-25-364",
    "title": "English Language Expert",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $35 / hour",
    "pay_min": -1,
    "pay_max": 35,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 0,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated English-language responses and/or generate expert language content, evaluating reasoning quality and step-by-step edits while providing precise, actionable feedback. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in logic, meaning, methodology, or conceptual understanding; fact-check information when needed; write high-quality explanations and model revisions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation-model labs. Your language expertise directly helps improve the world’s premier AI models by making their outputs more accurate, natural, and clearly explained.",
    "keywords": "English, Grammar & Syntax, Semantics, Pragmatics, Discourse Analysis, Copyediting, Style Guides, Localization, Multilingual Communication",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Linguistics, English, Translation/Localization, Communications, Journalism, or a related field. | Expert command of English (C2/Native-level preferred) with Minimum C1 English proficiency required; multilingual ability (professional proficiency in at least one additional language) is strongly preferred. | Strong skills in grammar, syntax, semantics, pragmatics, discourse structure, and stylistic editing across registers. | Exceptional attention to detail in identifying meaning drift, ambiguity, inconsistencies, and subtle errors; able to explain corrections clearly in writing. | Comfortable applying style guides and enforcing consistency (tone, terminology, punctuation, capitalization) across varied content types. | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation, editorial QA, localization QA, or professional copyediting is strongly preferred. | Strong hands-on experience using tools like Perplexity, Gemini, ChatGPT and others",
    "sample_interview_questions": "Two answers are grammatically correct, but one feels unnatural. What concrete linguistic cues (collocation, register, information structure, presupposition) do you use to justify which is more natural? | A response contains no obvious grammar errors but subtly changes the prompt’s meaning. What semantic drift patterns do you look for, and how do you detect them quickly? | An answer is accurate but mismatched to the implied audience level. What signals tell you the register/reading level is wrong, and how do you correct it without losing precision? | A multi-paragraph response is locally coherent but globally confusing. What discourse diagnostics (topic progression, cohesion, anaphora management) do you use to localize the issue? | Two rewrites differ mainly in hedging (may/could/likely). How do you decide when hedging improves epistemic accuracy vs when it becomes evasive or misleading? | A response has high ambiguity due to stacked modifiers and PPs. What ambiguity tests do you apply, and how do you choose the safest disambiguation? | When is passive voice the best choice in English, and what criteria do you use to evaluate whether passive harms clarity or improves focus? | A response overuses nominalizations (‘implementation’, ‘optimization’). What principles guide when nominalization is acceptable and when it harms readability? | A response is fluent but subtly biased via framing. What linguistic markers (loaded terms, agency assignment, presuppositions) do you check to identify bias without overcorrecting? | A procedure explanation mixes tenses. When is tense variation acceptable, and when does it become a correctness or clarity problem? | A list is parallel in form but not in meaning. What parallelism checks ensure items align syntactically and semantically? | Pronouns have multiple possible antecedents. What strategy do you use to resolve and rewrite anaphora to prevent misinterpretation? | How do you distinguish a true grammar error from a nonstandard but acceptable construction (dialectal, rhetorical, or genre-specific)? | What punctuation pitfalls most often change meaning (restrictive vs nonrestrictive clauses, commas with coordinators), and how do you prioritize them in review? | Two edits both improve clarity, but one increases legal/technical risk by being too definitive. What framework balances clarity with appropriate uncertainty? | A response mixes US and UK conventions. Beyond spelling, what consistency checks (dates, punctuation, quotation style) do you enforce and why? | An answer is correct but too verbose. What editing heuristics compress text while preserving constraints, caveats, and dependencies? | A response uses idioms that may confuse global audiences. What criteria do you use to keep idioms vs localize for clarity? | You suspect confident fabrication. What textual signals (over-precise numerics, unnamed authorities, improbable specifics) trigger skepticism first? | When comparing two strong responses, what senior-level evaluation dimensions beyond grammar do you apply (truth-conditional fidelity, pragmatic fit, discourse effectiveness), and how do you operationalize them?",
    "source_url": "https://editor.superannotate.com/jobs/English-25-364"
  },
  {
    "job_id": "French-25-301",
    "title": "French/English Bilingual Expert",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "France",
    "pay_rate": "$18 - $33 / hour",
    "pay_min": 18,
    "pay_max": 33,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 2,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated French-language responses and/or generate expert language content, evaluating reasoning quality and step-by-step edits while providing precise, actionable feedback. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in logic, meaning, methodology, or conceptual understanding; fact-check information when needed; write high-quality explanations and model revisions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation-model labs. Your language expertise directly helps improve the world’s premier AI models by making their outputs more accurate, natural, and clearly explained.",
    "keywords": "French, English, French Grammar, Syntax & Agreement, Semantics, Pragmatics, Register & Tone, Copyediting, Localization, Multilingual Communication",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Linguistics, French, Translation/Localization, Communications, or a related field. | Native or near-native French proficiency (C2 preferred) and Minimum C1 English proficiency required; multilingual ability beyond French/English is a plus. | Strong command of French grammar (agreement, tense/mood), semantics, pragmatics, and discourse-level editing across registers. | Exceptional attention to detail in identifying meaning drift, ambiguity, locale inconsistencies, and subtle errors; able to explain corrections clearly in writing. | Comfortable applying French style and punctuation conventions and enforcing terminology consistency for specific audiences and locales. | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation, editorial QA, translation/localization QA, or professional editing is strongly preferred.",
    "sample_interview_questions": "Two French answers are grammatical, but one sounds translated. What calques/anglicisms do you look for, and how do you justify a more idiomatic alternative? | A response has agreement errors inside complex noun phrases. What systematic method do you use to catch agreement issues in long-distance dependencies? | A response mixes ‘tu/vous’ with inconsistent formality. What contextual cues determine pronoun choice, and when is mixing unacceptable? | A sentence is correct but reads awkwardly. What restructuring strategies (clivage, mise en relief, ordre des propositions) improve flow without changing meaning? | How do you decide between passé composé and imparfait in narrative explanations, and what errors typically cause misuse? | Subjonctif vs indicatif is used inconsistently. What triggers do you check first, and how do you handle borderline or style-driven cases? | A response overuses abstract nominalizations. What editing heuristics in French restore concreteness and clarity while staying formal? | Pronoun references (‘il/elle/ce’) are ambiguous. What disambiguation tactics do you use while preserving naturalness? | Two edits differ mainly in connectors (‘cependant’, ‘en revanche’, ‘or’). How do you select connectors that match the intended discourse relation precisely? | What French punctuation conventions most often affect professionalism (espaces avant ‘; : ? !’, guillemets), and how do you enforce them consistently? | A response includes false friends that alter meaning. What are the most common false-friend patterns you proactively scan for in bilingual review? | An answer alternates ‘on’ and ‘nous’. What criteria determine which is appropriate for a given audience and context? | Technical terms have multiple acceptable French renderings. How do you decide which terminology is correct (domain usage, standardization, audience)? | You suspect subtle bias via framing. What linguistic markers in French (présupposés, agents/causes) do you check to diagnose bias? | A translation preserves words but loses implicature/politeness. What pragmatic elements are most commonly lost, and how do you test for them? | A response mixes France-French and other locale norms. When does locale inconsistency materially matter, and what consistency checks do you apply? | You suspect fabricated specifics. What textual signals (over-precise dates, vague authorities, improbable proper nouns) raise suspicion first? | One response is elegant but slightly less precise; the other is precise but clunky. What framework do you use to balance correctness vs naturalness for the prompt’s purpose? | A multi-paragraph explanation is locally coherent but globally weak. What discourse-structure checks (chaînes thématiques, transitions, synthèse) do you apply? | What are your top senior-level evaluation dimensions beyond grammar (fidelity, pragmatics, discourse), and how do you operationalize them for consistent scoring?",
    "source_url": "https://editor.superannotate.com/jobs/French-25-301"
  },
  {
    "job_id": "Italian-25-317",
    "title": "Italian Language Expert",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Italy",
    "pay_rate": "$-1 - $37 / hour",
    "pay_min": -1,
    "pay_max": 37,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 2,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated Italian-language responses and/or generate expert language content, evaluating reasoning quality and step-by-step edits while providing precise, actionable feedback. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in logic, meaning, methodology, or conceptual understanding; fact-check information when needed; write high-quality explanations and model revisions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation-model labs. Your language expertise directly helps improve the world’s premier AI models by making their outputs more accurate, natural, and clearly explained.",
    "keywords": "Italian, English, Italian Grammar, Morphosyntax, Semantics, Pragmatics, Register & Tone, Copyediting, Localization, Multilingual Communication",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Linguistics, Italian, Translation/Localization, Communications, or a related field. | Native or near-native Italian proficiency (C2 preferred) and Minimum C1 English proficiency required; multilingual ability beyond Italian/English is a plus. | Strong command of Italian grammar (incl. clitics, tense/aspect, mood), semantics, pragmatics, and discourse-level editing across registers. | Exceptional attention to detail in identifying meaning drift, ambiguity, locale inconsistencies, and subtle errors; able to explain corrections clearly in writing. | Comfortable applying style guides and enforcing terminology and tone consistency for specific audiences and locales. | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation, editorial QA, translation/localization QA, or professional editing is strongly preferred. | Location requirement: Only candidates based in Italy will be considered for this role",
    "sample_interview_questions": "Two Italian answers are correct, but one sounds like a literal translation. What calques or word-order artifacts do you look for to justify an idiomatic rewrite? | A response mishandles clitic pronouns (position/combination). What systematic approach do you use to detect and correct clitic errors in long sentences? | How do you decide between passato prossimo and imperfetto, and what reasoning errors most often cause tense misuse? | Congiuntivo is used inconsistently. What triggers and constructions do you check first, and how do you handle variation vs standard usage? | A sentence is grammatical but heavy. What restructuring techniques (subordination vs coordination, participial clauses, topic-fronting) improve readability without meaning drift? | A response mixes ‘Lei/tu’ with inconsistent formality. What cues determine formality in Italian, and when is mixing unacceptable? | A multi-paragraph response is locally coherent but globally confusing. What discourse checks (topic continuity, anaphora, connectors, paragraph function) localize the breakdown? | Referents for ‘lo/ci/ne’ are ambiguous. What disambiguation tactics do you apply while keeping the text natural? | Two edits differ mainly in connectors (‘tuttavia’, ‘invece’, ‘pertanto’). How do you choose connectors that precisely match the intended logical relation? | An answer uses technical anglicisms. What criteria determine whether to keep an anglicism or replace it with an established Italian equivalent? | A response subtly changes obligations (must/should/can). What cues help you catch modality drift in Italian, especially in instructions and policies? | A text overuses nominalizations and passive. When are these appropriate in Italian, and when do they undermine clarity and agency? | What Italian punctuation conventions most affect professionalism and meaning, and how do you enforce consistency at scale? | A response contains false friends from English that change meaning. What are the most common patterns you proactively scan for? | You suspect subtle bias via framing and agency assignment. What markers (lexical choices, presuppositions, labeling) do you check to diagnose bias? | A translation preserves meaning but loses mitigation/politeness. What pragmatic elements are most likely to be lost, and how do you test for them? | A response mixes Italy-Italian norms with other regional conventions. When does this materially matter, and what consistency checks do you apply? | You suspect fabricated specifics. What textual signals (over-precise claims, dubious citations, unnatural named entities) raise suspicion first? | One response is elegant but slightly less precise; the other is precise but clunky. What framework balances correctness, naturalness, and risk given the prompt’s context? | What are your top senior-level evaluation dimensions beyond grammar (fidelity, pragmatics, discourse), and how do you operationalize them for consistent scoring?",
    "source_url": "https://editor.superannotate.com/jobs/Italian-25-317"
  },
  {
    "job_id": "German-25-302",
    "title": "English German Teacher",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Germany",
    "pay_rate": "$-1 - $49 / hour",
    "pay_min": -1,
    "pay_max": 49,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 2,
    "role_description": "In this hourly-paid, fully remote contractor role, you will review AI-generated responses and/or generate language content, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. You will apply senior-level German localization/translation judgment alongside Minimum C1 English proficiency to ensure terminology, tone, and cultural nuance are consistently correct across use cases. This role is with SME Careers, a fast-growing AI Data Services company and a subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation-model labs. Your work directly helps improve the world’s premier AI models.",
    "keywords": "German, English, Localization, Translation, Linguistic QA (LQA), MQM error typology, Terminology management, Style guides, CAT tools, Copyediting",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in German Translation, Linguistics, Localization, Communications, or a related field. | Native or near-native German proficiency and strong command of the locale conventions for Germany. | Minimum C1 English proficiency (reading/writing) for interpreting prompts, sources, and evaluation guidelines. | 5+ years of professional localization/translation experience (multilingual localization experience strongly preferred). | Demonstrated ability to apply MQM/LQA concepts (severity, category, and root-cause thinking) to produce consistent quality decisions. | Proficiency with CAT tools and QA workflows (translation memory, termbases, automated checks) and comfort working with structured guidelines. | Excellent editorial judgment (register, tone, inclusivity, and cultural nuance) with extreme attention to detail. | Ability to rigorously fact-check localized content (units, references, names, dates) using reliable sources and consistent reasoning. | Comfort working as an hourly contractor: dependable throughput, clear documentation, and responsiveness across time zones. | Previous experience with AI data training / annotation / evaluation is preferred. | Location requirement: Only candidates based in Germany will be considered for this role",
    "sample_interview_questions": "When an English source string is ambiguous, what decision framework do you use to choose the most accurate German interpretation without inventing intent, and what triggers escalation? | How do you separate meaning errors from stylistic preferences in LQA, and how do you keep those judgments consistent across reviewers? | Under an MQM-style rubric, how do you decide severity for: (a) terminology deviation, (b) punctuation/typography, (c) factual inconsistency, (d) tone mismatch? | In Germany, what are the highest-risk locale conventions (dates, time, numbers, currency, honorifics/formality), and how do you validate them reliably? | A translation is fluent but subtly changes modality (must/should/may) or certainty. What cues help you detect meaning drift quickly? | How do you decide when to translate, transliterate, or retain an English product term, and how do you prevent inconsistent outcomes across a large dataset? | What is your approach to resolving terminology conflicts when legacy translation memory contradicts the current glossary or style guide? | How do you evaluate whether a string is over-localized (added cultural assumptions) versus appropriately adapted for the audience? | How do you handle gendered language and inclusivity requirements when the source is gender-neutral but the target language typically is not? | A localized UI string is accurate but exceeds character limits and breaks layout. What are your preferred mitigation tactics and acceptance criteria? | How do you assess and enforce tone consistency across different surfaces (UI microcopy, help center, marketing) while preserving meaning? | What are common false-friend traps or polysemy pitfalls in your language pair, and what verification habit reduces errors under time pressure? | How do you verify named entities (people, brands, place names) and avoid incorrect localization when multiple conventions exist? | If two reviewers disagree—one calling a result “too literal,” another “too free”—how do you arbitrate using objective standards? | How do you identify when translation memory is propagating systemic errors, and what governance steps do you recommend to stop recurrence? | What is your method for spotting hallucinated facts in otherwise natural-sounding target-language content, especially in high-stakes domains? | In a multilingual release with a tight deadline, how do you triage work using risk-based prioritization and decide what can ship vs. must block? | What leading indicators of localization quality do you trust beyond raw error counts, and how would you use them to improve consistency? | If you find a repeated error pattern across hundreds of items (e.g., one term consistently mistranslated), what root-cause path do you follow? | How do you ensure your own decisions remain stable over time (inter- and intra-rater reliability), especially when guidelines evolve?",
    "source_url": "https://editor.superannotate.com/jobs/German-25-302"
  },
  {
    "job_id": "Japanese-25-303",
    "title": "Japanese Translator",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Japan",
    "pay_rate": "$-1 - $39 / hour",
    "pay_min": -1,
    "pay_max": 39,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 2,
    "role_description": "In this hourly-paid, fully remote contractor role, you will review AI-generated responses and/or generate language content, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. You will apply senior-level Japanese localization/translation judgment alongside Minimum C1 English proficiency to ensure terminology, tone, and cultural nuance are consistently correct across use cases. This role is with SME Careers, a fast-growing AI Data Services company and a subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation-model labs. Your work directly helps improve the world’s premier AI models.",
    "keywords": "Japanese, English, Localization, Translation, Linguistic QA (LQA), MQM error typology, Terminology management, Style guides, CAT tools, Copyediting",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Japanese Translation, Linguistics, Localization, Communications, or a related field. | Native or near-native Japanese proficiency and strong command of the locale conventions for Japan. | Minimum C1 English proficiency (reading/writing) for interpreting prompts, sources, and evaluation guidelines. | 5+ years of professional localization/translation experience (multilingual localization experience strongly preferred). | Demonstrated ability to apply MQM/LQA concepts (severity, category, and root-cause thinking) to produce consistent quality decisions. | Proficiency with CAT tools and QA workflows (translation memory, termbases, automated checks) and comfort working with structured guidelines. | Excellent editorial judgment (register, tone, inclusivity, and cultural nuance) with extreme attention to detail. | Ability to rigorously fact-check localized content (units, references, names, dates) using reliable sources and consistent reasoning. | Comfort working as an hourly contractor: dependable throughput, clear documentation, and responsiveness across time zones. | Previous experience with AI data training / annotation / evaluation is preferred.",
    "sample_interview_questions": "When an English source string is ambiguous, what decision framework do you use to choose the most accurate Japanese interpretation without inventing intent, and what triggers escalation? | How do you separate meaning errors from stylistic preferences in LQA, and how do you keep those judgments consistent across reviewers? | Under an MQM-style rubric, how do you decide severity for: (a) terminology deviation, (b) punctuation/typography, (c) factual inconsistency, (d) tone mismatch? | In Japan, what are the highest-risk locale conventions (dates, time, numbers, currency, honorifics/formality), and how do you validate them reliably? | A translation is fluent but subtly changes modality (must/should/may) or certainty. What cues help you detect meaning drift quickly? | How do you decide when to translate, transliterate, or retain an English product term, and how do you prevent inconsistent outcomes across a large dataset? | What is your approach to resolving terminology conflicts when legacy translation memory contradicts the current glossary or style guide? | How do you evaluate whether a string is over-localized (added cultural assumptions) versus appropriately adapted for the audience? | How do you handle gendered language and inclusivity requirements when the source is gender-neutral but the target language typically is not? | A localized UI string is accurate but exceeds character limits and breaks layout. What are your preferred mitigation tactics and acceptance criteria? | How do you assess and enforce tone consistency across different surfaces (UI microcopy, help center, marketing) while preserving meaning? | What are common false-friend traps or polysemy pitfalls in your language pair, and what verification habit reduces errors under time pressure? | How do you verify named entities (people, brands, place names) and avoid incorrect localization when multiple conventions exist? | If two reviewers disagree—one calling a result “too literal,” another “too free”—how do you arbitrate using objective standards? | How do you identify when translation memory is propagating systemic errors, and what governance steps do you recommend to stop recurrence? | What is your method for spotting hallucinated facts in otherwise natural-sounding target-language content, especially in high-stakes domains? | In a multilingual release with a tight deadline, how do you triage work using risk-based prioritization and decide what can ship vs. must block? | What leading indicators of localization quality do you trust beyond raw error counts, and how would you use them to improve consistency? | If you find a repeated error pattern across hundreds of items (e.g., one term consistently mistranslated), what root-cause path do you follow? | How do you ensure your own decisions remain stable over time (inter- and intra-rater reliability), especially when guidelines evolve?",
    "source_url": "https://editor.superannotate.com/jobs/Japanese-25-303"
  },
  {
    "job_id": "Chinese-25-304",
    "title": "Chinese/English Bilingual Expert",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "China",
    "pay_rate": "$15 - $30 / hour",
    "pay_min": 15,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 1,
    "role_description": "In this hourly-paid, fully remote contractor role, you will review AI-generated responses and/or generate language content, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. You will apply senior-level Chinese (Simplified) localization/translation judgment alongside Minimum C1 English proficiency to ensure terminology, tone, and cultural nuance are consistently correct across use cases. This role is with SME Careers, a fast-growing AI Data Services company and a subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation-model labs. Your work directly helps improve the world’s premier AI models.",
    "keywords": "Chinese (Simplified), English, Localization, Translation, Linguistic QA (LQA), MQM error typology, Terminology management, Style guides, CAT tools, Copyediting",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Translation, Linguistics, Localization, Communications, or a related field. | Native or near-native Chinese (Simplified) proficiency and strong command of the locale conventions for China. | Minimum C1 English proficiency (reading/writing) for interpreting prompts, sources, and evaluation guidelines. | 5+ years of professional localization/translation experience (multilingual localization experience strongly preferred). | Demonstrated ability to apply MQM/LQA concepts (severity, category, and root-cause thinking) to produce consistent quality decisions. | Proficiency with CAT tools and QA workflows (translation memory, termbases, automated checks) and comfort working with structured guidelines. | Excellent editorial judgment (register, tone, inclusivity, and cultural nuance) with extreme attention to detail. | Ability to rigorously fact-check localized content (units, references, names, dates) using reliable sources and consistent reasoning. | Comfort working as an hourly contractor: dependable throughput, clear documentation, and responsiveness across time zones. | Previous experience with AI data training / annotation / evaluation is preferred.",
    "sample_interview_questions": "When an English source string is ambiguous, what decision framework do you use to choose the most accurate Chinese (Simplified) interpretation without inventing intent, and what triggers escalation? | How do you separate meaning errors from stylistic preferences in LQA, and how do you keep those judgments consistent across reviewers? | Under an MQM-style rubric, how do you decide severity for: (a) terminology deviation, (b) punctuation/typography, (c) factual inconsistency, (d) tone mismatch? | In China, what are the highest-risk locale conventions (dates, time, numbers, currency, honorifics/formality), and how do you validate them reliably? | A translation is fluent but subtly changes modality (must/should/may) or certainty. What cues help you detect meaning drift quickly? | How do you decide when to translate, transliterate, or retain an English product term, and how do you prevent inconsistent outcomes across a large dataset? | What is your approach to resolving terminology conflicts when legacy translation memory contradicts the current glossary or style guide? | How do you evaluate whether a string is over-localized (added cultural assumptions) versus appropriately adapted for the audience? | How do you handle gendered language and inclusivity requirements when the source is gender-neutral but the target language typically is not? | A localized UI string is accurate but exceeds character limits and breaks layout. What are your preferred mitigation tactics and acceptance criteria? | How do you assess and enforce tone consistency across different surfaces (UI microcopy, help center, marketing) while preserving meaning? | What are common false-friend traps or polysemy pitfalls in your language pair, and what verification habit reduces errors under time pressure? | How do you verify named entities (people, brands, place names) and avoid incorrect localization when multiple conventions exist? | If two reviewers disagree—one calling a result “too literal,” another “too free”—how do you arbitrate using objective standards? | How do you identify when translation memory is propagating systemic errors, and what governance steps do you recommend to stop recurrence? | What is your method for spotting hallucinated facts in otherwise natural-sounding target-language content, especially in high-stakes domains? | In a multilingual release with a tight deadline, how do you triage work using risk-based prioritization and decide what can ship vs. must block? | What leading indicators of localization quality do you trust beyond raw error counts, and how would you use them to improve consistency? | If you find a repeated error pattern across hundreds of items (e.g., one term consistently mistranslated), what root-cause path do you follow? | How do you ensure your own decisions remain stable over time (inter- and intra-rater reliability), especially when guidelines evolve?",
    "source_url": "https://editor.superannotate.com/jobs/Chinese-25-304"
  },
  {
    "job_id": "Portuguese-25-305",
    "title": "Portuguese Translator",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Brazil",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 2,
    "role_description": "In this hourly-paid, fully remote contractor role, you will review AI-generated responses and/or generate language content, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. You will apply senior-level Portuguese localization/translation judgment alongside Minimum C1 English proficiency to ensure terminology, tone, and cultural nuance are consistently correct across use cases. This role is with SME Careers, a fast-growing AI Data Services company and a subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation-model labs. Your work directly helps improve the world’s premier AI models.",
    "keywords": "Portuguese, English, Localization, Translation, Linguistic QA (LQA), MQM error typology, Terminology management, Style guides, CAT tools, Copyediting",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Translation, Linguistics, Localization, Communications, or a related field. | Native or near-native Portuguese proficiency and strong command of the locale conventions for Portugal. | Minimum C1 English proficiency (reading/writing) for interpreting prompts, sources, and evaluation guidelines. | 5+ years of professional localization/translation experience (multilingual localization experience strongly preferred). | Demonstrated ability to apply MQM/LQA concepts (severity, category, and root-cause thinking) to produce consistent quality decisions. | Proficiency with CAT tools and QA workflows (translation memory, termbases, automated checks) and comfort working with structured guidelines. | Excellent editorial judgment (register, tone, inclusivity, and cultural nuance) with extreme attention to detail. | Ability to rigorously fact-check localized content (units, references, names, dates) using reliable sources and consistent reasoning. | Comfort working as an hourly contractor: dependable throughput, clear documentation, and responsiveness across time zones. | Previous experience with AI data training / annotation / evaluation is preferred. | Location requirement: Only candidates based in Brazil will be considered for this role",
    "sample_interview_questions": "When an English source string is ambiguous, what decision framework do you use to choose the most accurate Portuguese interpretation without inventing intent, and what triggers escalation? | How do you separate meaning errors from stylistic preferences in LQA, and how do you keep those judgments consistent across reviewers? | Under an MQM-style rubric, how do you decide severity for: (a) terminology deviation, (b) punctuation/typography, (c) factual inconsistency, (d) tone mismatch? | In Portugal, what are the highest-risk locale conventions (dates, time, numbers, currency, honorifics/formality), and how do you validate them reliably? | A translation is fluent but subtly changes modality (must/should/may) or certainty. What cues help you detect meaning drift quickly? | How do you decide when to translate, transliterate, or retain an English product term, and how do you prevent inconsistent outcomes across a large dataset? | What is your approach to resolving terminology conflicts when legacy translation memory contradicts the current glossary or style guide? | How do you evaluate whether a string is over-localized (added cultural assumptions) versus appropriately adapted for the audience? | How do you handle gendered language and inclusivity requirements when the source is gender-neutral but the target language typically is not? | A localized UI string is accurate but exceeds character limits and breaks layout. What are your preferred mitigation tactics and acceptance criteria? | How do you assess and enforce tone consistency across different surfaces (UI microcopy, help center, marketing) while preserving meaning? | What are common false-friend traps or polysemy pitfalls in your language pair, and what verification habit reduces errors under time pressure? | How do you verify named entities (people, brands, place names) and avoid incorrect localization when multiple conventions exist? | If two reviewers disagree—one calling a result “too literal,” another “too free”—how do you arbitrate using objective standards? | How do you identify when translation memory is propagating systemic errors, and what governance steps do you recommend to stop recurrence? | What is your method for spotting hallucinated facts in otherwise natural-sounding target-language content, especially in high-stakes domains? | In a multilingual release with a tight deadline, how do you triage work using risk-based prioritization and decide what can ship vs. must block? | What leading indicators of localization quality do you trust beyond raw error counts, and how would you use them to improve consistency? | If you find a repeated error pattern across hundreds of items (e.g., one term consistently mistranslated), what root-cause path do you follow? | How do you ensure your own decisions remain stable over time (inter- and intra-rater reliability), especially when guidelines evolve?",
    "source_url": "https://editor.superannotate.com/jobs/Portuguese-25-305"
  },
  {
    "job_id": "Spanish-25-306",
    "title": "English Spanish Translator",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Spain",
    "pay_rate": "$-1 - $33 / hour",
    "pay_min": -1,
    "pay_max": 33,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 2,
    "role_description": "In this hourly-paid, fully remote contractor role, you will review AI-generated responses and/or generate language content, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. You will apply senior-level Spanish localization/translation judgment alongside Minimum C1 English proficiency to ensure terminology, tone, and cultural nuance are consistently correct across use cases. This role is with SME Careers, a fast-growing AI Data Services company and a subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation-model labs. Your work directly helps improve the world’s premier AI models.",
    "keywords": "Spanish, English, Localization, Translation, Linguistic QA (LQA), MQM error typology, Terminology management, Style guides, CAT tools, Copyediting",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Translation, Linguistics, Localization, Communications, or a related field. | Native or near-native Spanish proficiency and strong command of the locale conventions for Spain. | Minimum C1 English proficiency (reading/writing) for interpreting prompts, sources, and evaluation guidelines. | 5+ years of professional localization/translation experience (multilingual localization experience strongly preferred). | Demonstrated ability to apply MQM/LQA concepts (severity, category, and root-cause thinking) to produce consistent quality decisions. | Proficiency with CAT tools and QA workflows (translation memory, termbases, automated checks) and comfort working with structured guidelines. | Excellent editorial judgment (register, tone, inclusivity, and cultural nuance) with extreme attention to detail. | Ability to rigorously fact-check localized content (units, references, names, dates) using reliable sources and consistent reasoning. | Comfort working as an hourly contractor: dependable throughput, clear documentation, and responsiveness across time zones. | Previous experience with AI data training / annotation / evaluation is preferred. | Location requirement: Only candidates based in Spain will be considered for this role",
    "sample_interview_questions": "When an English source string is ambiguous, what decision framework do you use to choose the most accurate Spanish interpretation without inventing intent, and what triggers escalation? | How do you separate meaning errors from stylistic preferences in LQA, and how do you keep those judgments consistent across reviewers? | Under an MQM-style rubric, how do you decide severity for: (a) terminology deviation, (b) punctuation/typography, (c) factual inconsistency, (d) tone mismatch? | In Spain, what are the highest-risk locale conventions (dates, time, numbers, currency, honorifics/formality), and how do you validate them reliably? | A translation is fluent but subtly changes modality (must/should/may) or certainty. What cues help you detect meaning drift quickly? | How do you decide when to translate, transliterate, or retain an English product term, and how do you prevent inconsistent outcomes across a large dataset? | What is your approach to resolving terminology conflicts when legacy translation memory contradicts the current glossary or style guide? | How do you evaluate whether a string is over-localized (added cultural assumptions) versus appropriately adapted for the audience? | How do you handle gendered language and inclusivity requirements when the source is gender-neutral but the target language typically is not? | A localized UI string is accurate but exceeds character limits and breaks layout. What are your preferred mitigation tactics and acceptance criteria? | How do you assess and enforce tone consistency across different surfaces (UI microcopy, help center, marketing) while preserving meaning? | What are common false-friend traps or polysemy pitfalls in your language pair, and what verification habit reduces errors under time pressure? | How do you verify named entities (people, brands, place names) and avoid incorrect localization when multiple conventions exist? | If two reviewers disagree—one calling a result “too literal,” another “too free”—how do you arbitrate using objective standards? | How do you identify when translation memory is propagating systemic errors, and what governance steps do you recommend to stop recurrence? | What is your method for spotting hallucinated facts in otherwise natural-sounding target-language content, especially in high-stakes domains? | In a multilingual release with a tight deadline, how do you triage work using risk-based prioritization and decide what can ship vs. must block? | What leading indicators of localization quality do you trust beyond raw error counts, and how would you use them to improve consistency? | If you find a repeated error pattern across hundreds of items (e.g., one term consistently mistranslated), what root-cause path do you follow? | How do you ensure your own decisions remain stable over time (inter- and intra-rater reliability), especially when guidelines evolve?",
    "source_url": "https://editor.superannotate.com/jobs/Spanish-25-306"
  },
  {
    "job_id": "Hindi-25-308",
    "title": "Hindi/English Bilingual Expert",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "India",
    "pay_rate": "$8 - $20 / hour",
    "pay_min": 8,
    "pay_max": 20,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "In this hourly-paid, fully remote contractor role, you will review AI-generated responses and/or generate language content, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. You will apply senior-level Hindi localization/translation judgment alongside Minimum C1 English proficiency to ensure terminology, tone, and cultural nuance are consistently correct across use cases. This role is with SME Careers, a fast-growing AI Data Services company and a subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation-model labs. Your work directly helps improve the world’s premier AI models.",
    "keywords": "Hindi, English, Localization, Translation, Linguistic QA (LQA), MQM error typology, Terminology management, Style guides, CAT tools, Copyediting",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Translation, Linguistics, Localization, Communications, or a related field. | Native or near-native Hindi proficiency and strong command of the locale conventions for India. | Minimum C1 English proficiency (reading/writing) for interpreting prompts, sources, and evaluation guidelines. | 5+ years of professional localization/translation experience (multilingual localization experience strongly preferred). | Demonstrated ability to apply MQM/LQA concepts (severity, category, and root-cause thinking) to produce consistent quality decisions. | Proficiency with CAT tools and QA workflows (translation memory, termbases, automated checks) and comfort working with structured guidelines. | Excellent editorial judgment (register, tone, inclusivity, and cultural nuance) with extreme attention to detail. | Ability to rigorously fact-check localized content (units, references, names, dates) using reliable sources and consistent reasoning. | Comfort working as an hourly contractor: dependable throughput, clear documentation, and responsiveness across time zones. | Previous experience with AI data training / annotation / evaluation is preferred. | Location requirement: Only candidates based in Hindi will be considered for this role",
    "sample_interview_questions": "When an English source string is ambiguous, what decision framework do you use to choose the most accurate Hindi interpretation without inventing intent, and what triggers escalation? | How do you separate meaning errors from stylistic preferences in LQA, and how do you keep those judgments consistent across reviewers? | Under an MQM-style rubric, how do you decide severity for: (a) terminology deviation, (b) punctuation/typography, (c) factual inconsistency, (d) tone mismatch? | In India, what are the highest-risk locale conventions (dates, time, numbers, currency, honorifics/formality), and how do you validate them reliably? | A translation is fluent but subtly changes modality (must/should/may) or certainty. What cues help you detect meaning drift quickly? | How do you decide when to translate, transliterate, or retain an English product term, and how do you prevent inconsistent outcomes across a large dataset? | What is your approach to resolving terminology conflicts when legacy translation memory contradicts the current glossary or style guide? | How do you evaluate whether a string is over-localized (added cultural assumptions) versus appropriately adapted for the audience? | How do you handle gendered language and inclusivity requirements when the source is gender-neutral but the target language typically is not? | A localized UI string is accurate but exceeds character limits and breaks layout. What are your preferred mitigation tactics and acceptance criteria? | How do you assess and enforce tone consistency across different surfaces (UI microcopy, help center, marketing) while preserving meaning? | What are common false-friend traps or polysemy pitfalls in your language pair, and what verification habit reduces errors under time pressure? | How do you verify named entities (people, brands, place names) and avoid incorrect localization when multiple conventions exist? | If two reviewers disagree—one calling a result “too literal,” another “too free”—how do you arbitrate using objective standards? | How do you identify when translation memory is propagating systemic errors, and what governance steps do you recommend to stop recurrence? | What is your method for spotting hallucinated facts in otherwise natural-sounding target-language content, especially in high-stakes domains? | In a multilingual release with a tight deadline, how do you triage work using risk-based prioritization and decide what can ship vs. must block? | What leading indicators of localization quality do you trust beyond raw error counts, and how would you use them to improve consistency? | If you find a repeated error pattern across hundreds of items (e.g., one term consistently mistranslated), what root-cause path do you follow? | How do you ensure your own decisions remain stable over time (inter- and intra-rater reliability), especially when guidelines evolve?",
    "source_url": "https://editor.superannotate.com/jobs/Hindi-25-308"
  },
  {
    "job_id": "Korean-25-318",
    "title": "Korean/English Bilingual Expert",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "South Korea",
    "pay_rate": "$13 - $39 / hour",
    "pay_min": 13,
    "pay_max": 39,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 2,
    "role_description": "In this hourly-paid, fully remote contractor role, you will review AI-generated responses and/or generate language content, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. You will apply senior-level Korean localization/translation judgment alongside Minimum C1 English proficiency to ensure terminology, tone, and cultural nuance are consistently correct across use cases. This role is with SME Careers, a fast-growing AI Data Services company and a subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation-model labs. Your work directly helps improve the world’s premier AI models.",
    "keywords": "Korean, English, Localization, Translation, Linguistic QA (LQA), MQM error typology, Terminology management, Style guides, CAT tools, Copyediting",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Translation, Linguistics, Localization, Communications, or a related field. | Native or near-native Korean proficiency and strong command of the locale conventions for South Korea. | Minimum C1 English proficiency (reading/writing) for interpreting prompts, sources, and evaluation guidelines. | 5+ years of professional localization/translation experience (multilingual localization experience strongly preferred). | Demonstrated ability to apply MQM/LQA concepts (severity, category, and root-cause thinking) to produce consistent quality decisions. | Proficiency with CAT tools and QA workflows (translation memory, termbases, automated checks) and comfort working with structured guidelines. | Excellent editorial judgment (register, tone, inclusivity, and cultural nuance) with extreme attention to detail. | Ability to rigorously fact-check localized content (units, references, names, dates) using reliable sources and consistent reasoning. | Comfort working as an hourly contractor: dependable throughput, clear documentation, and responsiveness across time zones. | Previous experience with AI data training / annotation / evaluation is preferred. | Location requirement: Only candidates based in South Korea will be considered for this role",
    "sample_interview_questions": "When an English source string is ambiguous, what decision framework do you use to choose the most accurate Korean interpretation without inventing intent, and what triggers escalation? | How do you separate meaning errors from stylistic preferences in LQA, and how do you keep those judgments consistent across reviewers? | Under an MQM-style rubric, how do you decide severity for: (a) terminology deviation, (b) punctuation/typography, (c) factual inconsistency, (d) tone mismatch? | In South Korea, what are the highest-risk locale conventions (dates, time, numbers, currency, honorifics/formality), and how do you validate them reliably? | A translation is fluent but subtly changes modality (must/should/may) or certainty. What cues help you detect meaning drift quickly? | How do you decide when to translate, transliterate, or retain an English product term, and how do you prevent inconsistent outcomes across a large dataset? | What is your approach to resolving terminology conflicts when legacy translation memory contradicts the current glossary or style guide? | How do you evaluate whether a string is over-localized (added cultural assumptions) versus appropriately adapted for the audience? | How do you handle gendered language and inclusivity requirements when the source is gender-neutral but the target language typically is not? | A localized UI string is accurate but exceeds character limits and breaks layout. What are your preferred mitigation tactics and acceptance criteria? | How do you assess and enforce tone consistency across different surfaces (UI microcopy, help center, marketing) while preserving meaning? | What are common false-friend traps or polysemy pitfalls in your language pair, and what verification habit reduces errors under time pressure? | How do you verify named entities (people, brands, place names) and avoid incorrect localization when multiple conventions exist? | If two reviewers disagree—one calling a result “too literal,” another “too free”—how do you arbitrate using objective standards? | How do you identify when translation memory is propagating systemic errors, and what governance steps do you recommend to stop recurrence? | What is your method for spotting hallucinated facts in otherwise natural-sounding target-language content, especially in high-stakes domains? | In a multilingual release with a tight deadline, how do you triage work using risk-based prioritization and decide what can ship vs. must block? | What leading indicators of localization quality do you trust beyond raw error counts, and how would you use them to improve consistency? | If you find a repeated error pattern across hundreds of items (e.g., one term consistently mistranslated), what root-cause path do you follow? | How do you ensure your own decisions remain stable over time (inter- and intra-rater reliability), especially when guidelines evolve?",
    "source_url": "https://editor.superannotate.com/jobs/Korean-25-318"
  },
  {
    "job_id": "FrenchS-25-301",
    "title": "French Trust & Safety Data Trainer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "France",
    "pay_rate": "$-1 - $50 / hour",
    "pay_min": -1,
    "pay_max": 50,
    "currency": "USD",
    "num_people": 10,
    "priority_level": 3,
    "role_description": "In this hourly-paid, fully remote contractor role, you will review AI-generated responses and/or generate safety-focused evaluation content, assess reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, safe, and clearly explained. You will annotate and evaluate content in English and French; you must be fluent/proficient (near-native or native) in French and able to apply Minimum C1 English to interpret prompts and policies precisely. Your annotations on these explicit tasks will be used to prevent the Large Language Model from generating unintentional or adversarial, toxic, or unsafe outputs. The types of explicit content you may be exposed to may include, but are not limited to, those of a sexual, violent, or psychologically disturbing nature. This role is with SME Careers, a fast-growing AI Data Services company and a subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation-model labs, helping improve the world’s premier AI models.",
    "keywords": "French, English, Trust & Safety, Content policy, Red teaming, Hate & harassment, Self-harm safety, Misinformation, Malicious code risk, Bias assessment",
    "key_responsibilities": "Curate and label safety-focused training examples (including adversarial/red-team cases) in English and French that probe model behavior across hate/harassment, sexual content, self-harm, violence, bias, illegal services, malicious activity, malicious code, and misinformation—capturing nuance and intent with Minimum C1 English and near-native French proficiency. | Review, score, and compare multiple model responses against safety policy and quality rubrics, documenting why an output is safe/unsafe and identifying failure modes such as evasion, normalization, escalation, or procedural enablement. | Continuously stress-test and audit model behavior for policy gaps and edge cases; flag ambiguous scenarios, propose clearer decision rules, and help maintain consistent annotation standards across reviewers.",
    "your_profile": "Bachelor’s degree or higher in a relevant field (e.g., Communications, Linguistics, Psychology, Law/Policy, Security Studies) or equivalent professional experience. | Near-native or native French proficiency (reading/writing) for high-precision safety labeling and cultural-linguistic nuance. | Minimum C1 English proficiency (reading/writing) for policy interpretation, prompt understanding, and consistent documentation. | Experience in Trust & Safety, content moderation, policy enforcement, risk operations, investigations, or safety evaluation work. | LLM red teaming experience is a must (proven ability to probe safety boundaries and document adversarial patterns). | Strong knowledge of safety domains: Hate & Harassment, Sexual content, Suicide & Self-Harm, Violence, Bias, Illegal goods/services, malicious activities, malicious code, and deliberate misinformation. | Emotional resilience: an understanding that this role requires annotating texts that contain unsafe, explicit, and/or toxic content, including content of a sexual, violent, or psychologically disturbing nature. | Excellent judgment under ambiguity, with the ability to apply written policies consistently and explain decisions succinctly. | Comfort working as an hourly contractor: dependable throughput, clear documentation, and responsiveness across time zones. | Previous experience with AI data training / annotation / evaluation is preferred. | Strong hands-on experience using tools like Perplexity, Gemini, ChatGPT and others.",
    "sample_interview_questions": "In French, a user uses coded language to demean a protected group without slurs. What signals help you distinguish hate/harassment from general rudeness, and how do you justify the decision consistently? | How do you separate harassment targeting an individual from harassment targeting a protected class when the text contains both, and how do you decide the primary violation category? | When evaluating sexual content, what criteria help you differentiate sexual education, consensual adult sexual content, exploitation, and grooming—especially when the text is euphemistic? | How do you triage self-harm content into (a) ideation, (b) intent, (c) instructions, and (d) recovery/support, and what escalation triggers do you apply for each? | A piece of content depicts violence but is presented as news reporting. What factors determine whether it is allowed, restricted, or disallowed, and how do you handle graphic detail thresholds? | How do you handle content that blends dark humor with suicide/self-harm themes, where user intent is unclear and community risk is high? | What is your approach to identifying bias in model outputs when the harm is subtle (stereotyping, differential treatment, omission) rather than explicit slurs? | A prompt asks for advice that could enable illegal activity (e.g., forging documents) but is framed as fiction or ‘for research.’ What reasoning steps lead to a safe decision? | How do you evaluate ‘dual-use’ requests where legitimate and harmful interpretations are plausible (e.g., chemistry, security, or lock-related questions) without over-blocking? | What are the most common adversarial tactics you’ve seen in LLM red teaming to bypass safety constraints (e.g., roleplay, translation, obfuscation), and how do you categorize them for training data? | How do you judge whether a model response is ‘harmful’ when it contains accurate facts but presents them in a way that increases risk (e.g., operational detail, normalization, glamorization)? | When reviewing misinformation in French, how do you distinguish satire, opinion, unverified claims, and deliberate deception—especially when ‘sources’ are selectively quoted or misleading? | A model output includes a plausible-sounding medical claim with no source. What verification approach do you use to decide whether it is misinformation, and what safety handling would you apply? | How do you decide the boundary between ‘discussion of extremist ideology’ and ‘promotion/endorsement,’ when the content is subtle and uses dog whistles? | For ‘malicious activities’ requests, what conceptual cues help you identify social engineering attempts, credential theft intent, or scam enablement even without explicit instructions? | A user asks for code that could be used for harm (e.g., a keylogger) but claims benign intent. What factors do you use to classify risk and decide on refusal vs. safe alternative? | How do you evaluate and label prompt-injection attempts against a system instruction, and what annotation fields best capture the attack pattern and severity? | When multiple policy violations appear in one exchange (e.g., hate + threat + doxxing), how do you prioritize labels and severity so downstream training is unambiguous? | What techniques do you use to keep annotation quality high over long sessions with disturbing material (without compromising throughput), and what early warning signs indicate performance drift? | How would you calibrate a team of reviewers to achieve consistent decisions (inter-rater reliability) when policy language is high-level and edge cases are common?",
    "source_url": "https://editor.superannotate.com/jobs/FrenchS-25-301"
  },
  {
    "job_id": "GermanS-25-302",
    "title": "German Trust & Safety Data Trainer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Germany",
    "pay_rate": "$-1 - $65 / hour",
    "pay_min": -1,
    "pay_max": 65,
    "currency": "USD",
    "num_people": 10,
    "priority_level": 3,
    "role_description": "This is a remote, hourly-paid contract role where you will review AI-generated content and safety decisions, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, logical, and clearly explained. You will assess solutions for correctness and clarity, spot methodological or conceptual errors, fact-check where needed, and rate/compare multiple responses based on safety and policy alignment. You must be fluent/proficient (near-native or native) in German and able to make nuanced judgments across German and English content. This role is with the fast-growing AI Data Services company SME Careers (a subsidiary of SuperAnnotate), supporting many of the world’s largest AI companies and foundation-model labs. Your annotations on explicit safety tasks will be used to prevent the model from generating unintentional or adversarial, toxic, or unsafe outputs. You may be exposed to content that is sexual, violent, or psychologically disturbing in nature as part of this work to improve the world’s premier AI models.",
    "keywords": "Trust & Safety, Content Moderation, Policy Interpretation, Red Teaming, Risk Assessment, Hate & Harassment, Self-Harm, Misinformation, English, German",
    "key_responsibilities": "Label and quality-check safety data across categories such as hate/harassment, sexual content, self-harm, violence, bias, illegal goods/services, malicious activities, malicious code, and deliberate misinformation. | Perform red-teaming and adversarial testing by identifying realistic attack patterns, edge cases, and policy gray areas; document rationales and recommend mitigations to reduce unsafe outcomes. | Apply and localize safety policies consistently across German and English: detect cultural nuance, slang, coded language, and context shifts; escalate uncertainty using documented decision paths.",
    "your_profile": "Bachelor’s degree or higher in a relevant field (e.g., Communications, Linguistics, Psychology, Law/Policy, Security Studies) or equivalent professional experience. | Near-native or native German proficiency (reading/writing) for high-precision safety labeling and cultural-linguistic nuance. | Minimum C1 English proficiency (reading/writing) for policy interpretation, prompt understanding, and consistent documentation. | Experience in Trust & Safety, content moderation, policy operations, risk, compliance, investigations, or related safety functions (senior level). | LLM red-teaming / adversarial testing experience is required (documented examples of edge-case discovery and mitigation recommendations). | Localization/translation experience is highly preferred; able to preserve meaning, severity, and intent across languages. | Emotional resilience: comfortable annotating unsafe, explicit, and/or toxic content, including content of a sexual, violent, or psychologically disturbing nature. | Highly detail-oriented with strong judgment, consistency, and ability to follow evolving written guidelines. | Strong analytical writing: concise rationales, clear decision paths, and reproducible reasoning for disagreements. | Secure and confidential handling of sensitive content; reliable remote work practices and time management. | Strong hands-on experience using tools like Perplexity, Gemini, ChatGPT and others.",
    "sample_interview_questions": "When two policies appear to conflict in a single piece of content, how do you decide which policy has precedence and why? | How do you distinguish protected-class hate from general harassment when the target is ambiguous or implied rather than explicit? | What signals help you tell satire or dark humor from genuine hate/harassment in borderline cases? | How would you handle a case where a slur is reclaimed by an in-group speaker—what context must be present to change your decision? | What criteria would you use to separate consensual adult sexual content from coercion or exploitation when details are incomplete? | How do you assess risk and appropriate action for self-harm content that includes ideation but no stated intent or plan? | What are the most common failure modes when classifying violence content, and how do you avoid them? | How would you evaluate content that encourages discrimination indirectly (e.g., dog whistles or coded language) without explicit slurs? | How would you handle misinformation that mixes true facts with a false conclusion—what is your decision logic? | How do you differentiate legitimate political opinion from incitement, harassment, or targeted threats? | What factors help you decide whether instructions are illegal goods/services versus lawful discussion (e.g., academic, journalistic, or harm-reduction framing)? | How do you recognize social engineering attempts (phishing, impersonation, credential harvesting) when the message looks benign on the surface? | How do you classify malicious code requests when code is incomplete, obfuscated, or presented as debugging? | How do you deal with a user request that is dual-use (has legitimate and harmful applications) while staying consistent with policy? | What’s your method for ensuring consistency across a large guideline set when you are labeling at high volume under time pressure? | How do you audit your own decisions for bias, especially when content involves sensitive identities or stereotypes? | In multilingual moderation, what are the biggest pitfalls of direct translation, and how do you preserve intent and severity in German? | How would you handle a case where German slang changes the meaning of a seemingly neutral phrase into harassment or sexual content? | What would you do if you strongly disagree with a policy outcome but the guideline is unambiguous—how do you document and proceed? | What is one edge case you’d expect to find in a safety red-teaming pass, and how would you validate it?",
    "source_url": "https://editor.superannotate.com/jobs/GermanS-25-302"
  },
  {
    "job_id": "JapaneseS-25-303",
    "title": "Japanese Trust & Safety Data Trainer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Japan",
    "pay_rate": "$-1 - $39 / hour",
    "pay_min": -1,
    "pay_max": 39,
    "currency": "USD",
    "num_people": 10,
    "priority_level": 3,
    "role_description": "This is a remote, hourly-paid contract role where you will review AI-generated content and safety decisions, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, logical, and clearly explained. You will assess solutions for correctness and clarity, spot methodological or conceptual errors, fact-check where needed, and rate/compare multiple responses based on safety and policy alignment. You must be fluent/proficient (near-native or native) in Japanese and able to make nuanced judgments across Japanese and English content. This role is with the fast-growing AI Data Services company SME Careers (a subsidiary of SuperAnnotate), supporting many of the world’s largest AI companies and foundation-model labs. Your annotations on explicit safety tasks will be used to prevent the model from generating unintentional or adversarial, toxic, or unsafe outputs. You may be exposed to content that is sexual, violent, or psychologically disturbing in nature as part of this work to improve the world’s premier AI models.",
    "keywords": "Trust & Safety, Content Moderation, Policy Interpretation, Red Teaming, Risk Assessment, Hate & Harassment, Self-Harm, Misinformation, English, Japanese",
    "key_responsibilities": "Label and quality-check safety data across categories such as hate/harassment, sexual content, self-harm, violence, bias, illegal goods/services, malicious activities, malicious code, and deliberate misinformation. | Perform red-teaming and adversarial testing by identifying realistic attack patterns, edge cases, and policy gray areas; document rationales and recommend mitigations to reduce unsafe outcomes. | Apply and localize safety policies consistently across Japanese and English: detect cultural nuance, slang, coded language, and context shifts; escalate uncertainty using documented decision paths.",
    "your_profile": "Bachelor’s degree or higher in a relevant field (e.g., Communications, Linguistics, Psychology, Law/Policy, Security Studies) or equivalent professional experience. | Near-native or native Japanese proficiency (reading/writing) for high-precision safety labeling and cultural-linguistic nuance. | Minimum C1 English proficiency (reading/writing) for policy interpretation, prompt understanding, and consistent documentation. | Experience years in Trust & Safety, content moderation, policy operations, risk, compliance, investigations, or related safety functions (senior level). | LLM red-teaming / adversarial testing experience is required (documented examples of edge-case discovery and mitigation recommendations). | Localization/translation experience is highly preferred; able to preserve meaning, severity, and intent across languages. | Emotional resilience: comfortable annotating unsafe, explicit, and/or toxic content, including content of a sexual, violent, or psychologically disturbing nature. | Highly detail-oriented with strong judgment, consistency, and ability to follow evolving written guidelines. | Strong analytical writing: concise rationales, clear decision paths, and reproducible reasoning for disagreements. | Secure and confidential handling of sensitive content; reliable remote work practices and time management. | Strong hands-on experience using tools like Perplexity, Gemini, ChatGPT and others.",
    "sample_interview_questions": "When two policies appear to conflict in a single piece of content, how do you decide which policy has precedence and why? | How do you distinguish protected-class hate from general harassment when the target is ambiguous or implied rather than explicit? | What signals help you tell satire or dark humor from genuine hate/harassment in borderline cases? | How would you handle a case where a slur is reclaimed by an in-group speaker—what context must be present to change your decision? | What criteria would you use to separate consensual adult sexual content from coercion or exploitation when details are incomplete? | How do you assess risk and appropriate action for self-harm content that includes ideation but no stated intent or plan? | What are the most common failure modes when classifying violence content, and how do you avoid them? | How would you evaluate content that encourages discrimination indirectly (e.g., dog whistles or coded language) without explicit slurs? | How would you handle misinformation that mixes true facts with a false conclusion—what is your decision logic? | How do you differentiate legitimate political opinion from incitement, harassment, or targeted threats? | What factors help you decide whether instructions are illegal goods/services versus lawful discussion (e.g., academic, journalistic, or harm-reduction framing)? | How do you recognize social engineering attempts (phishing, impersonation, credential harvesting) when the message looks benign on the surface? | How do you classify malicious code requests when code is incomplete, obfuscated, or presented as debugging? | How do you deal with a user request that is dual-use (has legitimate and harmful applications) while staying consistent with policy? | What’s your method for ensuring consistency across a large guideline set when you are labeling at high volume under time pressure? | How do you audit your own decisions for bias, especially when content involves sensitive identities or stereotypes? | In multilingual moderation, what are the biggest pitfalls of direct translation, and how do you preserve intent and severity in Japanese? | How would you handle a case where Japanese slang changes the meaning of a seemingly neutral phrase into harassment or sexual content? | What would you do if you strongly disagree with a policy outcome but the guideline is unambiguous—how do you document and proceed? | What is one edge case you’d expect to find in a safety red-teaming pass, and how would you validate it?",
    "source_url": "https://editor.superannotate.com/jobs/JapaneseS-25-303"
  },
  {
    "job_id": "ChineseS-25-304",
    "title": "Chinese Trust & Safety Data Trainer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "China",
    "pay_rate": "$20 - $50 / hour",
    "pay_min": 20,
    "pay_max": 50,
    "currency": "USD",
    "num_people": 10,
    "priority_level": 3,
    "role_description": "This is a remote, hourly-paid contract role where you will review AI-generated content and safety decisions, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, logical, and clearly explained. You will assess solutions for correctness and clarity, spot methodological or conceptual errors, fact-check where needed, and rate/compare multiple responses based on safety and policy alignment. You must be fluent/proficient (near-native or native) in Chinese and able to make nuanced judgments across Chinese and English content. This role is with the fast-growing AI Data Services company SME Careers (a subsidiary of SuperAnnotate), supporting many of the world’s largest AI companies and foundation-model labs. Your annotations on explicit safety tasks will be used to prevent the model from generating unintentional or adversarial, toxic, or unsafe outputs. You may be exposed to content that is sexual, violent, or psychologically disturbing in nature as part of this work to improve the world’s premier AI models.",
    "keywords": "Trust & Safety, Content Moderation, Policy Interpretation, Red Teaming, Risk Assessment, Hate & Harassment, Self-Harm, Misinformation, English, Chinese",
    "key_responsibilities": "Label and quality-check safety data across categories such as hate/harassment, sexual content, self-harm, violence, bias, illegal goods/services, malicious activities, malicious code, and deliberate misinformation. | Perform red-teaming and adversarial testing by identifying realistic attack patterns, edge cases, and policy gray areas; document rationales and recommend mitigations to reduce unsafe outcomes. | Apply and localize safety policies consistently across Chinese and English: detect cultural nuance, slang, coded language, and context shifts; escalate uncertainty using documented decision paths.",
    "your_profile": "Bachelor’s degree or higher in a relevant field (e.g., Communications, Linguistics, Psychology, Law/Policy, Security Studies) or equivalent professional experience. | Near-native or native Chinese proficiency (reading/writing) for high-precision safety labeling and cultural-linguistic nuance. | Minimum C1 English proficiency (reading/writing) for policy interpretation, prompt understanding, and consistent documentation. | Experience years in Trust & Safety, content moderation, policy operations, risk, compliance, investigations, or related safety functions (senior level). | LLM red-teaming / adversarial testing experience is required (documented examples of edge-case discovery and mitigation recommendations). | Localization/translation experience is highly preferred; able to preserve meaning, severity, and intent across languages. | Emotional resilience: comfortable annotating unsafe, explicit, and/or toxic content, including content of a sexual, violent, or psychologically disturbing nature. | Highly detail-oriented with strong judgment, consistency, and ability to follow evolving written guidelines. | Strong analytical writing: concise rationales, clear decision paths, and reproducible reasoning for disagreements. | Secure and confidential handling of sensitive content; reliable remote work practices and time management. | Strong hands-on experience using tools like Perplexity, Gemini, ChatGPT and others",
    "sample_interview_questions": "When two policies appear to conflict in a single piece of content, how do you decide which policy has precedence and why? | How do you distinguish protected-class hate from general harassment when the target is ambiguous or implied rather than explicit? | What signals help you tell satire or dark humor from genuine hate/harassment in borderline cases? | How would you handle a case where a slur is reclaimed by an in-group speaker—what context must be present to change your decision? | What criteria would you use to separate consensual adult sexual content from coercion or exploitation when details are incomplete? | How do you assess risk and appropriate action for self-harm content that includes ideation but no stated intent or plan? | What are the most common failure modes when classifying violence content, and how do you avoid them? | How would you evaluate content that encourages discrimination indirectly (e.g., dog whistles or coded language) without explicit slurs? | How would you handle misinformation that mixes true facts with a false conclusion—what is your decision logic? | How do you differentiate legitimate political opinion from incitement, harassment, or targeted threats? | What factors help you decide whether instructions are illegal goods/services versus lawful discussion (e.g., academic, journalistic, or harm-reduction framing)? | How do you recognize social engineering attempts (phishing, impersonation, credential harvesting) when the message looks benign on the surface? | How do you classify malicious code requests when code is incomplete, obfuscated, or presented as debugging? | How do you deal with a user request that is dual-use (has legitimate and harmful applications) while staying consistent with policy? | What’s your method for ensuring consistency across a large guideline set when you are labeling at high volume under time pressure? | How do you audit your own decisions for bias, especially when content involves sensitive identities or stereotypes? | In multilingual moderation, what are the biggest pitfalls of direct translation, and how do you preserve intent and severity in Chinese? | How would you handle a case where Chinese slang changes the meaning of a seemingly neutral phrase into harassment or sexual content? | What would you do if you strongly disagree with a policy outcome but the guideline is unambiguous—how do you document and proceed? | What is one edge case you’d expect to find in a safety red-teaming pass, and how would you validate it?",
    "source_url": "https://editor.superannotate.com/jobs/ChineseS-25-304"
  },
  {
    "job_id": "PortugueseS-25-305",
    "title": "Portuguese Trust & Safety Data Trainer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Brazil",
    "pay_rate": "$-1 - $25 / hour",
    "pay_min": -1,
    "pay_max": 25,
    "currency": "USD",
    "num_people": 10,
    "priority_level": 3,
    "role_description": "This is a remote, hourly-paid contract role where you will review AI-generated content and safety decisions, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, logical, and clearly explained. You will assess solutions for correctness and clarity, spot methodological or conceptual errors, fact-check where needed, and rate/compare multiple responses based on safety and policy alignment. You must be fluent/proficient (near-native or native) in Portuguese and able to make nuanced judgments across Portuguese and English content. This role is with the fast-growing AI Data Services company SME Careers (a subsidiary of SuperAnnotate), supporting many of the world’s largest AI companies and foundation-model labs. Your annotations on explicit safety tasks will be used to prevent the model from generating unintentional or adversarial, toxic, or unsafe outputs. You may be exposed to content that is sexual, violent, or psychologically disturbing in nature as part of this work to improve the world’s premier AI models.",
    "keywords": "Trust & Safety, Content Moderation, Policy Interpretation, Red Teaming, Risk Assessment, Hate & Harassment, Self-Harm, Misinformation, English, Portuguese",
    "key_responsibilities": "Label and quality-check safety data across categories such as hate/harassment, sexual content, self-harm, violence, bias, illegal goods/services, malicious activities, malicious code, and deliberate misinformation. | Perform red-teaming and adversarial testing by identifying realistic attack patterns, edge cases, and policy gray areas; document rationales and recommend mitigations to reduce unsafe outcomes. | Apply and localize safety policies consistently across Portuguese and English: detect cultural nuance, slang, coded language, and context shifts; escalate uncertainty using documented decision paths.",
    "your_profile": "Bachelor’s degree or higher in a relevant field (e.g., Communications, Linguistics, Psychology, Law/Policy, Security Studies) or equivalent professional experience. | Near-native or native Brazilian Portuguese proficiency (reading/writing) for high-precision safety labeling and cultural-linguistic nuance. | Minimum C1 English proficiency (reading/writing) for policy interpretation, prompt understanding, and consistent documentation. | 5+ years in Trust & Safety, content moderation, policy operations, risk, compliance, investigations, or related safety functions (senior level). | LLM red-teaming / adversarial testing experience is required (documented examples of edge-case discovery and mitigation recommendations). | Localization/translation experience is highly preferred; able to preserve meaning, severity, and intent across languages. | Emotional resilience: comfortable annotating unsafe, explicit, and/or toxic content, including content of a sexual, violent, or psychologically disturbing nature. | Highly detail-oriented with strong judgment, consistency, and ability to follow evolving written guidelines. | Strong analytical writing: concise rationales, clear decision paths, and reproducible reasoning for disagreements. | Secure and confidential handling of sensitive content; reliable remote work practices and time management. | Strong hands-on experience using tools like Perplexity, Gemini, ChatGPT and others.",
    "sample_interview_questions": "When two policies appear to conflict in a single piece of content, how do you decide which policy has precedence and why? | How do you distinguish protected-class hate from general harassment when the target is ambiguous or implied rather than explicit? | What signals help you tell satire or dark humor from genuine hate/harassment in borderline cases? | How would you handle a case where a slur is reclaimed by an in-group speaker—what context must be present to change your decision? | What criteria would you use to separate consensual adult sexual content from coercion or exploitation when details are incomplete? | How do you assess risk and appropriate action for self-harm content that includes ideation but no stated intent or plan? | What are the most common failure modes when classifying violence content, and how do you avoid them? | How would you evaluate content that encourages discrimination indirectly (e.g., dog whistles or coded language) without explicit slurs? | How would you handle misinformation that mixes true facts with a false conclusion—what is your decision logic? | How do you differentiate legitimate political opinion from incitement, harassment, or targeted threats? | What factors help you decide whether instructions are illegal goods/services versus lawful discussion (e.g., academic, journalistic, or harm-reduction framing)? | How do you recognize social engineering attempts (phishing, impersonation, credential harvesting) when the message looks benign on the surface? | How do you classify malicious code requests when code is incomplete, obfuscated, or presented as debugging? | How do you deal with a user request that is dual-use (has legitimate and harmful applications) while staying consistent with policy? | What’s your method for ensuring consistency across a large guideline set when you are labeling at high volume under time pressure? | How do you audit your own decisions for bias, especially when content involves sensitive identities or stereotypes? | In multilingual moderation, what are the biggest pitfalls of direct translation, and how do you preserve intent and severity in Portuguese? | How would you handle a case where Portuguese slang changes the meaning of a seemingly neutral phrase into harassment or sexual content? | What would you do if you strongly disagree with a policy outcome but the guideline is unambiguous—how do you document and proceed? | What is one edge case you’d expect to find in a safety red-teaming pass, and how would you validate it?",
    "source_url": "https://editor.superannotate.com/jobs/PortugueseS-25-305"
  },
  {
    "job_id": "SpanishS-25-306",
    "title": "Spanish Trust & Safety Data Trainer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Spain",
    "pay_rate": "$-1 - $35 / hour",
    "pay_min": -1,
    "pay_max": 35,
    "currency": "USD",
    "num_people": 10,
    "priority_level": 3,
    "role_description": "This is a remote, hourly-paid contract role where you will review AI-generated content and safety decisions, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, logical, and clearly explained. You will assess solutions for correctness and clarity, spot methodological or conceptual errors, fact-check where needed, and rate/compare multiple responses based on safety and policy alignment. You must be fluent/proficient (near-native or native) in Spanish and able to make nuanced judgments across Spanish and English content. This role is with the fast-growing AI Data Services company SME Careers (a subsidiary of SuperAnnotate), supporting many of the world’s largest AI companies and foundation-model labs. Your annotations on explicit safety tasks will be used to prevent the model from generating unintentional or adversarial, toxic, or unsafe outputs. You may be exposed to content that is sexual, violent, or psychologically disturbing in nature as part of this work to improve the world’s premier AI models.",
    "keywords": "Trust & Safety, Content Moderation, Policy Interpretation, Red Teaming, Risk Assessment, Hate & Harassment, Self-Harm, Misinformation, English, Spanish",
    "key_responsibilities": "Label and quality-check safety data across categories such as hate/harassment, sexual content, self-harm, violence, bias, illegal goods/services, malicious activities, malicious code, and deliberate misinformation. | Perform red-teaming and adversarial testing by identifying realistic attack patterns, edge cases, and policy gray areas; document rationales and recommend mitigations to reduce unsafe outcomes. | Apply and localize safety policies consistently across Spanish and English: detect cultural nuance, slang, coded language, and context shifts; escalate uncertainty using documented decision paths.",
    "your_profile": "Bachelor’s degree or higher in a relevant field (e.g., Communications, Linguistics, Psychology, Law/Policy, Security Studies) or equivalent professional experience. | Near-native or native Spanish (ES) proficiency (reading/writing) for high-precision safety labeling and cultural-linguistic nuance. | Minimum C1 English proficiency (reading/writing) for policy interpretation, prompt understanding, and consistent documentation. | 5+ years in Trust & Safety, content moderation, policy operations, risk, compliance, investigations, or related safety functions (senior level). | LLM red-teaming / adversarial testing experience is required (documented examples of edge-case discovery and mitigation recommendations). | Localization/translation experience is highly preferred; able to preserve meaning, severity, and intent across languages. | Emotional resilience: comfortable annotating unsafe, explicit, and/or toxic content, including content of a sexual, violent, or psychologically disturbing nature. | Highly detail-oriented with strong judgment, consistency, and ability to follow evolving written guidelines. | Strong analytical writing: concise rationales, clear decision paths, and reproducible reasoning for disagreements. | Secure and confidential handling of sensitive content; reliable remote work practices and time management. | Strong hands-on experience using tools like Perplexity, Gemini, ChatGPT and others.",
    "sample_interview_questions": "When two policies appear to conflict in a single piece of content, how do you decide which policy has precedence and why? | How do you distinguish protected-class hate from general harassment when the target is ambiguous or implied rather than explicit? | What signals help you tell satire or dark humor from genuine hate/harassment in borderline cases? | How would you handle a case where a slur is reclaimed by an in-group speaker—what context must be present to change your decision? | What criteria would you use to separate consensual adult sexual content from coercion or exploitation when details are incomplete? | How do you assess risk and appropriate action for self-harm content that includes ideation but no stated intent or plan? | What are the most common failure modes when classifying violence content, and how do you avoid them? | How would you evaluate content that encourages discrimination indirectly (e.g., dog whistles or coded language) without explicit slurs? | How would you handle misinformation that mixes true facts with a false conclusion—what is your decision logic? | How do you differentiate legitimate political opinion from incitement, harassment, or targeted threats? | What factors help you decide whether instructions are illegal goods/services versus lawful discussion (e.g., academic, journalistic, or harm-reduction framing)? | How do you recognize social engineering attempts (phishing, impersonation, credential harvesting) when the message looks benign on the surface? | How do you classify malicious code requests when code is incomplete, obfuscated, or presented as debugging? | How do you deal with a user request that is dual-use (has legitimate and harmful applications) while staying consistent with policy? | What’s your method for ensuring consistency across a large guideline set when you are labeling at high volume under time pressure? | How do you audit your own decisions for bias, especially when content involves sensitive identities or stereotypes? | In multilingual moderation, what are the biggest pitfalls of direct translation, and how do you preserve intent and severity in Spanish? | How would you handle a case where Spanish slang changes the meaning of a seemingly neutral phrase into harassment or sexual content? | What would you do if you strongly disagree with a policy outcome but the guideline is unambiguous—how do you document and proceed? | What is one edge case you’d expect to find in a safety red-teaming pass, and how would you validate it?",
    "source_url": "https://editor.superannotate.com/jobs/SpanishS-25-306"
  },
  {
    "job_id": "HindiS-25-308",
    "title": "Hindi Trust & Safety Data Trainer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "India",
    "pay_rate": "$-1 - $15 / hour",
    "pay_min": -1,
    "pay_max": 15,
    "currency": "USD",
    "num_people": 10,
    "priority_level": 3,
    "role_description": "This is a remote, hourly-paid contract role where you will review AI-generated content and safety decisions, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, logical, and clearly explained. You will assess solutions for correctness and clarity, spot methodological or conceptual errors, fact-check where needed, and rate/compare multiple responses based on safety and policy alignment. You must be fluent/proficient (near-native or native) in Hindi and able to make nuanced judgments across Hindi and English content. This role is with the fast-growing AI Data Services company SME Careers (a subsidiary of SuperAnnotate), supporting many of the world’s largest AI companies and foundation-model labs. Your annotations on explicit safety tasks will be used to prevent the model from generating unintentional or adversarial, toxic, or unsafe outputs. You may be exposed to content that is sexual, violent, or psychologically disturbing in nature as part of this work to improve the world’s premier AI models.",
    "keywords": "Trust & Safety, Content Moderation, Policy Interpretation, Red Teaming, Risk Assessment, Hate & Harassment, Self-Harm, Misinformation, English, Hindi",
    "key_responsibilities": "Label and quality-check safety data across categories such as hate/harassment, sexual content, self-harm, violence, bias, illegal goods/services, malicious activities, malicious code, and deliberate misinformation. | Perform red-teaming and adversarial testing by identifying realistic attack patterns, edge cases, and policy gray areas; document rationales and recommend mitigations to reduce unsafe outcomes. | Apply and localize safety policies consistently across Hindi and English: detect cultural nuance, slang, coded language, and context shifts; escalate uncertainty using documented decision paths.",
    "your_profile": "Bachelor’s degree or higher in a relevant field (e.g., Communications, Linguistics, Psychology, Law/Policy, Security Studies) or equivalent professional experience. | Near-native or native Hindi proficiency (reading/writing) for high-precision safety labeling and cultural-linguistic nuance. | Minimum C1 English proficiency (reading/writing) for policy interpretation, prompt understanding, and consistent documentation. | Experience years in Trust & Safety, content moderation, policy operations, risk, compliance, investigations, or related safety functions (senior level). | LLM red-teaming / adversarial testing experience is required (documented examples of edge-case discovery and mitigation recommendations). | Localization/translation experience is highly preferred; able to preserve meaning, severity, and intent across languages. | Emotional resilience: comfortable annotating unsafe, explicit, and/or toxic content, including content of a sexual, violent, or psychologically disturbing nature. | Highly detail-oriented with strong judgment, consistency, and ability to follow evolving written guidelines. | Strong analytical writing: concise rationales, clear decision paths, and reproducible reasoning for disagreements. | Secure and confidential handling of sensitive content; reliable remote work practices and time management. | Strong hands-on experience using tools like Perplexity, Gemini, ChatGPT and others",
    "sample_interview_questions": "When two policies appear to conflict in a single piece of content, how do you decide which policy has precedence and why? | How do you distinguish protected-class hate from general harassment when the target is ambiguous or implied rather than explicit? | What signals help you tell satire or dark humor from genuine hate/harassment in borderline cases? | How would you handle a case where a slur is reclaimed by an in-group speaker—what context must be present to change your decision? | What criteria would you use to separate consensual adult sexual content from coercion or exploitation when details are incomplete? | How do you assess risk and appropriate action for self-harm content that includes ideation but no stated intent or plan? | What are the most common failure modes when classifying violence content, and how do you avoid them? | How would you evaluate content that encourages discrimination indirectly (e.g., dog whistles or coded language) without explicit slurs? | How would you handle misinformation that mixes true facts with a false conclusion—what is your decision logic? | How do you differentiate legitimate political opinion from incitement, harassment, or targeted threats? | What factors help you decide whether instructions are illegal goods/services versus lawful discussion (e.g., academic, journalistic, or harm-reduction framing)? | How do you recognize social engineering attempts (phishing, impersonation, credential harvesting) when the message looks benign on the surface? | How do you classify malicious code requests when code is incomplete, obfuscated, or presented as debugging? | How do you deal with a user request that is dual-use (has legitimate and harmful applications) while staying consistent with policy? | What’s your method for ensuring consistency across a large guideline set when you are labeling at high volume under time pressure? | How do you audit your own decisions for bias, especially when content involves sensitive identities or stereotypes? | In multilingual moderation, what are the biggest pitfalls of direct translation, and how do you preserve intent and severity in Hindi? | How would you handle a case where Hindi slang changes the meaning of a seemingly neutral phrase into harassment or sexual content? | What would you do if you strongly disagree with a policy outcome but the guideline is unambiguous—how do you document and proceed? | What is one edge case you’d expect to find in a safety red-teaming pass, and how would you validate it?",
    "source_url": "https://editor.superannotate.com/jobs/HindiS-25-308"
  },
  {
    "job_id": "ItalianS-25-317",
    "title": "Italian Trust & Safety Data Trainer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Italy",
    "pay_rate": "$-1 - $40 / hour",
    "pay_min": -1,
    "pay_max": 40,
    "currency": "USD",
    "num_people": 10,
    "priority_level": 3,
    "role_description": "This is a remote, hourly-paid contract role where you will review AI-generated content and safety decisions, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, logical, and clearly explained. You will assess solutions for correctness and clarity, spot methodological or conceptual errors, fact-check where needed, and rate/compare multiple responses based on safety and policy alignment. You must be fluent/proficient (near-native or native) in Italian and able to make nuanced judgments across Italian and English content. This role is with the fast-growing AI Data Services company SME Careers (a subsidiary of SuperAnnotate), supporting many of the world’s largest AI companies and foundation-model labs. Your annotations on explicit safety tasks will be used to prevent the model from generating unintentional or adversarial, toxic, or unsafe outputs. You may be exposed to content that is sexual, violent, or psychologically disturbing in nature as part of this work to improve the world’s premier AI models.",
    "keywords": "Trust & Safety, Content Moderation, Policy Interpretation, Red Teaming, Risk Assessment, Hate & Harassment, Self-Harm, Misinformation, English, Italian",
    "key_responsibilities": "Label and quality-check safety data across categories such as hate/harassment, sexual content, self-harm, violence, bias, illegal goods/services, malicious activities, malicious code, and deliberate misinformation. | Perform red-teaming and adversarial testing by identifying realistic attack patterns, edge cases, and policy gray areas; document rationales and recommend mitigations to reduce unsafe outcomes. | Apply and localize safety policies consistently across Italian and English: detect cultural nuance, slang, coded language, and context shifts; escalate uncertainty using documented decision paths.",
    "your_profile": "Bachelor’s degree or higher in a relevant field (e.g., Communications, Linguistics, Psychology, Law/Policy, Security Studies) or equivalent professional experience. | Near-native or native Italian proficiency (reading/writing) for high-precision safety labeling and cultural-linguistic nuance. | Minimum C1 English proficiency (reading/writing) for policy interpretation, prompt understanding, and consistent documentation. | Experience in Trust & Safety, content moderation, policy operations, risk, compliance, investigations, or related safety functions (senior level). | LLM red-teaming / adversarial testing experience is required (documented examples of edge-case discovery and mitigation recommendations). | Localization/translation experience is highly preferred; able to preserve meaning, severity, and intent across languages. | Emotional resilience: comfortable annotating unsafe, explicit, and/or toxic content, including content of a sexual, violent, or psychologically disturbing nature. | Highly detail-oriented with strong judgment, consistency, and ability to follow evolving written guidelines. | Strong analytical writing: concise rationales, clear decision paths, and reproducible reasoning for disagreements. | Secure and confidential handling of sensitive content; reliable remote work practices and time management. | Strong hands-on experience using tools like Perplexity, Gemini, ChatGPT and others.",
    "sample_interview_questions": "When two policies appear to conflict in a single piece of content, how do you decide which policy has precedence and why? | How do you distinguish protected-class hate from general harassment when the target is ambiguous or implied rather than explicit? | What signals help you tell satire or dark humor from genuine hate/harassment in borderline cases? | How would you handle a case where a slur is reclaimed by an in-group speaker—what context must be present to change your decision? | What criteria would you use to separate consensual adult sexual content from coercion or exploitation when details are incomplete? | How do you assess risk and appropriate action for self-harm content that includes ideation but no stated intent or plan? | What are the most common failure modes when classifying violence content, and how do you avoid them? | How would you evaluate content that encourages discrimination indirectly (e.g., dog whistles or coded language) without explicit slurs? | How would you handle misinformation that mixes true facts with a false conclusion—what is your decision logic? | How do you differentiate legitimate political opinion from incitement, harassment, or targeted threats? | What factors help you decide whether instructions are illegal goods/services versus lawful discussion (e.g., academic, journalistic, or harm-reduction framing)? | How do you recognize social engineering attempts (phishing, impersonation, credential harvesting) when the message looks benign on the surface? | How do you classify malicious code requests when code is incomplete, obfuscated, or presented as debugging? | How do you deal with a user request that is dual-use (has legitimate and harmful applications) while staying consistent with policy? | What’s your method for ensuring consistency across a large guideline set when you are labeling at high volume under time pressure? | How do you audit your own decisions for bias, especially when content involves sensitive identities or stereotypes? | In multilingual moderation, what are the biggest pitfalls of direct translation, and how do you preserve intent and severity in Italian? | How would you handle a case where Italian slang changes the meaning of a seemingly neutral phrase into harassment or sexual content? | What would you do if you strongly disagree with a policy outcome but the guideline is unambiguous—how do you document and proceed? | What is one edge case you’d expect to find in a safety red-teaming pass, and how would you validate it?",
    "source_url": "https://editor.superannotate.com/jobs/ItalianS-25-317"
  },
  {
    "job_id": "KoreanS-25-318",
    "title": "Korean Trust & Safety Data Trainer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "South Korea",
    "pay_rate": "$20 - $50 / hour",
    "pay_min": 20,
    "pay_max": 50,
    "currency": "USD",
    "num_people": 10,
    "priority_level": 3,
    "role_description": "This is a remote, hourly-paid contract role where you will review AI-generated content and safety decisions, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, logical, and clearly explained. You will assess solutions for correctness and clarity, spot methodological or conceptual errors, fact-check where needed, and rate/compare multiple responses based on safety and policy alignment. You must be fluent/proficient (near-native or native) in Korean and able to make nuanced judgments across Korean and English content. This role is with the fast-growing AI Data Services company SME Careers (a subsidiary of SuperAnnotate), supporting many of the world’s largest AI companies and foundation-model labs. Your annotations on explicit safety tasks will be used to prevent the model from generating unintentional or adversarial, toxic, or unsafe outputs. You may be exposed to content that is sexual, violent, or psychologically disturbing in nature as part of this work to improve the world’s premier AI models.",
    "keywords": "Trust & Safety, Content Moderation, Policy Interpretation, Red Teaming, Risk Assessment, Hate & Harassment, Self-Harm, Misinformation, English, Korean",
    "key_responsibilities": "Label and quality-check safety data across categories such as hate/harassment, sexual content, self-harm, violence, bias, illegal goods/services, malicious activities, malicious code, and deliberate misinformation. | Perform red-teaming and adversarial testing by identifying realistic attack patterns, edge cases, and policy gray areas; document rationales and recommend mitigations to reduce unsafe outcomes. | Apply and localize safety policies consistently across Korean and English: detect cultural nuance, slang, coded language, and context shifts; escalate uncertainty using documented decision paths.",
    "your_profile": "Bachelor’s degree or higher in a relevant field (e.g., Communications, Linguistics, Psychology, Law/Policy, Security Studies) or equivalent professional experience. | Near-native or native Korean proficiency (reading/writing) for high-precision safety labeling and cultural-linguistic nuance. | Minimum C1 English proficiency (reading/writing) for policy interpretation, prompt understanding, and consistent documentation. | Experience in Trust & Safety, content moderation, policy operations, risk, compliance, investigations, or related safety functions (senior level). | LLM red-teaming / adversarial testing experience is required (documented examples of edge-case discovery and mitigation recommendations). | Localization/translation experience is highly preferred; able to preserve meaning, severity, and intent across languages. | Emotional resilience: comfortable annotating unsafe, explicit, and/or toxic content, including content of a sexual, violent, or psychologically disturbing nature. | Highly detail-oriented with strong judgment, consistency, and ability to follow evolving written guidelines. | Strong analytical writing: concise rationales, clear decision paths, and reproducible reasoning for disagreements. | Secure and confidential handling of sensitive content; reliable remote work practices and time management. | Strong hands-on experience using tools like Perplexity, Gemini, ChatGPT and others",
    "sample_interview_questions": "When two policies appear to conflict in a single piece of content, how do you decide which policy has precedence and why? | How do you distinguish protected-class hate from general harassment when the target is ambiguous or implied rather than explicit? | What signals help you tell satire or dark humor from genuine hate/harassment in borderline cases? | How would you handle a case where a slur is reclaimed by an in-group speaker—what context must be present to change your decision? | What criteria would you use to separate consensual adult sexual content from coercion or exploitation when details are incomplete? | How do you assess risk and appropriate action for self-harm content that includes ideation but no stated intent or plan? | What are the most common failure modes when classifying violence content, and how do you avoid them? | How would you evaluate content that encourages discrimination indirectly (e.g., dog whistles or coded language) without explicit slurs? | How would you handle misinformation that mixes true facts with a false conclusion—what is your decision logic? | How do you differentiate legitimate political opinion from incitement, harassment, or targeted threats? | What factors help you decide whether instructions are illegal goods/services versus lawful discussion (e.g., academic, journalistic, or harm-reduction framing)? | How do you recognize social engineering attempts (phishing, impersonation, credential harvesting) when the message looks benign on the surface? | How do you classify malicious code requests when code is incomplete, obfuscated, or presented as debugging? | How do you deal with a user request that is dual-use (has legitimate and harmful applications) while staying consistent with policy? | What’s your method for ensuring consistency across a large guideline set when you are labeling at high volume under time pressure? | How do you audit your own decisions for bias, especially when content involves sensitive identities or stereotypes? | In multilingual moderation, what are the biggest pitfalls of direct translation, and how do you preserve intent and severity in Korean? | How would you handle a case where Korean slang changes the meaning of a seemingly neutral phrase into harassment or sexual content? | What would you do if you strongly disagree with a policy outcome but the guideline is unambiguous—how do you document and proceed? | What is one edge case you’d expect to find in a safety red-teaming pass, and how would you validate it?",
    "source_url": "https://editor.superannotate.com/jobs/KoreanS-25-318"
  },
  {
    "job_id": "Hebrew-25-842",
    "title": "Hebrew/English Bilingual Expert",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Israel",
    "pay_rate": "$25 - $44 / hour",
    "pay_min": 25,
    "pay_max": 44,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "You will review AI-generated Hebrew and English responses and/or generate high-quality bilingual training content, evaluating reasoning quality and step-by-step problem-solving while providing expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. This is an hourly paid, fully remote contractor role with flexible work-from-anywhere hours. This job is with SME Careers (a subsidiary of SuperAnnotate), a fast-growing AI Data Services company providing training data for many of the world’s largest AI companies and foundation model labs—your work directly helps improve the world’s premier AI models.",
    "keywords": "Hebrew, English, Linguistic quality assurance, Bilingual evaluation, Localization, Terminology management, Fact-checking, Style guide adherence, Cultural nuance, Error taxonomy",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects (Hebrew/English; requires C1+ English). | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance (Hebrew/English; requires C1+ English). | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases (Hebrew/English; requires C1+ English).",
    "your_profile": "Bachelor’s degree (or higher) in Linguistics, Translation, Hebrew Language, Communications, Journalism, or a related field. | Native or near-native Hebrew proficiency with strong writing and editing skills across formal and informal registers. | Minimum C1 English proficiency (reading and writing) for bilingual evaluation and instruction adherence. | 3+ years of professional experience in translation, localization, editorial QA, content quality, or linguistic review (or equivalent). | Excellent attention to detail and ability to apply detailed rubrics consistently in high-volume, hourly contractor work. | Strong fact-checking habits and comfort validating claims using reputable sources when required by task guidelines. | Ability to identify reasoning gaps, methodological errors, and unclear explanations—even when language is fluent. | Comfort working independently in a remote, asynchronous environment with reliable availability and communication. | Prior experience with AI data training, annotation, or evaluation workflows is strongly preferred. | Familiarity with Israeli cultural context and terminology norms across common domains (news, education, consumer, tech) is preferred.",
    "sample_interview_questions": "When reviewing Hebrew text, what signals tell you the output is a literal translation rather than natural Hebrew, and how do you decide whether it is acceptable for the intended audience? | How would you distinguish between a true semantic error and a stylistic preference in Hebrew, and how would that affect your rating decision? | You see an answer mixing formal register with slang in Hebrew. What criteria do you use to judge register consistency, and what is the impact on perceived correctness? | How do you evaluate gender agreement and verb conjugation errors in Hebrew when the source prompt is ambiguous about the subject’s gender? | A response includes correct facts but uses an incorrect binyan or awkward collocation. How do you weigh linguistic naturalness versus factual correctness in scoring? | What is your approach to handling Hebrew spelling variants (ktiv male/ktiv haser) when both could be acceptable depending on the style guide? | How do you identify and categorize fabricated sources or bogus citations in a response, and what thresholds warrant a fail versus a minor issue? | In bilingual evaluation, how do you check for meaning drift between the English prompt and the Hebrew response without re-translating word-for-word? | How do you detect subtle polarity shifts (e.g., negation, hedging) in Hebrew that change the answer’s meaning while still sounding fluent? | If a response uses culturally sensitive terms (ethnicity, religion, politics) in Hebrew, what steps do you take to evaluate neutrality and potential bias while staying aligned to the prompt? | How do you validate named entities in Hebrew (people, places, organizations) when transliterations vary and some have multiple common forms? | You encounter a response that is internally consistent but contradicts a well-known public fact. How do you decide what to trust, and how do you document the issue? | How would you design an error taxonomy for Hebrew language issues (morphology, syntax, register, pragmatics) and use it to improve reviewer consistency? | What are the most common failure modes when AI generates Hebrew punctuation and quotation marks, and when do these issues meaningfully affect clarity? | A Hebrew response is fluent but answers a different question than asked. What checks do you apply to detect prompt misalignment early? | How do you evaluate step-by-step explanations for mathematical or logical problems written in Hebrew to ensure each step follows valid reasoning? | When two responses are both correct, what nuanced criteria do you use to rank them (e.g., completeness, ambiguity handling, caveats, instructional value)? | How do you handle cases where the prompt expects Israeli context but the response uses general/global assumptions that are not wrong but are less relevant? | What techniques do you use to spot subtle contradictions within a multi-paragraph Hebrew answer (e.g., timeline inconsistencies, definition shifts)? | How do you ensure your ratings stay consistent over time across different task types, and what personal QA process do you use to prevent drift?",
    "source_url": "https://editor.superannotate.com/jobs/Hebrew-25-842"
  },
  {
    "job_id": "Arabic-25-563",
    "title": "Arabic/English Bilingual Expert",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Egypt",
    "pay_rate": "$-1 - $15 / hour",
    "pay_min": -1,
    "pay_max": 15,
    "currency": "USD",
    "num_people": 20,
    "priority_level": 1,
    "role_description": "You will review AI-generated Arabic and English responses and/or generate high-quality bilingual training content, evaluating reasoning quality and step-by-step problem-solving while providing expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. This is an hourly paid, fully remote contractor role with flexible work-from-anywhere hours. This job is with SME Careers (a subsidiary of SuperAnnotate), a fast-growing AI Data Services company providing training data for many of the world’s largest AI companies and foundation model labs—your work directly helps improve the world’s premier AI models.",
    "keywords": "Arabic, English, Modern Standard Arabic, Egyptian Arabic, Localization, Linguistic quality assurance, Terminology management, Fact-checking, Style guide adherence",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. (English proficiency required: Minimum C1) | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. (English proficiency required: Minimum C1) | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases. (English proficiency required: Minimum C1)",
    "your_profile": "Bachelor’s degree (or higher) in Linguistics, Translation, Arabic Language, Communications, Journalism, or a related field. | Native or near-native Arabic proficiency with strong writing and editing skills across Modern Standard Arabic and at least one major dialect (Egyptian Arabic preferred). | Minimum C1 English proficiency (reading and writing) for bilingual evaluation and instruction adherence. | 3+ years of professional experience in translation, localization, editorial QA, content quality, or linguistic review (or equivalent). | Excellent attention to detail and ability to apply detailed rubrics consistently in high-volume, hourly contractor work. | Strong fact-checking habits and comfort validating claims using reputable sources when required by task guidelines. | Ability to identify reasoning gaps, methodological errors, and unclear explanations—even when language is fluent. | Comfort working independently in a remote, asynchronous environment with reliable availability and communication. | Prior experience with AI data training, annotation, or evaluation workflows is strongly preferred. | Familiarity with Arabic localization conventions for MENA (terminology, formatting, cultural nuance) is preferred.",
    "sample_interview_questions": "When comparing two Arabic responses that are both grammatical, what signals help you detect subtle meaning drift from the original prompt? | How do you decide whether a response should be in Modern Standard Arabic versus a dialect, when the prompt doesn’t specify audience or context? | What criteria do you use to judge whether an Arabic translation is overly literal versus acceptably faithful to the source meaning? | How do you evaluate register consistency in Arabic (formal vs informal) and determine when a register shift becomes a quality defect? | A response contains correct facts but awkward collocations and unnatural phrasing in Arabic—how do you weigh naturalness versus correctness in scoring? | How do you handle Arabic gender agreement and verb conjugation when the prompt is ambiguous about subject gender or number? | What is your approach to evaluating punctuation, quotation marks, and numerals in Arabic (Arabic-Indic vs Western) for different regional conventions? | How do you validate named entities in Arabic when multiple transliterations exist and some entities have established Arabic exonyms? | What are common failure modes with Arabic diacritics (harakat), and when is missing or incorrect diacritization materially harmful? | How do you detect and categorize common Arabic morphology errors (broken plurals, dual forms, attached pronouns) in a way that improves reviewer consistency? | A response uses culturally sensitive terms (religion, ethnicity, politics). What checks do you apply to evaluate neutrality, bias risk, and respectful language while staying aligned to the prompt? | How do you assess whether an Arabic response properly handles negation, hedging, and modality (e.g., ‘قد’, ‘ربما’, ‘ليس’) without changing the claim strength? | If a response mixes Egyptian Arabic with MSA, what framework do you use to judge whether it improves readability or introduces inconsistency? | How do you verify citations or source references when a response appears to reference publications that may not exist? | How do you distinguish between a factual error and an ambiguity introduced by Arabic pronoun reference (الإحالة) or unclear antecedents? | When evaluating step-by-step reasoning written in Arabic, what signals indicate a logical gap even if the language is fluent? | Two responses answer the prompt correctly; what nuanced criteria do you use to rank them (completeness, assumptions, caveats, clarity, audience fit)? | How do you handle domain terminology conflicts in Arabic (e.g., tech/medical/legal) when multiple translations are used in the region? | What is your method for spotting internal contradictions across paragraphs in Arabic (timeline shifts, definition changes, or inconsistent claims)? | How do you keep your ratings consistent over time across different task types, and what personal QA process do you use to prevent scoring drift?",
    "source_url": "https://editor.superannotate.com/jobs/Arabic-25-563"
  },
  {
    "job_id": "Arabic-25-563",
    "title": "Arabic/English Bilingual Expert",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Lebanon",
    "pay_rate": "$-1 - $15 / hour",
    "pay_min": -1,
    "pay_max": 15,
    "currency": "USD",
    "num_people": 20,
    "priority_level": 1,
    "role_description": "You will review AI-generated Arabic and English responses and/or generate high-quality bilingual training content, evaluating reasoning quality and step-by-step problem-solving while providing expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. This is an hourly paid, fully remote contractor role with flexible work-from-anywhere hours. This job is with SME Careers (a subsidiary of SuperAnnotate), a fast-growing AI Data Services company providing training data for many of the world’s largest AI companies and foundation model labs—your work directly helps improve the world’s premier AI models.",
    "keywords": "Arabic, English, Modern Standard Arabic, Localization, Linguistic quality assurance, Terminology management, Fact-checking, Style guide adherence, Research, Writing",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. (English proficiency required: Minimum C1) | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. (English proficiency required: Minimum C1) | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases. (English proficiency required: Minimum C1)",
    "your_profile": "Bachelor’s degree (or higher) in Linguistics, Translation, Arabic Language, Communications, Journalism, or a related field. | Native or near-native Arabic proficiency with strong writing and editing skills across Modern Standard Arabic and at least one major dialect (Egyptian Arabic preferred). | Minimum C1 English proficiency (reading and writing) for bilingual evaluation and instruction adherence. | 3+ years of professional experience in translation, localization, editorial QA, content quality, or linguistic review (or equivalent). | Excellent attention to detail and ability to apply detailed rubrics consistently in high-volume, hourly contractor work. | Strong fact-checking habits and comfort validating claims using reputable sources when required by task guidelines. | Ability to identify reasoning gaps, methodological errors, and unclear explanations—even when language is fluent. | Comfort working independently in a remote, asynchronous environment with reliable availability and communication. | Prior experience with AI data training, annotation, or evaluation workflows is strongly preferred. | Familiarity with Arabic localization conventions for MENA (terminology, formatting, cultural nuance) is preferred.",
    "sample_interview_questions": "When comparing two Arabic responses that are both grammatical, what signals help you detect subtle meaning drift from the original prompt? | How do you decide whether a response should be in Modern Standard Arabic versus a dialect, when the prompt doesn’t specify audience or context? | What criteria do you use to judge whether an Arabic translation is overly literal versus acceptably faithful to the source meaning? | How do you evaluate register consistency in Arabic (formal vs informal) and determine when a register shift becomes a quality defect? | A response contains correct facts but awkward collocations and unnatural phrasing in Arabic—how do you weigh naturalness versus correctness in scoring? | How do you handle Arabic gender agreement and verb conjugation when the prompt is ambiguous about subject gender or number? | What is your approach to evaluating punctuation, quotation marks, and numerals in Arabic (Arabic-Indic vs Western) for different regional conventions? | How do you validate named entities in Arabic when multiple transliterations exist and some entities have established Arabic exonyms? | What are common failure modes with Arabic diacritics (harakat), and when is missing or incorrect diacritization materially harmful? | How do you detect and categorize common Arabic morphology errors (broken plurals, dual forms, attached pronouns) in a way that improves reviewer consistency? | A response uses culturally sensitive terms (religion, ethnicity, politics). What checks do you apply to evaluate neutrality, bias risk, and respectful language while staying aligned to the prompt? | How do you assess whether an Arabic response properly handles negation, hedging, and modality (e.g., ‘قد’, ‘ربما’, ‘ليس’) without changing the claim strength? | If a response mixes Egyptian Arabic with MSA, what framework do you use to judge whether it improves readability or introduces inconsistency? | How do you verify citations or source references when a response appears to reference publications that may not exist? | How do you distinguish between a factual error and an ambiguity introduced by Arabic pronoun reference (الإحالة) or unclear antecedents? | When evaluating step-by-step reasoning written in Arabic, what signals indicate a logical gap even if the language is fluent? | Two responses answer the prompt correctly; what nuanced criteria do you use to rank them (completeness, assumptions, caveats, clarity, audience fit)? | How do you handle domain terminology conflicts in Arabic (e.g., tech/medical/legal) when multiple translations are used in the region? | What is your method for spotting internal contradictions across paragraphs in Arabic (timeline shifts, definition changes, or inconsistent claims)? | How do you keep your ratings consistent over time across different task types, and what personal QA process do you use to prevent scoring drift?",
    "source_url": "https://editor.superannotate.com/jobs/Arabic-25-563"
  },
  {
    "job_id": "Chinese-25-304",
    "title": "Chinese/English Bilingual Expert",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Singapore",
    "pay_rate": "$15 - $39 / hour",
    "pay_min": 15,
    "pay_max": 39,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "In this hourly-paid, fully remote contractor role, you will review AI-generated responses and/or generate language content, evaluate reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, logical, and clearly explained. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. You will apply senior-level Chinese (Simplified) localization/translation judgment alongside Minimum C1 English proficiency to ensure terminology, tone, and cultural nuance are consistently correct across use cases. This role is with SME Careers, a fast-growing AI Data Services company and a subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation-model labs. Your work directly helps improve the world’s premier AI models.",
    "keywords": "Chinese (Simplified), English, Localization, Translation, Mandarin, error typology, Terminology management, Style guides, CAT tools, Copyediting",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Translation, Linguistics, Localization, Communications, or a related field. | Native or near-native Chinese (Simplified) proficiency and strong command of the locale conventions for China. | Minimum C1 English proficiency (reading/writing) for interpreting prompts, sources, and evaluation guidelines. | 5+ years of professional localization/translation experience (multilingual localization experience strongly preferred). | Demonstrated ability to apply MQM/LQA concepts (severity, category, and root-cause thinking) to produce consistent quality decisions. | Proficiency with CAT tools and QA workflows (translation memory, termbases, automated checks) and comfort working with structured guidelines. | Excellent editorial judgment (register, tone, inclusivity, and cultural nuance) with extreme attention to detail. | Ability to rigorously fact-check localized content (units, references, names, dates) using reliable sources and consistent reasoning. | Comfort working as an hourly contractor: dependable throughput, clear documentation, and responsiveness across time zones. | Previous experience with AI data training / annotation / evaluation is preferred.",
    "sample_interview_questions": "When an English source string is ambiguous, what decision framework do you use to choose the most accurate Chinese (Simplified) interpretation without inventing intent, and what triggers escalation? | How do you separate meaning errors from stylistic preferences in LQA, and how do you keep those judgments consistent across reviewers? | Under an MQM-style rubric, how do you decide severity for: (a) terminology deviation, (b) punctuation/typography, (c) factual inconsistency, (d) tone mismatch? | In China, what are the highest-risk locale conventions (dates, time, numbers, currency, honorifics/formality), and how do you validate them reliably? | A translation is fluent but subtly changes modality (must/should/may) or certainty. What cues help you detect meaning drift quickly? | How do you decide when to translate, transliterate, or retain an English product term, and how do you prevent inconsistent outcomes across a large dataset? | What is your approach to resolving terminology conflicts when legacy translation memory contradicts the current glossary or style guide? | How do you evaluate whether a string is over-localized (added cultural assumptions) versus appropriately adapted for the audience? | How do you handle gendered language and inclusivity requirements when the source is gender-neutral but the target language typically is not? | A localized UI string is accurate but exceeds character limits and breaks layout. What are your preferred mitigation tactics and acceptance criteria? | How do you assess and enforce tone consistency across different surfaces (UI microcopy, help center, marketing) while preserving meaning? | What are common false-friend traps or polysemy pitfalls in your language pair, and what verification habit reduces errors under time pressure? | How do you verify named entities (people, brands, place names) and avoid incorrect localization when multiple conventions exist? | If two reviewers disagree—one calling a result “too literal,” another “too free”—how do you arbitrate using objective standards? | How do you identify when translation memory is propagating systemic errors, and what governance steps do you recommend to stop recurrence? | What is your method for spotting hallucinated facts in otherwise natural-sounding target-language content, especially in high-stakes domains? | In a multilingual release with a tight deadline, how do you triage work using risk-based prioritization and decide what can ship vs. must block? | What leading indicators of localization quality do you trust beyond raw error counts, and how would you use them to improve consistency? | If you find a repeated error pattern across hundreds of items (e.g., one term consistently mistranslated), what root-cause path do you follow? | How do you ensure your own decisions remain stable over time (inter- and intra-rater reliability), especially when guidelines evolve?",
    "source_url": "https://editor.superannotate.com/jobs/Chinese-25-304"
  },
  {
    "job_id": "HebrewS-25-842",
    "title": "Hebrew Trust & Safety Data Trainer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "France",
    "pay_rate": "$-1 - $50 / hour",
    "pay_min": -1,
    "pay_max": 50,
    "currency": "USD",
    "num_people": 10,
    "priority_level": 3,
    "role_description": "In this hourly-paid, fully remote contractor role, you will review AI-generated responses and/or generate safety-focused evaluation content, assess reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, safe, and clearly explained. You will annotate and evaluate content in English and Hebrew; you must be fluent/proficient (near-native or native) in Hebrew and able to apply Minimum C1 English to interpret prompts and policies precisely. Your annotations on these explicit tasks will be used to prevent the Large Language Model from generating unintentional or adversarial, toxic, or unsafe outputs. The types of explicit content you may be exposed to may include, but are not limited to, those of a sexual, violent, or psychologically disturbing nature. This role is with SME Careers, a fast-growing AI Data Services company and a subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation-model labs, helping improve the world’s premier AI models.",
    "keywords": "Hebrew, English, Trust & Safety, Content policy, Red teaming, Hate & harassment, Self-harm safety, Misinformation, Malicious code risk, Bias assessment",
    "key_responsibilities": "Curate and label safety-focused training examples (including adversarial/red-team cases) in English and Hebrew that probe model behavior across hate/harassment, sexual content, self-harm, violence, bias, illegal services, malicious activity, malicious code, and misinformation—capturing nuance and intent with Minimum C1 English and near-native French proficiency. | Review, score, and compare multiple model responses against safety policy and quality rubrics, documenting why an output is safe/unsafe and identifying failure modes such as evasion, normalization, escalation, or procedural enablement. | Continuously stress-test and audit model behavior for policy gaps and edge cases; flag ambiguous scenarios, propose clearer decision rules, and help maintain consistent annotation standards across reviewers.",
    "your_profile": "Bachelor’s degree or higher in a relevant field (e.g., Communications, Linguistics, Psychology, Law/Policy, Security Studies) or equivalent professional experience. | Near-native or native Hebrew proficiency (reading/writing) for high-precision safety labeling and cultural-linguistic nuance. | Minimum C1 English proficiency (reading/writing) for policy interpretation, prompt understanding, and consistent documentation. | Experience in Trust & Safety, content moderation, policy enforcement, risk operations, investigations, or safety evaluation work. | LLM red teaming experience is a must (proven ability to probe safety boundaries and document adversarial patterns). | Strong knowledge of safety domains: Hate & Harassment, Sexual content, Suicide & Self-Harm, Violence, Bias, Illegal goods/services, malicious activities, malicious code, and deliberate misinformation. | Emotional resilience: an understanding that this role requires annotating texts that contain unsafe, explicit, and/or toxic content, including content of a sexual, violent, or psychologically disturbing nature. | Excellent judgment under ambiguity, with the ability to apply written policies consistently and explain decisions succinctly. | Comfort working as an hourly contractor: dependable throughput, clear documentation, and responsiveness across time zones. | Previous experience with AI data training / annotation / evaluation is preferred. | Strong hands-on experience using tools like Perplexity, Gemini, ChatGPT and others.",
    "sample_interview_questions": "In Hebrew, a user uses coded language to demean a protected group without slurs. What signals help you distinguish hate/harassment from general rudeness, and how do you justify the decision consistently? | How do you separate harassment targeting an individual from harassment targeting a protected class when the text contains both, and how do you decide the primary violation category? | When evaluating sexual content, what criteria help you differentiate sexual education, consensual adult sexual content, exploitation, and grooming—especially when the text is euphemistic? | How do you triage self-harm content into (a) ideation, (b) intent, (c) instructions, and (d) recovery/support, and what escalation triggers do you apply for each? | A piece of content depicts violence but is presented as news reporting. What factors determine whether it is allowed, restricted, or disallowed, and how do you handle graphic detail thresholds? | How do you handle content that blends dark humor with suicide/self-harm themes, where user intent is unclear and community risk is high? | What is your approach to identifying bias in model outputs when the harm is subtle (stereotyping, differential treatment, omission) rather than explicit slurs? | A prompt asks for advice that could enable illegal activity (e.g., forging documents) but is framed as fiction or ‘for research.’ What reasoning steps lead to a safe decision? | How do you evaluate ‘dual-use’ requests where legitimate and harmful interpretations are plausible (e.g., chemistry, security, or lock-related questions) without over-blocking? | What are the most common adversarial tactics you’ve seen in LLM red teaming to bypass safety constraints (e.g., roleplay, translation, obfuscation), and how do you categorize them for training data? | How do you judge whether a model response is ‘harmful’ when it contains accurate facts but presents them in a way that increases risk (e.g., operational detail, normalization, glamorization)? | When reviewing misinformation in Hebrew, how do you distinguish satire, opinion, unverified claims, and deliberate deception—especially when ‘sources’ are selectively quoted or misleading? | A model output includes a plausible-sounding medical claim with no source. What verification approach do you use to decide whether it is misinformation, and what safety handling would you apply? | How do you decide the boundary between ‘discussion of extremist ideology’ and ‘promotion/endorsement,’ when the content is subtle and uses dog whistles? | For ‘malicious activities’ requests, what conceptual cues help you identify social engineering attempts, credential theft intent, or scam enablement even without explicit instructions? | A user asks for code that could be used for harm (e.g., a keylogger) but claims benign intent. What factors do you use to classify risk and decide on refusal vs. safe alternative? | How do you evaluate and label prompt-injection attempts against a system instruction, and what annotation fields best capture the attack pattern and severity? | When multiple policy violations appear in one exchange (e.g., hate + threat + doxxing), how do you prioritize labels and severity so downstream training is unambiguous? | What techniques do you use to keep annotation quality high over long sessions with disturbing material (without compromising throughput), and what early warning signs indicate performance drift? | How would you calibrate a team of reviewers to achieve consistent decisions (inter-rater reliability) when policy language is high-level and edge cases are common?",
    "source_url": "https://editor.superannotate.com/jobs/HebrewS-25-842"
  },
  {
    "job_id": "ArabicS-25-319",
    "title": "Arabic Trust & Safety Data Trainer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "France",
    "pay_rate": "$-1 - $50 / hour",
    "pay_min": -1,
    "pay_max": 50,
    "currency": "USD",
    "num_people": 10,
    "priority_level": 3,
    "role_description": "In this hourly-paid, fully remote contractor role, you will review AI-generated responses and/or generate safety-focused evaluation content, assess reasoning quality and step-by-step problem-solving, and provide expert feedback so outputs are accurate, safe, and clearly explained. You will annotate and evaluate content in English and Arabic; you must be fluent/proficient (near-native or native) in Arabic and able to apply Minimum C1 English to interpret prompts and policies precisely. Your annotations on these explicit tasks will be used to prevent the Large Language Model from generating unintentional or adversarial, toxic, or unsafe outputs. The types of explicit content you may be exposed to may include, but are not limited to, those of a sexual, violent, or psychologically disturbing nature. This role is with SME Careers, a fast-growing AI Data Services company and a subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation-model labs, helping improve the world’s premier AI models.",
    "keywords": "Arabic, English, Trust & Safety, Content policy, Red teaming, Hate & harassment, Self-harm safety, Misinformation, Malicious code risk, Bias assessment",
    "key_responsibilities": "Curate and label safety-focused training examples (including adversarial/red-team cases) in English and Arabic that probe model behavior across hate/harassment, sexual content, self-harm, violence, bias, illegal services, malicious activity, malicious code, and misinformation—capturing nuance and intent with Minimum C1 English and near-native French proficiency. | Review, score, and compare multiple model responses against safety policy and quality rubrics, documenting why an output is safe/unsafe and identifying failure modes such as evasion, normalization, escalation, or procedural enablement. | Continuously stress-test and audit model behavior for policy gaps and edge cases; flag ambiguous scenarios, propose clearer decision rules, and help maintain consistent annotation standards across reviewers.",
    "your_profile": "Bachelor’s degree or higher in a relevant field (e.g., Communications, Linguistics, Psychology, Law/Policy, Security Studies) or equivalent professional experience. | Near-native or native Arabic proficiency (reading/writing) for high-precision safety labeling and cultural-linguistic nuance. | Minimum C1 English proficiency (reading/writing) for policy interpretation, prompt understanding, and consistent documentation. | Experience in Trust & Safety, content moderation, policy enforcement, risk operations, investigations, or safety evaluation work. | LLM red teaming experience is a must (proven ability to probe safety boundaries and document adversarial patterns). | Strong knowledge of safety domains: Hate & Harassment, Sexual content, Suicide & Self-Harm, Violence, Bias, Illegal goods/services, malicious activities, malicious code, and deliberate misinformation. | Emotional resilience: an understanding that this role requires annotating texts that contain unsafe, explicit, and/or toxic content, including content of a sexual, violent, or psychologically disturbing nature. | Excellent judgment under ambiguity, with the ability to apply written policies consistently and explain decisions succinctly. | Comfort working as an hourly contractor: dependable throughput, clear documentation, and responsiveness across time zones. | Previous experience with AI data training / annotation / evaluation is preferred. | Strong hands-on experience using tools like Perplexity, Gemini, ChatGPT and others.",
    "sample_interview_questions": "In Arabic, a user uses coded language to demean a protected group without slurs. What signals help you distinguish hate/harassment from general rudeness, and how do you justify the decision consistently? | How do you separate harassment targeting an individual from harassment targeting a protected class when the text contains both, and how do you decide the primary violation category? | When evaluating sexual content, what criteria help you differentiate sexual education, consensual adult sexual content, exploitation, and grooming—especially when the text is euphemistic? | How do you triage self-harm content into (a) ideation, (b) intent, (c) instructions, and (d) recovery/support, and what escalation triggers do you apply for each? | A piece of content depicts violence but is presented as news reporting. What factors determine whether it is allowed, restricted, or disallowed, and how do you handle graphic detail thresholds? | How do you handle content that blends dark humor with suicide/self-harm themes, where user intent is unclear and community risk is high? | What is your approach to identifying bias in model outputs when the harm is subtle (stereotyping, differential treatment, omission) rather than explicit slurs? | A prompt asks for advice that could enable illegal activity (e.g., forging documents) but is framed as fiction or ‘for research.’ What reasoning steps lead to a safe decision? | How do you evaluate ‘dual-use’ requests where legitimate and harmful interpretations are plausible (e.g., chemistry, security, or lock-related questions) without over-blocking? | What are the most common adversarial tactics you’ve seen in LLM red teaming to bypass safety constraints (e.g., roleplay, translation, obfuscation), and how do you categorize them for training data? | How do you judge whether a model response is ‘harmful’ when it contains accurate facts but presents them in a way that increases risk (e.g., operational detail, normalization, glamorization)? | When reviewing misinformation in Arabic, how do you distinguish satire, opinion, unverified claims, and deliberate deception—especially when ‘sources’ are selectively quoted or misleading? | A model output includes a plausible-sounding medical claim with no source. What verification approach do you use to decide whether it is misinformation, and what safety handling would you apply? | How do you decide the boundary between ‘discussion of extremist ideology’ and ‘promotion/endorsement,’ when the content is subtle and uses dog whistles? | For ‘malicious activities’ requests, what conceptual cues help you identify social engineering attempts, credential theft intent, or scam enablement even without explicit instructions? | A user asks for code that could be used for harm (e.g., a keylogger) but claims benign intent. What factors do you use to classify risk and decide on refusal vs. safe alternative? | How do you evaluate and label prompt-injection attempts against a system instruction, and what annotation fields best capture the attack pattern and severity? | When multiple policy violations appear in one exchange (e.g., hate + threat + doxxing), how do you prioritize labels and severity so downstream training is unambiguous? | What techniques do you use to keep annotation quality high over long sessions with disturbing material (without compromising throughput), and what early warning signs indicate performance drift? | How would you calibrate a team of reviewers to achieve consistent decisions (inter-rater reliability) when policy language is high-level and edge cases are common?",
    "source_url": "https://editor.superannotate.com/jobs/ArabicS-25-319"
  },
  {
    "job_id": "Mechanical-25-777",
    "title": "Mechanical Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Bangladesh",
    "pay_rate": "$-1 - $22 / hour",
    "pay_min": -1,
    "pay_max": 22,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated engineering solutions and/or generate expert Mechanical Engineering content, evaluating reasoning quality and step-by-step problem-solving while providing crisp written feedback. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify methodological or conceptual errors; fact-check assumptions; write high-quality explanations and model solutions; and rate and compare multiple responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate, delivering training data for many of the world’s largest AI companies and foundation-model labs. Your mechanical engineering expertise directly helps improve the world’s premier AI models by making their technical reasoning more accurate, reliable, and clearly explained.",
    "keywords": "Mechanical Engineering, Thermodynamics, Heat Transfer, Fluid Mechanics, Statics, Dynamics, Differential Equations, System Modeling, Design Principles, English (C1+)",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Mechanical Engineering, Aerospace Engineering, or Engineering Physics (Thermo/Heat Transfer/Fluids/Statics-Dynamics/Differential Equations). | Strong, confident fundamentals in mechanics, thermodynamics, fluid mechanics, applied physics, engineering math, and system modeling. | Comfortable evaluating technical solutions for correctness, methodology, units/dimensional consistency, assumptions, and physical plausibility with high attention to detail. | Able to clearly explain reasoning and corrections in writing; Minimum C1 English proficiency. | Experience applying basic design principles (requirements, constraints, margins, trade-offs) in real engineering contexts. | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation or technical reviewing is strongly preferred.",
    "sample_interview_questions": "A transient conduction model predicts faster cool-down than observed. Without re-running anything, what hierarchy of checks do you apply (units, properties, boundary conditions, contact resistances, radiation, internal generation) to identify the most likely root cause? | In a compressible duct with heat addition (Rayleigh flow), what qualitative trends in Mach number and stagnation temperature would you expect as heat is added in subsonic vs supersonic regimes, and how can those expectations catch a flawed solution? | A CFD report shows turbulent pipe flow with y+ ~ 200 but claims wall-resolved accuracy. What questions would you ask to determine whether the wall treatment is appropriate and whether friction factor/ΔP should be trusted? | You see an analysis that uses constant viscosity and density for a thermal-fluid problem with large temperature rise. What dimensionless groups or sensitivity arguments would you use to judge whether variable-property effects materially change the conclusion? | A mechanism’s static equilibrium solution gives plausible reactions, but its predicted actuator torque is far too low. What are the most common modeling omissions (friction, moment arms, constraint forces, gravity distribution) that create this specific symptom? | A rotor-bearing system shows a critical speed near the operating range. What evidence would make you model gyroscopic effects and cross-coupled stiffness, and what failure modes do those effects tend to introduce if ignored? | An engineer estimates convective coefficients using a textbook correlation outside its stated range (Re, Pr, geometry). What ‘red flags’ tell you extrapolation is unsafe, and what alternative bounding approach would you use to sanity-check results? | A first-principles system model of a closed volume produces negative absolute pressure during a rapid valve event. What does that imply about compressibility assumptions, numerical integration stability, or state variable choice? | A lumped-capacitance model passes a Biot-number check, yet measured temperature gradients persist. What physical mechanisms (internal heat generation distribution, anisotropy, contact resistance, phase change, radiation) can invalidate the practical lumped assumption? | Two engineers disagree on whether buoyancy matters in a forced-convection enclosure flow. What scaling analysis (e.g., Richardson/Grashof vs Reynolds) would you apply, and how would you interpret borderline values? | A thin-wall pressure vessel calculation looks fine globally, but a nozzle penetration is present. What specific stress components and local effects would you expect to dominate near the discontinuity, and what would you look for in a credible solution narrative? | A statics solution ‘works’ numerically, but the structure exhibits unexpected motion in reality. What stability checks distinguish a solvable equilibrium set from an actually stable configuration (mechanism vs structure)? | In a boiling heat transfer loop, outlet quality and wall superheat rise sharply at similar heat flux. What tells you the system is approaching CHF versus experiencing flow instabilities or dryout due to maldistribution? | A finite element result shows extremely high stress at a sharp re-entrant corner; the mesh refinement increases the peak without convergence. How do you distinguish a mathematical singularity from a design-critical hotspot, and what quantity do you report instead? | A dynamic model linearized at an operating point matches small-signal response but fails under moderate step inputs. What features (nonlinear friction, saturation, regime switching, property variation) would you suspect first, and how would they manifest in residuals? | Given multiaxial, nonproportional loading on a ductile part, what limitations of von Mises/Tresca would you consider, and what additional evidence would you seek before accepting a single scalar margin? | Pump + system curves predict an operating point, but field measurements show higher head at lower flow. What hidden assumptions (NPSH, cavitation, instrumentation bias, viscosity correction, recirculation, control valve characteristics) are most diagnostic to check? | A radiation model assumes gray, diffuse surfaces and uses view factors, yet predicts net heat flow in the wrong direction. What quick consistency checks (energy balance, reciprocity, enclosure constraints, emissivity bounds) can reveal the modeling error? | When reviewing an engineering solution, what are your top three ‘physics sanity tests’ (order-of-magnitude bounds, limiting cases, conservation checks) and how do they change across fluids, thermo, and dynamics problems? | You’re comparing two solution approaches: one uses aggressive simplifying assumptions but is elegantly consistent; the other is more general but internally messy. What criteria do you use to judge correctness, robustness across edge cases, and decision risk?",
    "source_url": "https://editor.superannotate.com/jobs/Mechanical-25-777"
  },
  {
    "job_id": "Mechanical-25-777",
    "title": "Mechanical Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "India",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated engineering solutions and/or generate expert Mechanical Engineering content, evaluating reasoning quality and step-by-step problem-solving while providing crisp written feedback. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify methodological or conceptual errors; fact-check assumptions; write high-quality explanations and model solutions; and rate and compare multiple responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate, delivering training data for many of the world’s largest AI companies and foundation-model labs. Your mechanical engineering expertise directly helps improve the world’s premier AI models by making their technical reasoning more accurate, reliable, and clearly explained.",
    "keywords": "Mechanical Engineering, Thermodynamics, Heat Transfer, Fluid Mechanics, Statics, Dynamics, Differential Equations, System Modeling, Design Principles, English (C1+)",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Mechanical Engineering, Aerospace Engineering, or Engineering Physics (Thermo/Heat Transfer/Fluids/Statics-Dynamics/Differential Equations). | Strong, confident fundamentals in mechanics, thermodynamics, fluid mechanics, applied physics, engineering math, and system modeling. | Comfortable evaluating technical solutions for correctness, methodology, units/dimensional consistency, assumptions, and physical plausibility with high attention to detail. | Able to clearly explain reasoning and corrections in writing; Minimum C1 English proficiency. | Experience applying basic design principles (requirements, constraints, margins, trade-offs) in real engineering contexts. | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation or technical reviewing is strongly preferred.",
    "sample_interview_questions": "A transient conduction model predicts faster cool-down than observed. Without re-running anything, what hierarchy of checks do you apply (units, properties, boundary conditions, contact resistances, radiation, internal generation) to identify the most likely root cause? | In a compressible duct with heat addition (Rayleigh flow), what qualitative trends in Mach number and stagnation temperature would you expect as heat is added in subsonic vs supersonic regimes, and how can those expectations catch a flawed solution? | A CFD report shows turbulent pipe flow with y+ ~ 200 but claims wall-resolved accuracy. What questions would you ask to determine whether the wall treatment is appropriate and whether friction factor/ΔP should be trusted? | You see an analysis that uses constant viscosity and density for a thermal-fluid problem with large temperature rise. What dimensionless groups or sensitivity arguments would you use to judge whether variable-property effects materially change the conclusion? | A mechanism’s static equilibrium solution gives plausible reactions, but its predicted actuator torque is far too low. What are the most common modeling omissions (friction, moment arms, constraint forces, gravity distribution) that create this specific symptom? | A rotor-bearing system shows a critical speed near the operating range. What evidence would make you model gyroscopic effects and cross-coupled stiffness, and what failure modes do those effects tend to introduce if ignored? | An engineer estimates convective coefficients using a textbook correlation outside its stated range (Re, Pr, geometry). What ‘red flags’ tell you extrapolation is unsafe, and what alternative bounding approach would you use to sanity-check results? | A first-principles system model of a closed volume produces negative absolute pressure during a rapid valve event. What does that imply about compressibility assumptions, numerical integration stability, or state variable choice? | A lumped-capacitance model passes a Biot-number check, yet measured temperature gradients persist. What physical mechanisms (internal heat generation distribution, anisotropy, contact resistance, phase change, radiation) can invalidate the practical lumped assumption? | Two engineers disagree on whether buoyancy matters in a forced-convection enclosure flow. What scaling analysis (e.g., Richardson/Grashof vs Reynolds) would you apply, and how would you interpret borderline values? | A thin-wall pressure vessel calculation looks fine globally, but a nozzle penetration is present. What specific stress components and local effects would you expect to dominate near the discontinuity, and what would you look for in a credible solution narrative? | A statics solution ‘works’ numerically, but the structure exhibits unexpected motion in reality. What stability checks distinguish a solvable equilibrium set from an actually stable configuration (mechanism vs structure)? | In a boiling heat transfer loop, outlet quality and wall superheat rise sharply at similar heat flux. What tells you the system is approaching CHF versus experiencing flow instabilities or dryout due to maldistribution? | A finite element result shows extremely high stress at a sharp re-entrant corner; the mesh refinement increases the peak without convergence. How do you distinguish a mathematical singularity from a design-critical hotspot, and what quantity do you report instead? | A dynamic model linearized at an operating point matches small-signal response but fails under moderate step inputs. What features (nonlinear friction, saturation, regime switching, property variation) would you suspect first, and how would they manifest in residuals? | Given multiaxial, nonproportional loading on a ductile part, what limitations of von Mises/Tresca would you consider, and what additional evidence would you seek before accepting a single scalar margin? | Pump + system curves predict an operating point, but field measurements show higher head at lower flow. What hidden assumptions (NPSH, cavitation, instrumentation bias, viscosity correction, recirculation, control valve characteristics) are most diagnostic to check? | A radiation model assumes gray, diffuse surfaces and uses view factors, yet predicts net heat flow in the wrong direction. What quick consistency checks (energy balance, reciprocity, enclosure constraints, emissivity bounds) can reveal the modeling error? | When reviewing an engineering solution, what are your top three ‘physics sanity tests’ (order-of-magnitude bounds, limiting cases, conservation checks) and how do they change across fluids, thermo, and dynamics problems? | You’re comparing two solution approaches: one uses aggressive simplifying assumptions but is elegantly consistent; the other is more general but internally messy. What criteria do you use to judge correctness, robustness across edge cases, and decision risk?",
    "source_url": "https://editor.superannotate.com/jobs/Mechanical-25-777"
  },
  {
    "job_id": "Mechanical-25-777",
    "title": "Mechanical Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Indonesia",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated engineering solutions and/or generate expert Mechanical Engineering content, evaluating reasoning quality and step-by-step problem-solving while providing crisp written feedback. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify methodological or conceptual errors; fact-check assumptions; write high-quality explanations and model solutions; and rate and compare multiple responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate, delivering training data for many of the world’s largest AI companies and foundation-model labs. Your mechanical engineering expertise directly helps improve the world’s premier AI models by making their technical reasoning more accurate, reliable, and clearly explained.",
    "keywords": "Mechanical Engineering, Thermodynamics, Heat Transfer, Fluid Mechanics, Statics, Dynamics, Differential Equations, System Modeling, Design Principles, English (C1+)",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Mechanical Engineering, Aerospace Engineering, or Engineering Physics (Thermo/Heat Transfer/Fluids/Statics-Dynamics/Differential Equations). | Strong, confident fundamentals in mechanics, thermodynamics, fluid mechanics, applied physics, engineering math, and system modeling. | Comfortable evaluating technical solutions for correctness, methodology, units/dimensional consistency, assumptions, and physical plausibility with high attention to detail. | Able to clearly explain reasoning and corrections in writing; Minimum C1 English proficiency. | Experience applying basic design principles (requirements, constraints, margins, trade-offs) in real engineering contexts. | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation or technical reviewing is strongly preferred.",
    "sample_interview_questions": "A transient conduction model predicts faster cool-down than observed. Without re-running anything, what hierarchy of checks do you apply (units, properties, boundary conditions, contact resistances, radiation, internal generation) to identify the most likely root cause? | In a compressible duct with heat addition (Rayleigh flow), what qualitative trends in Mach number and stagnation temperature would you expect as heat is added in subsonic vs supersonic regimes, and how can those expectations catch a flawed solution? | A CFD report shows turbulent pipe flow with y+ ~ 200 but claims wall-resolved accuracy. What questions would you ask to determine whether the wall treatment is appropriate and whether friction factor/ΔP should be trusted? | You see an analysis that uses constant viscosity and density for a thermal-fluid problem with large temperature rise. What dimensionless groups or sensitivity arguments would you use to judge whether variable-property effects materially change the conclusion? | A mechanism’s static equilibrium solution gives plausible reactions, but its predicted actuator torque is far too low. What are the most common modeling omissions (friction, moment arms, constraint forces, gravity distribution) that create this specific symptom? | A rotor-bearing system shows a critical speed near the operating range. What evidence would make you model gyroscopic effects and cross-coupled stiffness, and what failure modes do those effects tend to introduce if ignored? | An engineer estimates convective coefficients using a textbook correlation outside its stated range (Re, Pr, geometry). What ‘red flags’ tell you extrapolation is unsafe, and what alternative bounding approach would you use to sanity-check results? | A first-principles system model of a closed volume produces negative absolute pressure during a rapid valve event. What does that imply about compressibility assumptions, numerical integration stability, or state variable choice? | A lumped-capacitance model passes a Biot-number check, yet measured temperature gradients persist. What physical mechanisms (internal heat generation distribution, anisotropy, contact resistance, phase change, radiation) can invalidate the practical lumped assumption? | Two engineers disagree on whether buoyancy matters in a forced-convection enclosure flow. What scaling analysis (e.g., Richardson/Grashof vs Reynolds) would you apply, and how would you interpret borderline values? | A thin-wall pressure vessel calculation looks fine globally, but a nozzle penetration is present. What specific stress components and local effects would you expect to dominate near the discontinuity, and what would you look for in a credible solution narrative? | A statics solution ‘works’ numerically, but the structure exhibits unexpected motion in reality. What stability checks distinguish a solvable equilibrium set from an actually stable configuration (mechanism vs structure)? | In a boiling heat transfer loop, outlet quality and wall superheat rise sharply at similar heat flux. What tells you the system is approaching CHF versus experiencing flow instabilities or dryout due to maldistribution? | A finite element result shows extremely high stress at a sharp re-entrant corner; the mesh refinement increases the peak without convergence. How do you distinguish a mathematical singularity from a design-critical hotspot, and what quantity do you report instead? | A dynamic model linearized at an operating point matches small-signal response but fails under moderate step inputs. What features (nonlinear friction, saturation, regime switching, property variation) would you suspect first, and how would they manifest in residuals? | Given multiaxial, nonproportional loading on a ductile part, what limitations of von Mises/Tresca would you consider, and what additional evidence would you seek before accepting a single scalar margin? | Pump + system curves predict an operating point, but field measurements show higher head at lower flow. What hidden assumptions (NPSH, cavitation, instrumentation bias, viscosity correction, recirculation, control valve characteristics) are most diagnostic to check? | A radiation model assumes gray, diffuse surfaces and uses view factors, yet predicts net heat flow in the wrong direction. What quick consistency checks (energy balance, reciprocity, enclosure constraints, emissivity bounds) can reveal the modeling error? | When reviewing an engineering solution, what are your top three ‘physics sanity tests’ (order-of-magnitude bounds, limiting cases, conservation checks) and how do they change across fluids, thermo, and dynamics problems? | You’re comparing two solution approaches: one uses aggressive simplifying assumptions but is elegantly consistent; the other is more general but internally messy. What criteria do you use to judge correctness, robustness across edge cases, and decision risk?",
    "source_url": "https://editor.superannotate.com/jobs/Mechanical-25-777"
  },
  {
    "job_id": "Mechanical-25-777",
    "title": "Mechanical Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Egypt",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated engineering solutions and/or generate expert Mechanical Engineering content, evaluating reasoning quality and step-by-step problem-solving while providing crisp written feedback. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify methodological or conceptual errors; fact-check assumptions; write high-quality explanations and model solutions; and rate and compare multiple responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate, delivering training data for many of the world’s largest AI companies and foundation-model labs. Your mechanical engineering expertise directly helps improve the world’s premier AI models by making their technical reasoning more accurate, reliable, and clearly explained.",
    "keywords": "Mechanical Engineering, Thermodynamics, Heat Transfer, Fluid Mechanics, Statics, Dynamics, Differential Equations, System Modeling, Design Principles, English (C1+)",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Mechanical Engineering, Aerospace Engineering, or Engineering Physics (Thermo/Heat Transfer/Fluids/Statics-Dynamics/Differential Equations). | Strong, confident fundamentals in mechanics, thermodynamics, fluid mechanics, applied physics, engineering math, and system modeling. | Comfortable evaluating technical solutions for correctness, methodology, units/dimensional consistency, assumptions, and physical plausibility with high attention to detail. | Able to clearly explain reasoning and corrections in writing; Minimum C1 English proficiency. | Experience applying basic design principles (requirements, constraints, margins, trade-offs) in real engineering contexts. | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation or technical reviewing is strongly preferred.",
    "sample_interview_questions": "A transient conduction model predicts faster cool-down than observed. Without re-running anything, what hierarchy of checks do you apply (units, properties, boundary conditions, contact resistances, radiation, internal generation) to identify the most likely root cause? | In a compressible duct with heat addition (Rayleigh flow), what qualitative trends in Mach number and stagnation temperature would you expect as heat is added in subsonic vs supersonic regimes, and how can those expectations catch a flawed solution? | A CFD report shows turbulent pipe flow with y+ ~ 200 but claims wall-resolved accuracy. What questions would you ask to determine whether the wall treatment is appropriate and whether friction factor/ΔP should be trusted? | You see an analysis that uses constant viscosity and density for a thermal-fluid problem with large temperature rise. What dimensionless groups or sensitivity arguments would you use to judge whether variable-property effects materially change the conclusion? | A mechanism’s static equilibrium solution gives plausible reactions, but its predicted actuator torque is far too low. What are the most common modeling omissions (friction, moment arms, constraint forces, gravity distribution) that create this specific symptom? | A rotor-bearing system shows a critical speed near the operating range. What evidence would make you model gyroscopic effects and cross-coupled stiffness, and what failure modes do those effects tend to introduce if ignored? | An engineer estimates convective coefficients using a textbook correlation outside its stated range (Re, Pr, geometry). What ‘red flags’ tell you extrapolation is unsafe, and what alternative bounding approach would you use to sanity-check results? | A first-principles system model of a closed volume produces negative absolute pressure during a rapid valve event. What does that imply about compressibility assumptions, numerical integration stability, or state variable choice? | A lumped-capacitance model passes a Biot-number check, yet measured temperature gradients persist. What physical mechanisms (internal heat generation distribution, anisotropy, contact resistance, phase change, radiation) can invalidate the practical lumped assumption? | Two engineers disagree on whether buoyancy matters in a forced-convection enclosure flow. What scaling analysis (e.g., Richardson/Grashof vs Reynolds) would you apply, and how would you interpret borderline values? | A thin-wall pressure vessel calculation looks fine globally, but a nozzle penetration is present. What specific stress components and local effects would you expect to dominate near the discontinuity, and what would you look for in a credible solution narrative? | A statics solution ‘works’ numerically, but the structure exhibits unexpected motion in reality. What stability checks distinguish a solvable equilibrium set from an actually stable configuration (mechanism vs structure)? | In a boiling heat transfer loop, outlet quality and wall superheat rise sharply at similar heat flux. What tells you the system is approaching CHF versus experiencing flow instabilities or dryout due to maldistribution? | A finite element result shows extremely high stress at a sharp re-entrant corner; the mesh refinement increases the peak without convergence. How do you distinguish a mathematical singularity from a design-critical hotspot, and what quantity do you report instead? | A dynamic model linearized at an operating point matches small-signal response but fails under moderate step inputs. What features (nonlinear friction, saturation, regime switching, property variation) would you suspect first, and how would they manifest in residuals? | Given multiaxial, nonproportional loading on a ductile part, what limitations of von Mises/Tresca would you consider, and what additional evidence would you seek before accepting a single scalar margin? | Pump + system curves predict an operating point, but field measurements show higher head at lower flow. What hidden assumptions (NPSH, cavitation, instrumentation bias, viscosity correction, recirculation, control valve characteristics) are most diagnostic to check? | A radiation model assumes gray, diffuse surfaces and uses view factors, yet predicts net heat flow in the wrong direction. What quick consistency checks (energy balance, reciprocity, enclosure constraints, emissivity bounds) can reveal the modeling error? | When reviewing an engineering solution, what are your top three ‘physics sanity tests’ (order-of-magnitude bounds, limiting cases, conservation checks) and how do they change across fluids, thermo, and dynamics problems? | You’re comparing two solution approaches: one uses aggressive simplifying assumptions but is elegantly consistent; the other is more general but internally messy. What criteria do you use to judge correctness, robustness across edge cases, and decision risk?",
    "source_url": "https://editor.superannotate.com/jobs/Mechanical-25-777"
  },
  {
    "job_id": "Mechanical-25-777",
    "title": "Mechanical Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Brazil",
    "pay_rate": "$-1 - $48 / hour",
    "pay_min": -1,
    "pay_max": 48,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated engineering solutions and/or generate expert Mechanical Engineering content, evaluating reasoning quality and step-by-step problem-solving while providing crisp written feedback. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify methodological or conceptual errors; fact-check assumptions; write high-quality explanations and model solutions; and rate and compare multiple responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate, delivering training data for many of the world’s largest AI companies and foundation-model labs. Your mechanical engineering expertise directly helps improve the world’s premier AI models by making their technical reasoning more accurate, reliable, and clearly explained.",
    "keywords": "Mechanical Engineering, Thermodynamics, Heat Transfer, Fluid Mechanics, Statics, Dynamics, Differential Equations, System Modeling, Design Principles, English (C1+)",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Mechanical Engineering, Aerospace Engineering, or Engineering Physics (Thermo/Heat Transfer/Fluids/Statics-Dynamics/Differential Equations). | Strong, confident fundamentals in mechanics, thermodynamics, fluid mechanics, applied physics, engineering math, and system modeling. | Comfortable evaluating technical solutions for correctness, methodology, units/dimensional consistency, assumptions, and physical plausibility with high attention to detail. | Able to clearly explain reasoning and corrections in writing; Minimum C1 English proficiency. | Experience applying basic design principles (requirements, constraints, margins, trade-offs) in real engineering contexts. | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation or technical reviewing is strongly preferred.",
    "sample_interview_questions": "A transient conduction model predicts faster cool-down than observed. Without re-running anything, what hierarchy of checks do you apply (units, properties, boundary conditions, contact resistances, radiation, internal generation) to identify the most likely root cause? | In a compressible duct with heat addition (Rayleigh flow), what qualitative trends in Mach number and stagnation temperature would you expect as heat is added in subsonic vs supersonic regimes, and how can those expectations catch a flawed solution? | A CFD report shows turbulent pipe flow with y+ ~ 200 but claims wall-resolved accuracy. What questions would you ask to determine whether the wall treatment is appropriate and whether friction factor/ΔP should be trusted? | You see an analysis that uses constant viscosity and density for a thermal-fluid problem with large temperature rise. What dimensionless groups or sensitivity arguments would you use to judge whether variable-property effects materially change the conclusion? | A mechanism’s static equilibrium solution gives plausible reactions, but its predicted actuator torque is far too low. What are the most common modeling omissions (friction, moment arms, constraint forces, gravity distribution) that create this specific symptom? | A rotor-bearing system shows a critical speed near the operating range. What evidence would make you model gyroscopic effects and cross-coupled stiffness, and what failure modes do those effects tend to introduce if ignored? | An engineer estimates convective coefficients using a textbook correlation outside its stated range (Re, Pr, geometry). What ‘red flags’ tell you extrapolation is unsafe, and what alternative bounding approach would you use to sanity-check results? | A first-principles system model of a closed volume produces negative absolute pressure during a rapid valve event. What does that imply about compressibility assumptions, numerical integration stability, or state variable choice? | A lumped-capacitance model passes a Biot-number check, yet measured temperature gradients persist. What physical mechanisms (internal heat generation distribution, anisotropy, contact resistance, phase change, radiation) can invalidate the practical lumped assumption? | Two engineers disagree on whether buoyancy matters in a forced-convection enclosure flow. What scaling analysis (e.g., Richardson/Grashof vs Reynolds) would you apply, and how would you interpret borderline values? | A thin-wall pressure vessel calculation looks fine globally, but a nozzle penetration is present. What specific stress components and local effects would you expect to dominate near the discontinuity, and what would you look for in a credible solution narrative? | A statics solution ‘works’ numerically, but the structure exhibits unexpected motion in reality. What stability checks distinguish a solvable equilibrium set from an actually stable configuration (mechanism vs structure)? | In a boiling heat transfer loop, outlet quality and wall superheat rise sharply at similar heat flux. What tells you the system is approaching CHF versus experiencing flow instabilities or dryout due to maldistribution? | A finite element result shows extremely high stress at a sharp re-entrant corner; the mesh refinement increases the peak without convergence. How do you distinguish a mathematical singularity from a design-critical hotspot, and what quantity do you report instead? | A dynamic model linearized at an operating point matches small-signal response but fails under moderate step inputs. What features (nonlinear friction, saturation, regime switching, property variation) would you suspect first, and how would they manifest in residuals? | Given multiaxial, nonproportional loading on a ductile part, what limitations of von Mises/Tresca would you consider, and what additional evidence would you seek before accepting a single scalar margin? | Pump + system curves predict an operating point, but field measurements show higher head at lower flow. What hidden assumptions (NPSH, cavitation, instrumentation bias, viscosity correction, recirculation, control valve characteristics) are most diagnostic to check? | A radiation model assumes gray, diffuse surfaces and uses view factors, yet predicts net heat flow in the wrong direction. What quick consistency checks (energy balance, reciprocity, enclosure constraints, emissivity bounds) can reveal the modeling error? | When reviewing an engineering solution, what are your top three ‘physics sanity tests’ (order-of-magnitude bounds, limiting cases, conservation checks) and how do they change across fluids, thermo, and dynamics problems? | You’re comparing two solution approaches: one uses aggressive simplifying assumptions but is elegantly consistent; the other is more general but internally messy. What criteria do you use to judge correctness, robustness across edge cases, and decision risk?",
    "source_url": "https://editor.superannotate.com/jobs/Mechanical-25-777"
  },
  {
    "job_id": "Mechanical-25-777",
    "title": "Mechanical Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Portugal",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated engineering solutions and/or generate expert Mechanical Engineering content, evaluating reasoning quality and step-by-step problem-solving while providing crisp written feedback. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify methodological or conceptual errors; fact-check assumptions; write high-quality explanations and model solutions; and rate and compare multiple responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate, delivering training data for many of the world’s largest AI companies and foundation-model labs. Your mechanical engineering expertise directly helps improve the world’s premier AI models by making their technical reasoning more accurate, reliable, and clearly explained.",
    "keywords": "Mechanical Engineering, Thermodynamics, Heat Transfer, Fluid Mechanics, Statics, Dynamics, Differential Equations, System Modeling, Design Principles, English (C1+)",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Mechanical Engineering, Aerospace Engineering, or Engineering Physics (Thermo/Heat Transfer/Fluids/Statics-Dynamics/Differential Equations). | Strong, confident fundamentals in mechanics, thermodynamics, fluid mechanics, applied physics, engineering math, and system modeling. | Comfortable evaluating technical solutions for correctness, methodology, units/dimensional consistency, assumptions, and physical plausibility with high attention to detail. | Able to clearly explain reasoning and corrections in writing; Minimum C1 English proficiency. | Experience applying basic design principles (requirements, constraints, margins, trade-offs) in real engineering contexts. | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation or technical reviewing is strongly preferred.",
    "sample_interview_questions": "A transient conduction model predicts faster cool-down than observed. Without re-running anything, what hierarchy of checks do you apply (units, properties, boundary conditions, contact resistances, radiation, internal generation) to identify the most likely root cause? | In a compressible duct with heat addition (Rayleigh flow), what qualitative trends in Mach number and stagnation temperature would you expect as heat is added in subsonic vs supersonic regimes, and how can those expectations catch a flawed solution? | A CFD report shows turbulent pipe flow with y+ ~ 200 but claims wall-resolved accuracy. What questions would you ask to determine whether the wall treatment is appropriate and whether friction factor/ΔP should be trusted? | You see an analysis that uses constant viscosity and density for a thermal-fluid problem with large temperature rise. What dimensionless groups or sensitivity arguments would you use to judge whether variable-property effects materially change the conclusion? | A mechanism’s static equilibrium solution gives plausible reactions, but its predicted actuator torque is far too low. What are the most common modeling omissions (friction, moment arms, constraint forces, gravity distribution) that create this specific symptom? | A rotor-bearing system shows a critical speed near the operating range. What evidence would make you model gyroscopic effects and cross-coupled stiffness, and what failure modes do those effects tend to introduce if ignored? | An engineer estimates convective coefficients using a textbook correlation outside its stated range (Re, Pr, geometry). What ‘red flags’ tell you extrapolation is unsafe, and what alternative bounding approach would you use to sanity-check results? | A first-principles system model of a closed volume produces negative absolute pressure during a rapid valve event. What does that imply about compressibility assumptions, numerical integration stability, or state variable choice? | A lumped-capacitance model passes a Biot-number check, yet measured temperature gradients persist. What physical mechanisms (internal heat generation distribution, anisotropy, contact resistance, phase change, radiation) can invalidate the practical lumped assumption? | Two engineers disagree on whether buoyancy matters in a forced-convection enclosure flow. What scaling analysis (e.g., Richardson/Grashof vs Reynolds) would you apply, and how would you interpret borderline values? | A thin-wall pressure vessel calculation looks fine globally, but a nozzle penetration is present. What specific stress components and local effects would you expect to dominate near the discontinuity, and what would you look for in a credible solution narrative? | A statics solution ‘works’ numerically, but the structure exhibits unexpected motion in reality. What stability checks distinguish a solvable equilibrium set from an actually stable configuration (mechanism vs structure)? | In a boiling heat transfer loop, outlet quality and wall superheat rise sharply at similar heat flux. What tells you the system is approaching CHF versus experiencing flow instabilities or dryout due to maldistribution? | A finite element result shows extremely high stress at a sharp re-entrant corner; the mesh refinement increases the peak without convergence. How do you distinguish a mathematical singularity from a design-critical hotspot, and what quantity do you report instead? | A dynamic model linearized at an operating point matches small-signal response but fails under moderate step inputs. What features (nonlinear friction, saturation, regime switching, property variation) would you suspect first, and how would they manifest in residuals? | Given multiaxial, nonproportional loading on a ductile part, what limitations of von Mises/Tresca would you consider, and what additional evidence would you seek before accepting a single scalar margin? | Pump + system curves predict an operating point, but field measurements show higher head at lower flow. What hidden assumptions (NPSH, cavitation, instrumentation bias, viscosity correction, recirculation, control valve characteristics) are most diagnostic to check? | A radiation model assumes gray, diffuse surfaces and uses view factors, yet predicts net heat flow in the wrong direction. What quick consistency checks (energy balance, reciprocity, enclosure constraints, emissivity bounds) can reveal the modeling error? | When reviewing an engineering solution, what are your top three ‘physics sanity tests’ (order-of-magnitude bounds, limiting cases, conservation checks) and how do they change across fluids, thermo, and dynamics problems? | You’re comparing two solution approaches: one uses aggressive simplifying assumptions but is elegantly consistent; the other is more general but internally messy. What criteria do you use to judge correctness, robustness across edge cases, and decision risk?",
    "source_url": "https://editor.superannotate.com/jobs/Mechanical-25-777"
  },
  {
    "job_id": "Mechanical-25-777",
    "title": "Mechanical Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Spain",
    "pay_rate": "$-1 - $60 / hour",
    "pay_min": -1,
    "pay_max": 60,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated engineering solutions and/or generate expert Mechanical Engineering content, evaluating reasoning quality and step-by-step problem-solving while providing crisp written feedback. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify methodological or conceptual errors; fact-check assumptions; write high-quality explanations and model solutions; and rate and compare multiple responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate, delivering training data for many of the world’s largest AI companies and foundation-model labs. Your mechanical engineering expertise directly helps improve the world’s premier AI models by making their technical reasoning more accurate, reliable, and clearly explained.",
    "keywords": "Mechanical Engineering, Thermodynamics, Heat Transfer, Fluid Mechanics, Statics, Dynamics, Differential Equations, System Modeling, Design Principles, English (C1+)",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Mechanical Engineering, Aerospace Engineering, or Engineering Physics (Thermo/Heat Transfer/Fluids/Statics-Dynamics/Differential Equations). | Strong, confident fundamentals in mechanics, thermodynamics, fluid mechanics, applied physics, engineering math, and system modeling. | Comfortable evaluating technical solutions for correctness, methodology, units/dimensional consistency, assumptions, and physical plausibility with high attention to detail. | Able to clearly explain reasoning and corrections in writing; Minimum C1 English proficiency. | Experience applying basic design principles (requirements, constraints, margins, trade-offs) in real engineering contexts. | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation or technical reviewing is strongly preferred.",
    "sample_interview_questions": "A transient conduction model predicts faster cool-down than observed. Without re-running anything, what hierarchy of checks do you apply (units, properties, boundary conditions, contact resistances, radiation, internal generation) to identify the most likely root cause? | In a compressible duct with heat addition (Rayleigh flow), what qualitative trends in Mach number and stagnation temperature would you expect as heat is added in subsonic vs supersonic regimes, and how can those expectations catch a flawed solution? | A CFD report shows turbulent pipe flow with y+ ~ 200 but claims wall-resolved accuracy. What questions would you ask to determine whether the wall treatment is appropriate and whether friction factor/ΔP should be trusted? | You see an analysis that uses constant viscosity and density for a thermal-fluid problem with large temperature rise. What dimensionless groups or sensitivity arguments would you use to judge whether variable-property effects materially change the conclusion? | A mechanism’s static equilibrium solution gives plausible reactions, but its predicted actuator torque is far too low. What are the most common modeling omissions (friction, moment arms, constraint forces, gravity distribution) that create this specific symptom? | A rotor-bearing system shows a critical speed near the operating range. What evidence would make you model gyroscopic effects and cross-coupled stiffness, and what failure modes do those effects tend to introduce if ignored? | An engineer estimates convective coefficients using a textbook correlation outside its stated range (Re, Pr, geometry). What ‘red flags’ tell you extrapolation is unsafe, and what alternative bounding approach would you use to sanity-check results? | A first-principles system model of a closed volume produces negative absolute pressure during a rapid valve event. What does that imply about compressibility assumptions, numerical integration stability, or state variable choice? | A lumped-capacitance model passes a Biot-number check, yet measured temperature gradients persist. What physical mechanisms (internal heat generation distribution, anisotropy, contact resistance, phase change, radiation) can invalidate the practical lumped assumption? | Two engineers disagree on whether buoyancy matters in a forced-convection enclosure flow. What scaling analysis (e.g., Richardson/Grashof vs Reynolds) would you apply, and how would you interpret borderline values? | A thin-wall pressure vessel calculation looks fine globally, but a nozzle penetration is present. What specific stress components and local effects would you expect to dominate near the discontinuity, and what would you look for in a credible solution narrative? | A statics solution ‘works’ numerically, but the structure exhibits unexpected motion in reality. What stability checks distinguish a solvable equilibrium set from an actually stable configuration (mechanism vs structure)? | In a boiling heat transfer loop, outlet quality and wall superheat rise sharply at similar heat flux. What tells you the system is approaching CHF versus experiencing flow instabilities or dryout due to maldistribution? | A finite element result shows extremely high stress at a sharp re-entrant corner; the mesh refinement increases the peak without convergence. How do you distinguish a mathematical singularity from a design-critical hotspot, and what quantity do you report instead? | A dynamic model linearized at an operating point matches small-signal response but fails under moderate step inputs. What features (nonlinear friction, saturation, regime switching, property variation) would you suspect first, and how would they manifest in residuals? | Given multiaxial, nonproportional loading on a ductile part, what limitations of von Mises/Tresca would you consider, and what additional evidence would you seek before accepting a single scalar margin? | Pump + system curves predict an operating point, but field measurements show higher head at lower flow. What hidden assumptions (NPSH, cavitation, instrumentation bias, viscosity correction, recirculation, control valve characteristics) are most diagnostic to check? | A radiation model assumes gray, diffuse surfaces and uses view factors, yet predicts net heat flow in the wrong direction. What quick consistency checks (energy balance, reciprocity, enclosure constraints, emissivity bounds) can reveal the modeling error? | When reviewing an engineering solution, what are your top three ‘physics sanity tests’ (order-of-magnitude bounds, limiting cases, conservation checks) and how do they change across fluids, thermo, and dynamics problems? | You’re comparing two solution approaches: one uses aggressive simplifying assumptions but is elegantly consistent; the other is more general but internally messy. What criteria do you use to judge correctness, robustness across edge cases, and decision risk?",
    "source_url": "https://editor.superannotate.com/jobs/Mechanical-25-777"
  },
  {
    "job_id": "Go-25-319",
    "title": "Go Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Bangladesh",
    "pay_rate": "$-1 - $22 / hour",
    "pay_min": -1,
    "pay_max": 22,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Go Engineer for AI Data Training, you will review AI-generated Go code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, readability, and adherence to backend-style best practices; identify errors in concurrency, error handling, or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Go; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Go, Golang, Concurrency, Backend development, Microservices, gRPC, Cloud-native, Error handling, Profiling, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Go programming experience, ideally focused on backend services, APIs, or systems work. | Strong understanding of Go idioms, including clear error handling, composition over inheritance, and simple, readable interfaces. | Hands-on software engineering experience building and maintaining backend-style Go applications in production environments. | Ability to evaluate correctness and readability in backend-oriented Go code, including concurrency patterns, error propagation, and API design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: hands-on experience with Go frameworks and libraries such as Gin, gRPC, or Echo in real projects. | Preferred: merged PRs in Go open-source projects, competitive programming experience, cloud-native development exposure, and familiarity with Go performance profiling tools and techniques.",
    "sample_interview_questions": "How would you describe Go’s philosophy and core idioms to a developer coming from an object-oriented language like Java or C#? | Can you explain how goroutines and channels work together to implement concurrency in Go, and when you would favor one pattern over another? | When reviewing Go code that uses shared state, what signs tell you that data races or concurrency issues may be present? | How do you typically structure error handling in Go, and what patterns do you consider idiomatic versus overly complex? | What factors do you evaluate to judge whether a Go function or method is sufficiently readable and maintainable for a production codebase? | Can you walk through how the Go scheduler works at a high level and how that influences your design of concurrent systems? | How do you approach designing and reviewing HTTP or gRPC handlers in Go for clarity, separation of concerns, and testability? | What strategies do you use to organize Go packages and modules so that dependencies remain clear and the architecture stays modular? | How do you evaluate the trade-offs between using interfaces and concrete types in Go APIs, and what are common mistakes you watch for? | When reading Go code that interacts with context.Context, what patterns indicate that timeouts, cancellations, and deadlines are being handled correctly? | How do you reason about performance in Go, and what kinds of issues do you look for before resorting to profiling tools? | What is your process for using Go’s profiling tools (pprof, trace) to diagnose bottlenecks or memory leaks in a service? | How do you review Go code that uses third-party libraries or frameworks like Gin or Echo to ensure they are integrated safely and consistently? | When evaluating persistence-related code in Go (for example, database access), what patterns help ensure correctness, transaction safety, and clear error propagation? | How do you decide whether to use channels versus other synchronization primitives like mutexes or wait groups in a given design? | What are some common anti-patterns you have seen in Go projects, and how do you recommend refactoring them toward more idiomatic solutions? | How do you ensure that logging, metrics, and observability concerns are handled cleanly within Go services without polluting business logic? | When reviewing a Go-based microservice, what aspects of configuration, deployment, and cloud-native integration do you pay attention to? | How do you critically evaluate suggestions from LLMs or code assistants for Go, and what types of errors or non-idiomatic patterns do you most frequently catch? | Can you describe a concrete example where you improved an existing Go codebase for correctness, concurrency behavior, or readability, and what steps you took to get there?",
    "source_url": "https://editor.superannotate.com/jobs/Go-25-319"
  },
  {
    "job_id": "Go-25-319",
    "title": "Go Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Indonesia",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Go Engineer for AI Data Training, you will review AI-generated Go code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, readability, and adherence to backend-style best practices; identify errors in concurrency, error handling, or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Go; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Go, Golang, Concurrency, Backend development, Microservices, gRPC, Cloud-native, Error handling, Profiling, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Go programming experience, ideally focused on backend services, APIs, or systems work. | Strong understanding of Go idioms, including clear error handling, composition over inheritance, and simple, readable interfaces. | Hands-on software engineering experience building and maintaining backend-style Go applications in production environments. | Ability to evaluate correctness and readability in backend-oriented Go code, including concurrency patterns, error propagation, and API design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: hands-on experience with Go frameworks and libraries such as Gin, gRPC, or Echo in real projects. | Preferred: merged PRs in Go open-source projects, competitive programming experience, cloud-native development exposure, and familiarity with Go performance profiling tools and techniques.",
    "sample_interview_questions": "How would you describe Go’s philosophy and core idioms to a developer coming from an object-oriented language like Java or C#? | Can you explain how goroutines and channels work together to implement concurrency in Go, and when you would favor one pattern over another? | When reviewing Go code that uses shared state, what signs tell you that data races or concurrency issues may be present? | How do you typically structure error handling in Go, and what patterns do you consider idiomatic versus overly complex? | What factors do you evaluate to judge whether a Go function or method is sufficiently readable and maintainable for a production codebase? | Can you walk through how the Go scheduler works at a high level and how that influences your design of concurrent systems? | How do you approach designing and reviewing HTTP or gRPC handlers in Go for clarity, separation of concerns, and testability? | What strategies do you use to organize Go packages and modules so that dependencies remain clear and the architecture stays modular? | How do you evaluate the trade-offs between using interfaces and concrete types in Go APIs, and what are common mistakes you watch for? | When reading Go code that interacts with context.Context, what patterns indicate that timeouts, cancellations, and deadlines are being handled correctly? | How do you reason about performance in Go, and what kinds of issues do you look for before resorting to profiling tools? | What is your process for using Go’s profiling tools (pprof, trace) to diagnose bottlenecks or memory leaks in a service? | How do you review Go code that uses third-party libraries or frameworks like Gin or Echo to ensure they are integrated safely and consistently? | When evaluating persistence-related code in Go (for example, database access), what patterns help ensure correctness, transaction safety, and clear error propagation? | How do you decide whether to use channels versus other synchronization primitives like mutexes or wait groups in a given design? | What are some common anti-patterns you have seen in Go projects, and how do you recommend refactoring them toward more idiomatic solutions? | How do you ensure that logging, metrics, and observability concerns are handled cleanly within Go services without polluting business logic? | When reviewing a Go-based microservice, what aspects of configuration, deployment, and cloud-native integration do you pay attention to? | How do you critically evaluate suggestions from LLMs or code assistants for Go, and what types of errors or non-idiomatic patterns do you most frequently catch? | Can you describe a concrete example where you improved an existing Go codebase for correctness, concurrency behavior, or readability, and what steps you took to get there?",
    "source_url": "https://editor.superannotate.com/jobs/Go-25-319"
  },
  {
    "job_id": "Go-25-319",
    "title": "Go Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Egypt",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Go Engineer for AI Data Training, you will review AI-generated Go code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, readability, and adherence to backend-style best practices; identify errors in concurrency, error handling, or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Go; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Go, Golang, Concurrency, Backend development, Microservices, gRPC, Cloud-native, Error handling, Profiling, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Go programming experience, ideally focused on backend services, APIs, or systems work. | Strong understanding of Go idioms, including clear error handling, composition over inheritance, and simple, readable interfaces. | Hands-on software engineering experience building and maintaining backend-style Go applications in production environments. | Ability to evaluate correctness and readability in backend-oriented Go code, including concurrency patterns, error propagation, and API design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: hands-on experience with Go frameworks and libraries such as Gin, gRPC, or Echo in real projects. | Preferred: merged PRs in Go open-source projects, competitive programming experience, cloud-native development exposure, and familiarity with Go performance profiling tools and techniques.",
    "sample_interview_questions": "How would you describe Go’s philosophy and core idioms to a developer coming from an object-oriented language like Java or C#? | Can you explain how goroutines and channels work together to implement concurrency in Go, and when you would favor one pattern over another? | When reviewing Go code that uses shared state, what signs tell you that data races or concurrency issues may be present? | How do you typically structure error handling in Go, and what patterns do you consider idiomatic versus overly complex? | What factors do you evaluate to judge whether a Go function or method is sufficiently readable and maintainable for a production codebase? | Can you walk through how the Go scheduler works at a high level and how that influences your design of concurrent systems? | How do you approach designing and reviewing HTTP or gRPC handlers in Go for clarity, separation of concerns, and testability? | What strategies do you use to organize Go packages and modules so that dependencies remain clear and the architecture stays modular? | How do you evaluate the trade-offs between using interfaces and concrete types in Go APIs, and what are common mistakes you watch for? | When reading Go code that interacts with context.Context, what patterns indicate that timeouts, cancellations, and deadlines are being handled correctly? | How do you reason about performance in Go, and what kinds of issues do you look for before resorting to profiling tools? | What is your process for using Go’s profiling tools (pprof, trace) to diagnose bottlenecks or memory leaks in a service? | How do you review Go code that uses third-party libraries or frameworks like Gin or Echo to ensure they are integrated safely and consistently? | When evaluating persistence-related code in Go (for example, database access), what patterns help ensure correctness, transaction safety, and clear error propagation? | How do you decide whether to use channels versus other synchronization primitives like mutexes or wait groups in a given design? | What are some common anti-patterns you have seen in Go projects, and how do you recommend refactoring them toward more idiomatic solutions? | How do you ensure that logging, metrics, and observability concerns are handled cleanly within Go services without polluting business logic? | When reviewing a Go-based microservice, what aspects of configuration, deployment, and cloud-native integration do you pay attention to? | How do you critically evaluate suggestions from LLMs or code assistants for Go, and what types of errors or non-idiomatic patterns do you most frequently catch? | Can you describe a concrete example where you improved an existing Go codebase for correctness, concurrency behavior, or readability, and what steps you took to get there?",
    "source_url": "https://editor.superannotate.com/jobs/Go-25-319"
  },
  {
    "job_id": "Go-25-319",
    "title": "Go Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Brazil",
    "pay_rate": "$-1 - $40 / hour",
    "pay_min": -1,
    "pay_max": 40,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Go Engineer for AI Data Training, you will review AI-generated Go code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, readability, and adherence to backend-style best practices; identify errors in concurrency, error handling, or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Go; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Go, Golang, Concurrency, Backend development, Microservices, gRPC, Cloud-native, Error handling, Profiling, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Go programming experience, ideally focused on backend services, APIs, or systems work. | Strong understanding of Go idioms, including clear error handling, composition over inheritance, and simple, readable interfaces. | Hands-on software engineering experience building and maintaining backend-style Go applications in production environments. | Ability to evaluate correctness and readability in backend-oriented Go code, including concurrency patterns, error propagation, and API design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: hands-on experience with Go frameworks and libraries such as Gin, gRPC, or Echo in real projects. | Preferred: merged PRs in Go open-source projects, competitive programming experience, cloud-native development exposure, and familiarity with Go performance profiling tools and techniques.",
    "sample_interview_questions": "How would you describe Go’s philosophy and core idioms to a developer coming from an object-oriented language like Java or C#? | Can you explain how goroutines and channels work together to implement concurrency in Go, and when you would favor one pattern over another? | When reviewing Go code that uses shared state, what signs tell you that data races or concurrency issues may be present? | How do you typically structure error handling in Go, and what patterns do you consider idiomatic versus overly complex? | What factors do you evaluate to judge whether a Go function or method is sufficiently readable and maintainable for a production codebase? | Can you walk through how the Go scheduler works at a high level and how that influences your design of concurrent systems? | How do you approach designing and reviewing HTTP or gRPC handlers in Go for clarity, separation of concerns, and testability? | What strategies do you use to organize Go packages and modules so that dependencies remain clear and the architecture stays modular? | How do you evaluate the trade-offs between using interfaces and concrete types in Go APIs, and what are common mistakes you watch for? | When reading Go code that interacts with context.Context, what patterns indicate that timeouts, cancellations, and deadlines are being handled correctly? | How do you reason about performance in Go, and what kinds of issues do you look for before resorting to profiling tools? | What is your process for using Go’s profiling tools (pprof, trace) to diagnose bottlenecks or memory leaks in a service? | How do you review Go code that uses third-party libraries or frameworks like Gin or Echo to ensure they are integrated safely and consistently? | When evaluating persistence-related code in Go (for example, database access), what patterns help ensure correctness, transaction safety, and clear error propagation? | How do you decide whether to use channels versus other synchronization primitives like mutexes or wait groups in a given design? | What are some common anti-patterns you have seen in Go projects, and how do you recommend refactoring them toward more idiomatic solutions? | How do you ensure that logging, metrics, and observability concerns are handled cleanly within Go services without polluting business logic? | When reviewing a Go-based microservice, what aspects of configuration, deployment, and cloud-native integration do you pay attention to? | How do you critically evaluate suggestions from LLMs or code assistants for Go, and what types of errors or non-idiomatic patterns do you most frequently catch? | Can you describe a concrete example where you improved an existing Go codebase for correctness, concurrency behavior, or readability, and what steps you took to get there?",
    "source_url": "https://editor.superannotate.com/jobs/Go-25-319"
  },
  {
    "job_id": "Go-25-319",
    "title": "Go Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Portugal",
    "pay_rate": "$-1 - $45 / hour",
    "pay_min": -1,
    "pay_max": 45,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Go Engineer for AI Data Training, you will review AI-generated Go code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, readability, and adherence to backend-style best practices; identify errors in concurrency, error handling, or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Go; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Go, Golang, Concurrency, Backend development, Microservices, gRPC, Cloud-native, Error handling, Profiling, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Go programming experience, ideally focused on backend services, APIs, or systems work. | Strong understanding of Go idioms, including clear error handling, composition over inheritance, and simple, readable interfaces. | Hands-on software engineering experience building and maintaining backend-style Go applications in production environments. | Ability to evaluate correctness and readability in backend-oriented Go code, including concurrency patterns, error propagation, and API design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: hands-on experience with Go frameworks and libraries such as Gin, gRPC, or Echo in real projects. | Preferred: merged PRs in Go open-source projects, competitive programming experience, cloud-native development exposure, and familiarity with Go performance profiling tools and techniques.",
    "sample_interview_questions": "How would you describe Go’s philosophy and core idioms to a developer coming from an object-oriented language like Java or C#? | Can you explain how goroutines and channels work together to implement concurrency in Go, and when you would favor one pattern over another? | When reviewing Go code that uses shared state, what signs tell you that data races or concurrency issues may be present? | How do you typically structure error handling in Go, and what patterns do you consider idiomatic versus overly complex? | What factors do you evaluate to judge whether a Go function or method is sufficiently readable and maintainable for a production codebase? | Can you walk through how the Go scheduler works at a high level and how that influences your design of concurrent systems? | How do you approach designing and reviewing HTTP or gRPC handlers in Go for clarity, separation of concerns, and testability? | What strategies do you use to organize Go packages and modules so that dependencies remain clear and the architecture stays modular? | How do you evaluate the trade-offs between using interfaces and concrete types in Go APIs, and what are common mistakes you watch for? | When reading Go code that interacts with context.Context, what patterns indicate that timeouts, cancellations, and deadlines are being handled correctly? | How do you reason about performance in Go, and what kinds of issues do you look for before resorting to profiling tools? | What is your process for using Go’s profiling tools (pprof, trace) to diagnose bottlenecks or memory leaks in a service? | How do you review Go code that uses third-party libraries or frameworks like Gin or Echo to ensure they are integrated safely and consistently? | When evaluating persistence-related code in Go (for example, database access), what patterns help ensure correctness, transaction safety, and clear error propagation? | How do you decide whether to use channels versus other synchronization primitives like mutexes or wait groups in a given design? | What are some common anti-patterns you have seen in Go projects, and how do you recommend refactoring them toward more idiomatic solutions? | How do you ensure that logging, metrics, and observability concerns are handled cleanly within Go services without polluting business logic? | When reviewing a Go-based microservice, what aspects of configuration, deployment, and cloud-native integration do you pay attention to? | How do you critically evaluate suggestions from LLMs or code assistants for Go, and what types of errors or non-idiomatic patterns do you most frequently catch? | Can you describe a concrete example where you improved an existing Go codebase for correctness, concurrency behavior, or readability, and what steps you took to get there?",
    "source_url": "https://editor.superannotate.com/jobs/Go-25-319"
  },
  {
    "job_id": "Go-25-319",
    "title": "Go Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Spain",
    "pay_rate": "$-1 - $50 / hour",
    "pay_min": -1,
    "pay_max": 50,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Go Engineer for AI Data Training, you will review AI-generated Go code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, readability, and adherence to backend-style best practices; identify errors in concurrency, error handling, or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Go; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Go, Golang, Concurrency, Backend development, Microservices, gRPC, Cloud-native, Error handling, Profiling, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Go programming experience, ideally focused on backend services, APIs, or systems work. | Strong understanding of Go idioms, including clear error handling, composition over inheritance, and simple, readable interfaces. | Hands-on software engineering experience building and maintaining backend-style Go applications in production environments. | Ability to evaluate correctness and readability in backend-oriented Go code, including concurrency patterns, error propagation, and API design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: hands-on experience with Go frameworks and libraries such as Gin, gRPC, or Echo in real projects. | Preferred: merged PRs in Go open-source projects, competitive programming experience, cloud-native development exposure, and familiarity with Go performance profiling tools and techniques.",
    "sample_interview_questions": "How would you describe Go’s philosophy and core idioms to a developer coming from an object-oriented language like Java or C#? | Can you explain how goroutines and channels work together to implement concurrency in Go, and when you would favor one pattern over another? | When reviewing Go code that uses shared state, what signs tell you that data races or concurrency issues may be present? | How do you typically structure error handling in Go, and what patterns do you consider idiomatic versus overly complex? | What factors do you evaluate to judge whether a Go function or method is sufficiently readable and maintainable for a production codebase? | Can you walk through how the Go scheduler works at a high level and how that influences your design of concurrent systems? | How do you approach designing and reviewing HTTP or gRPC handlers in Go for clarity, separation of concerns, and testability? | What strategies do you use to organize Go packages and modules so that dependencies remain clear and the architecture stays modular? | How do you evaluate the trade-offs between using interfaces and concrete types in Go APIs, and what are common mistakes you watch for? | When reading Go code that interacts with context.Context, what patterns indicate that timeouts, cancellations, and deadlines are being handled correctly? | How do you reason about performance in Go, and what kinds of issues do you look for before resorting to profiling tools? | What is your process for using Go’s profiling tools (pprof, trace) to diagnose bottlenecks or memory leaks in a service? | How do you review Go code that uses third-party libraries or frameworks like Gin or Echo to ensure they are integrated safely and consistently? | When evaluating persistence-related code in Go (for example, database access), what patterns help ensure correctness, transaction safety, and clear error propagation? | How do you decide whether to use channels versus other synchronization primitives like mutexes or wait groups in a given design? | What are some common anti-patterns you have seen in Go projects, and how do you recommend refactoring them toward more idiomatic solutions? | How do you ensure that logging, metrics, and observability concerns are handled cleanly within Go services without polluting business logic? | When reviewing a Go-based microservice, what aspects of configuration, deployment, and cloud-native integration do you pay attention to? | How do you critically evaluate suggestions from LLMs or code assistants for Go, and what types of errors or non-idiomatic patterns do you most frequently catch? | Can you describe a concrete example where you improved an existing Go codebase for correctness, concurrency behavior, or readability, and what steps you took to get there?",
    "source_url": "https://editor.superannotate.com/jobs/Go-25-319"
  },
  {
    "job_id": "Go-25-319",
    "title": "Go Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Go Engineer for AI Data Training, you will review AI-generated Go code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, readability, and adherence to backend-style best practices; identify errors in concurrency, error handling, or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Go; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Go, Golang, Concurrency, Backend development, Microservices, gRPC, Cloud-native, Error handling, Profiling, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Go programming experience, ideally focused on backend services, APIs, or systems work. | Strong understanding of Go idioms, including clear error handling, composition over inheritance, and simple, readable interfaces. | Hands-on software engineering experience building and maintaining backend-style Go applications in production environments. | Ability to evaluate correctness and readability in backend-oriented Go code, including concurrency patterns, error propagation, and API design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: hands-on experience with Go frameworks and libraries such as Gin, gRPC, or Echo in real projects. | Preferred: merged PRs in Go open-source projects, competitive programming experience, cloud-native development exposure, and familiarity with Go performance profiling tools and techniques.",
    "sample_interview_questions": "How would you describe Go’s philosophy and core idioms to a developer coming from an object-oriented language like Java or C#? | Can you explain how goroutines and channels work together to implement concurrency in Go, and when you would favor one pattern over another? | When reviewing Go code that uses shared state, what signs tell you that data races or concurrency issues may be present? | How do you typically structure error handling in Go, and what patterns do you consider idiomatic versus overly complex? | What factors do you evaluate to judge whether a Go function or method is sufficiently readable and maintainable for a production codebase? | Can you walk through how the Go scheduler works at a high level and how that influences your design of concurrent systems? | How do you approach designing and reviewing HTTP or gRPC handlers in Go for clarity, separation of concerns, and testability? | What strategies do you use to organize Go packages and modules so that dependencies remain clear and the architecture stays modular? | How do you evaluate the trade-offs between using interfaces and concrete types in Go APIs, and what are common mistakes you watch for? | When reading Go code that interacts with context.Context, what patterns indicate that timeouts, cancellations, and deadlines are being handled correctly? | How do you reason about performance in Go, and what kinds of issues do you look for before resorting to profiling tools? | What is your process for using Go’s profiling tools (pprof, trace) to diagnose bottlenecks or memory leaks in a service? | How do you review Go code that uses third-party libraries or frameworks like Gin or Echo to ensure they are integrated safely and consistently? | When evaluating persistence-related code in Go (for example, database access), what patterns help ensure correctness, transaction safety, and clear error propagation? | How do you decide whether to use channels versus other synchronization primitives like mutexes or wait groups in a given design? | What are some common anti-patterns you have seen in Go projects, and how do you recommend refactoring them toward more idiomatic solutions? | How do you ensure that logging, metrics, and observability concerns are handled cleanly within Go services without polluting business logic? | When reviewing a Go-based microservice, what aspects of configuration, deployment, and cloud-native integration do you pay attention to? | How do you critically evaluate suggestions from LLMs or code assistants for Go, and what types of errors or non-idiomatic patterns do you most frequently catch? | Can you describe a concrete example where you improved an existing Go codebase for correctness, concurrency behavior, or readability, and what steps you took to get there?",
    "source_url": "https://editor.superannotate.com/jobs/Go-25-319"
  },
  {
    "job_id": "Rust-25-804",
    "title": "Rust Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Bangladesh",
    "pay_rate": "$-1 - $25 / hour",
    "pay_min": -1,
    "pay_max": 25,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Rust Engineer for AI Data Training, you will review AI-generated Rust code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, safety, and adherence to the prompt; identify errors in ownership, borrowing, lifetimes, or algorithmic reasoning; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Rust patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs. Your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Rust, Lifetimes, Async Rust, Tokio, Actix, Serde, Systems programming, Backend development, English, Research",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Rust development experience in backend, CLI, or systems-focused projects. | Strong understanding of Rust’s ownership, borrowing, and lifetime model, with the ability to reason clearly about aliasing and data races. | Solid software engineering experience in at least one of backend services, command-line tools, or systems programming using Rust. | Ability to evaluate safe, idiomatic Rust code, including appropriate use of traits, generics, pattern matching, and error handling. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: experience with Tokio, Actix, Serde, and async Rust patterns in production or advanced side projects. | Preferred: competitive programming experience, contributions to Rust open-source ecosystems, and demonstrated ability to reason about performance, concurrency, and correctness in Rust code.",
    "sample_interview_questions": "How would you explain Rust’s ownership model to another engineer, and why is it central to memory safety in Rust? | Can you describe the difference between borrowing by reference and borrowing mutably in Rust, and how the borrow checker enforces these rules? | When reviewing Rust code, what patterns or signals tell you that lifetimes are being handled correctly versus only compiling by coincidence? | How do you reason about choosing between Box, Rc, Arc, and borrowing in a design that involves shared data structures? | What are some examples of code that may compile but still be considered non-idiomatic or unsafe in practice, and how do you identify them during review? | How do you compare Rust’s error handling approaches (Result, ? operator, custom error types) and decide which pattern fits a particular API? | Can you walk through how you would evaluate the correctness and edge-case handling of a function manipulating collections in Rust, such as using iterators and combinators? | When you encounter a complex lifetime error message, what is your systematic process for understanding and resolving it? | How do you assess the trade-offs between using synchronous Rust and async Rust in the context of backend, CLI, or systems tasks? | What factors do you consider when reviewing async Rust code that uses Tokio or Actix, especially around cancellation, backpressure, and resource leaks? | How do you evaluate the design of a trait-based abstraction in Rust for clarity, extensibility, and minimal coupling? | What steps do you take to assess whether Serde-based serialization and deserialization code is robust to schema changes and malformed input? | Can you discuss how you would review a Rust API for ergonomics, including naming, use of generics, and the balance between flexibility and simplicity? | How do you approach reasoning about performance characteristics in Rust, such as allocations, copies, and use of iterators versus indexing? | When examining unsafe blocks in Rust, what specific criteria do you apply to decide whether the usage is justified and correctly encapsulated? | How do you validate that a Rust CLI tool handles errors, logging, and exit codes in a way that is predictable and friendly for users and scripts? | What is your approach to reviewing module and crate structure in a Rust project for cohesion, clear boundaries, and maintainable dependencies? | How do you critically evaluate suggestions from LLMs or code assistants for Rust code, and what kinds of mistakes do you most frequently catch? | Can you describe an example where you refactored existing Rust code to improve safety, clarity, or performance, and how you measured the improvement? | When looking at contributions to a Rust open-source project, what aspects of the code and discussions do you examine to judge quality and alignment with project standards?",
    "source_url": "https://editor.superannotate.com/jobs/Rust-25-804"
  },
  {
    "job_id": "Rust-25-804",
    "title": "Rust Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "India",
    "pay_rate": "$-1 - $35 / hour",
    "pay_min": -1,
    "pay_max": 35,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Rust Engineer for AI Data Training, you will review AI-generated Rust code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, safety, and adherence to the prompt; identify errors in ownership, borrowing, lifetimes, or algorithmic reasoning; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Rust patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs. Your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Rust, Lifetimes, Async Rust, Tokio, Actix, Serde, Systems programming, Backend development, English, Research",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Rust development experience in backend, CLI, or systems-focused projects. | Strong understanding of Rust’s ownership, borrowing, and lifetime model, with the ability to reason clearly about aliasing and data races. | Solid software engineering experience in at least one of backend services, command-line tools, or systems programming using Rust. | Ability to evaluate safe, idiomatic Rust code, including appropriate use of traits, generics, pattern matching, and error handling. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: experience with Tokio, Actix, Serde, and async Rust patterns in production or advanced side projects. | Preferred: competitive programming experience, contributions to Rust open-source ecosystems, and demonstrated ability to reason about performance, concurrency, and correctness in Rust code.",
    "sample_interview_questions": "How would you explain Rust’s ownership model to another engineer, and why is it central to memory safety in Rust? | Can you describe the difference between borrowing by reference and borrowing mutably in Rust, and how the borrow checker enforces these rules? | When reviewing Rust code, what patterns or signals tell you that lifetimes are being handled correctly versus only compiling by coincidence? | How do you reason about choosing between Box, Rc, Arc, and borrowing in a design that involves shared data structures? | What are some examples of code that may compile but still be considered non-idiomatic or unsafe in practice, and how do you identify them during review? | How do you compare Rust’s error handling approaches (Result, ? operator, custom error types) and decide which pattern fits a particular API? | Can you walk through how you would evaluate the correctness and edge-case handling of a function manipulating collections in Rust, such as using iterators and combinators? | When you encounter a complex lifetime error message, what is your systematic process for understanding and resolving it? | How do you assess the trade-offs between using synchronous Rust and async Rust in the context of backend, CLI, or systems tasks? | What factors do you consider when reviewing async Rust code that uses Tokio or Actix, especially around cancellation, backpressure, and resource leaks? | How do you evaluate the design of a trait-based abstraction in Rust for clarity, extensibility, and minimal coupling? | What steps do you take to assess whether Serde-based serialization and deserialization code is robust to schema changes and malformed input? | Can you discuss how you would review a Rust API for ergonomics, including naming, use of generics, and the balance between flexibility and simplicity? | How do you approach reasoning about performance characteristics in Rust, such as allocations, copies, and use of iterators versus indexing? | When examining unsafe blocks in Rust, what specific criteria do you apply to decide whether the usage is justified and correctly encapsulated? | How do you validate that a Rust CLI tool handles errors, logging, and exit codes in a way that is predictable and friendly for users and scripts? | What is your approach to reviewing module and crate structure in a Rust project for cohesion, clear boundaries, and maintainable dependencies? | How do you critically evaluate suggestions from LLMs or code assistants for Rust code, and what kinds of mistakes do you most frequently catch? | Can you describe an example where you refactored existing Rust code to improve safety, clarity, or performance, and how you measured the improvement? | When looking at contributions to a Rust open-source project, what aspects of the code and discussions do you examine to judge quality and alignment with project standards?",
    "source_url": "https://editor.superannotate.com/jobs/Rust-25-804"
  },
  {
    "job_id": "Rust-25-804",
    "title": "Rust Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Indonesia",
    "pay_rate": "$-1 - $35 / hour",
    "pay_min": -1,
    "pay_max": 35,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Rust Engineer for AI Data Training, you will review AI-generated Rust code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, safety, and adherence to the prompt; identify errors in ownership, borrowing, lifetimes, or algorithmic reasoning; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Rust patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs. Your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Rust, Lifetimes, Async Rust, Tokio, Actix, Serde, Systems programming, Backend development, English, Research",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Rust development experience in backend, CLI, or systems-focused projects. | Strong understanding of Rust’s ownership, borrowing, and lifetime model, with the ability to reason clearly about aliasing and data races. | Solid software engineering experience in at least one of backend services, command-line tools, or systems programming using Rust. | Ability to evaluate safe, idiomatic Rust code, including appropriate use of traits, generics, pattern matching, and error handling. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: experience with Tokio, Actix, Serde, and async Rust patterns in production or advanced side projects. | Preferred: competitive programming experience, contributions to Rust open-source ecosystems, and demonstrated ability to reason about performance, concurrency, and correctness in Rust code.",
    "sample_interview_questions": "How would you explain Rust’s ownership model to another engineer, and why is it central to memory safety in Rust? | Can you describe the difference between borrowing by reference and borrowing mutably in Rust, and how the borrow checker enforces these rules? | When reviewing Rust code, what patterns or signals tell you that lifetimes are being handled correctly versus only compiling by coincidence? | How do you reason about choosing between Box, Rc, Arc, and borrowing in a design that involves shared data structures? | What are some examples of code that may compile but still be considered non-idiomatic or unsafe in practice, and how do you identify them during review? | How do you compare Rust’s error handling approaches (Result, ? operator, custom error types) and decide which pattern fits a particular API? | Can you walk through how you would evaluate the correctness and edge-case handling of a function manipulating collections in Rust, such as using iterators and combinators? | When you encounter a complex lifetime error message, what is your systematic process for understanding and resolving it? | How do you assess the trade-offs between using synchronous Rust and async Rust in the context of backend, CLI, or systems tasks? | What factors do you consider when reviewing async Rust code that uses Tokio or Actix, especially around cancellation, backpressure, and resource leaks? | How do you evaluate the design of a trait-based abstraction in Rust for clarity, extensibility, and minimal coupling? | What steps do you take to assess whether Serde-based serialization and deserialization code is robust to schema changes and malformed input? | Can you discuss how you would review a Rust API for ergonomics, including naming, use of generics, and the balance between flexibility and simplicity? | How do you approach reasoning about performance characteristics in Rust, such as allocations, copies, and use of iterators versus indexing? | When examining unsafe blocks in Rust, what specific criteria do you apply to decide whether the usage is justified and correctly encapsulated? | How do you validate that a Rust CLI tool handles errors, logging, and exit codes in a way that is predictable and friendly for users and scripts? | What is your approach to reviewing module and crate structure in a Rust project for cohesion, clear boundaries, and maintainable dependencies? | How do you critically evaluate suggestions from LLMs or code assistants for Rust code, and what kinds of mistakes do you most frequently catch? | Can you describe an example where you refactored existing Rust code to improve safety, clarity, or performance, and how you measured the improvement? | When looking at contributions to a Rust open-source project, what aspects of the code and discussions do you examine to judge quality and alignment with project standards?",
    "source_url": "https://editor.superannotate.com/jobs/Rust-25-804"
  },
  {
    "job_id": "Rust-25-804",
    "title": "Rust Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Egypt",
    "pay_rate": "$-1 - $35 / hour",
    "pay_min": -1,
    "pay_max": 35,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Rust Engineer for AI Data Training, you will review AI-generated Rust code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, safety, and adherence to the prompt; identify errors in ownership, borrowing, lifetimes, or algorithmic reasoning; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Rust patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs. Your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Rust, Lifetimes, Async Rust, Tokio, Actix, Serde, Systems programming, Backend development, English, Research",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Rust development experience in backend, CLI, or systems-focused projects. | Strong understanding of Rust’s ownership, borrowing, and lifetime model, with the ability to reason clearly about aliasing and data races. | Solid software engineering experience in at least one of backend services, command-line tools, or systems programming using Rust. | Ability to evaluate safe, idiomatic Rust code, including appropriate use of traits, generics, pattern matching, and error handling. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: experience with Tokio, Actix, Serde, and async Rust patterns in production or advanced side projects. | Preferred: competitive programming experience, contributions to Rust open-source ecosystems, and demonstrated ability to reason about performance, concurrency, and correctness in Rust code.",
    "sample_interview_questions": "How would you explain Rust’s ownership model to another engineer, and why is it central to memory safety in Rust? | Can you describe the difference between borrowing by reference and borrowing mutably in Rust, and how the borrow checker enforces these rules? | When reviewing Rust code, what patterns or signals tell you that lifetimes are being handled correctly versus only compiling by coincidence? | How do you reason about choosing between Box, Rc, Arc, and borrowing in a design that involves shared data structures? | What are some examples of code that may compile but still be considered non-idiomatic or unsafe in practice, and how do you identify them during review? | How do you compare Rust’s error handling approaches (Result, ? operator, custom error types) and decide which pattern fits a particular API? | Can you walk through how you would evaluate the correctness and edge-case handling of a function manipulating collections in Rust, such as using iterators and combinators? | When you encounter a complex lifetime error message, what is your systematic process for understanding and resolving it? | How do you assess the trade-offs between using synchronous Rust and async Rust in the context of backend, CLI, or systems tasks? | What factors do you consider when reviewing async Rust code that uses Tokio or Actix, especially around cancellation, backpressure, and resource leaks? | How do you evaluate the design of a trait-based abstraction in Rust for clarity, extensibility, and minimal coupling? | What steps do you take to assess whether Serde-based serialization and deserialization code is robust to schema changes and malformed input? | Can you discuss how you would review a Rust API for ergonomics, including naming, use of generics, and the balance between flexibility and simplicity? | How do you approach reasoning about performance characteristics in Rust, such as allocations, copies, and use of iterators versus indexing? | When examining unsafe blocks in Rust, what specific criteria do you apply to decide whether the usage is justified and correctly encapsulated? | How do you validate that a Rust CLI tool handles errors, logging, and exit codes in a way that is predictable and friendly for users and scripts? | What is your approach to reviewing module and crate structure in a Rust project for cohesion, clear boundaries, and maintainable dependencies? | How do you critically evaluate suggestions from LLMs or code assistants for Rust code, and what kinds of mistakes do you most frequently catch? | Can you describe an example where you refactored existing Rust code to improve safety, clarity, or performance, and how you measured the improvement? | When looking at contributions to a Rust open-source project, what aspects of the code and discussions do you examine to judge quality and alignment with project standards?",
    "source_url": "https://editor.superannotate.com/jobs/Rust-25-804"
  },
  {
    "job_id": "Rust-25-804",
    "title": "Rust Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Brazil",
    "pay_rate": "$-1 - $50 / hour",
    "pay_min": -1,
    "pay_max": 50,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Rust Engineer for AI Data Training, you will review AI-generated Rust code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, safety, and adherence to the prompt; identify errors in ownership, borrowing, lifetimes, or algorithmic reasoning; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Rust patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs. Your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Rust, Lifetimes, Async Rust, Tokio, Actix, Serde, Systems programming, Backend development, English, Research",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Rust development experience in backend, CLI, or systems-focused projects. | Strong understanding of Rust’s ownership, borrowing, and lifetime model, with the ability to reason clearly about aliasing and data races. | Solid software engineering experience in at least one of backend services, command-line tools, or systems programming using Rust. | Ability to evaluate safe, idiomatic Rust code, including appropriate use of traits, generics, pattern matching, and error handling. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: experience with Tokio, Actix, Serde, and async Rust patterns in production or advanced side projects. | Preferred: competitive programming experience, contributions to Rust open-source ecosystems, and demonstrated ability to reason about performance, concurrency, and correctness in Rust code.",
    "sample_interview_questions": "How would you explain Rust’s ownership model to another engineer, and why is it central to memory safety in Rust? | Can you describe the difference between borrowing by reference and borrowing mutably in Rust, and how the borrow checker enforces these rules? | When reviewing Rust code, what patterns or signals tell you that lifetimes are being handled correctly versus only compiling by coincidence? | How do you reason about choosing between Box, Rc, Arc, and borrowing in a design that involves shared data structures? | What are some examples of code that may compile but still be considered non-idiomatic or unsafe in practice, and how do you identify them during review? | How do you compare Rust’s error handling approaches (Result, ? operator, custom error types) and decide which pattern fits a particular API? | Can you walk through how you would evaluate the correctness and edge-case handling of a function manipulating collections in Rust, such as using iterators and combinators? | When you encounter a complex lifetime error message, what is your systematic process for understanding and resolving it? | How do you assess the trade-offs between using synchronous Rust and async Rust in the context of backend, CLI, or systems tasks? | What factors do you consider when reviewing async Rust code that uses Tokio or Actix, especially around cancellation, backpressure, and resource leaks? | How do you evaluate the design of a trait-based abstraction in Rust for clarity, extensibility, and minimal coupling? | What steps do you take to assess whether Serde-based serialization and deserialization code is robust to schema changes and malformed input? | Can you discuss how you would review a Rust API for ergonomics, including naming, use of generics, and the balance between flexibility and simplicity? | How do you approach reasoning about performance characteristics in Rust, such as allocations, copies, and use of iterators versus indexing? | When examining unsafe blocks in Rust, what specific criteria do you apply to decide whether the usage is justified and correctly encapsulated? | How do you validate that a Rust CLI tool handles errors, logging, and exit codes in a way that is predictable and friendly for users and scripts? | What is your approach to reviewing module and crate structure in a Rust project for cohesion, clear boundaries, and maintainable dependencies? | How do you critically evaluate suggestions from LLMs or code assistants for Rust code, and what kinds of mistakes do you most frequently catch? | Can you describe an example where you refactored existing Rust code to improve safety, clarity, or performance, and how you measured the improvement? | When looking at contributions to a Rust open-source project, what aspects of the code and discussions do you examine to judge quality and alignment with project standards?",
    "source_url": "https://editor.superannotate.com/jobs/Rust-25-804"
  },
  {
    "job_id": "Rust-25-804",
    "title": "Rust Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Spain",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Rust Engineer for AI Data Training, you will review AI-generated Rust code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, safety, and adherence to the prompt; identify errors in ownership, borrowing, lifetimes, or algorithmic reasoning; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Rust patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs. Your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Rust, Lifetimes, Async Rust, Tokio, Actix, Serde, Systems programming, Backend development, English, Research",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Rust development experience in backend, CLI, or systems-focused projects. | Strong understanding of Rust’s ownership, borrowing, and lifetime model, with the ability to reason clearly about aliasing and data races. | Solid software engineering experience in at least one of backend services, command-line tools, or systems programming using Rust. | Ability to evaluate safe, idiomatic Rust code, including appropriate use of traits, generics, pattern matching, and error handling. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: experience with Tokio, Actix, Serde, and async Rust patterns in production or advanced side projects. | Preferred: competitive programming experience, contributions to Rust open-source ecosystems, and demonstrated ability to reason about performance, concurrency, and correctness in Rust code.",
    "sample_interview_questions": "How would you explain Rust’s ownership model to another engineer, and why is it central to memory safety in Rust? | Can you describe the difference between borrowing by reference and borrowing mutably in Rust, and how the borrow checker enforces these rules? | When reviewing Rust code, what patterns or signals tell you that lifetimes are being handled correctly versus only compiling by coincidence? | How do you reason about choosing between Box, Rc, Arc, and borrowing in a design that involves shared data structures? | What are some examples of code that may compile but still be considered non-idiomatic or unsafe in practice, and how do you identify them during review? | How do you compare Rust’s error handling approaches (Result, ? operator, custom error types) and decide which pattern fits a particular API? | Can you walk through how you would evaluate the correctness and edge-case handling of a function manipulating collections in Rust, such as using iterators and combinators? | When you encounter a complex lifetime error message, what is your systematic process for understanding and resolving it? | How do you assess the trade-offs between using synchronous Rust and async Rust in the context of backend, CLI, or systems tasks? | What factors do you consider when reviewing async Rust code that uses Tokio or Actix, especially around cancellation, backpressure, and resource leaks? | How do you evaluate the design of a trait-based abstraction in Rust for clarity, extensibility, and minimal coupling? | What steps do you take to assess whether Serde-based serialization and deserialization code is robust to schema changes and malformed input? | Can you discuss how you would review a Rust API for ergonomics, including naming, use of generics, and the balance between flexibility and simplicity? | How do you approach reasoning about performance characteristics in Rust, such as allocations, copies, and use of iterators versus indexing? | When examining unsafe blocks in Rust, what specific criteria do you apply to decide whether the usage is justified and correctly encapsulated? | How do you validate that a Rust CLI tool handles errors, logging, and exit codes in a way that is predictable and friendly for users and scripts? | What is your approach to reviewing module and crate structure in a Rust project for cohesion, clear boundaries, and maintainable dependencies? | How do you critically evaluate suggestions from LLMs or code assistants for Rust code, and what kinds of mistakes do you most frequently catch? | Can you describe an example where you refactored existing Rust code to improve safety, clarity, or performance, and how you measured the improvement? | When looking at contributions to a Rust open-source project, what aspects of the code and discussions do you examine to judge quality and alignment with project standards?",
    "source_url": "https://editor.superannotate.com/jobs/Rust-25-804"
  },
  {
    "job_id": "Rust-25-804",
    "title": "Rust Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Portugal",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Rust Engineer for AI Data Training, you will review AI-generated Rust code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, safety, and adherence to the prompt; identify errors in ownership, borrowing, lifetimes, or algorithmic reasoning; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Rust patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs. Your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Rust, Lifetimes, Async Rust, Tokio, Actix, Serde, Systems programming, Backend development, English, Research",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Rust development experience in backend, CLI, or systems-focused projects. | Strong understanding of Rust’s ownership, borrowing, and lifetime model, with the ability to reason clearly about aliasing and data races. | Solid software engineering experience in at least one of backend services, command-line tools, or systems programming using Rust. | Ability to evaluate safe, idiomatic Rust code, including appropriate use of traits, generics, pattern matching, and error handling. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: experience with Tokio, Actix, Serde, and async Rust patterns in production or advanced side projects. | Preferred: competitive programming experience, contributions to Rust open-source ecosystems, and demonstrated ability to reason about performance, concurrency, and correctness in Rust code.",
    "sample_interview_questions": "How would you explain Rust’s ownership model to another engineer, and why is it central to memory safety in Rust? | Can you describe the difference between borrowing by reference and borrowing mutably in Rust, and how the borrow checker enforces these rules? | When reviewing Rust code, what patterns or signals tell you that lifetimes are being handled correctly versus only compiling by coincidence? | How do you reason about choosing between Box, Rc, Arc, and borrowing in a design that involves shared data structures? | What are some examples of code that may compile but still be considered non-idiomatic or unsafe in practice, and how do you identify them during review? | How do you compare Rust’s error handling approaches (Result, ? operator, custom error types) and decide which pattern fits a particular API? | Can you walk through how you would evaluate the correctness and edge-case handling of a function manipulating collections in Rust, such as using iterators and combinators? | When you encounter a complex lifetime error message, what is your systematic process for understanding and resolving it? | How do you assess the trade-offs between using synchronous Rust and async Rust in the context of backend, CLI, or systems tasks? | What factors do you consider when reviewing async Rust code that uses Tokio or Actix, especially around cancellation, backpressure, and resource leaks? | How do you evaluate the design of a trait-based abstraction in Rust for clarity, extensibility, and minimal coupling? | What steps do you take to assess whether Serde-based serialization and deserialization code is robust to schema changes and malformed input? | Can you discuss how you would review a Rust API for ergonomics, including naming, use of generics, and the balance between flexibility and simplicity? | How do you approach reasoning about performance characteristics in Rust, such as allocations, copies, and use of iterators versus indexing? | When examining unsafe blocks in Rust, what specific criteria do you apply to decide whether the usage is justified and correctly encapsulated? | How do you validate that a Rust CLI tool handles errors, logging, and exit codes in a way that is predictable and friendly for users and scripts? | What is your approach to reviewing module and crate structure in a Rust project for cohesion, clear boundaries, and maintainable dependencies? | How do you critically evaluate suggestions from LLMs or code assistants for Rust code, and what kinds of mistakes do you most frequently catch? | Can you describe an example where you refactored existing Rust code to improve safety, clarity, or performance, and how you measured the improvement? | When looking at contributions to a Rust open-source project, what aspects of the code and discussions do you examine to judge quality and alignment with project standards?",
    "source_url": "https://editor.superannotate.com/jobs/Rust-25-804"
  },
  {
    "job_id": "Kotlin-25-582",
    "title": "Kotlin Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Bangladesh",
    "pay_rate": "$-1 - $22 / hour",
    "pay_min": -1,
    "pay_max": 22,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Kotlin Engineer, you will review AI-generated responses and generate high-quality Kotlin-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical information; craft expert-level explanations and model solutions that demonstrate correct approaches; and rate and compare multiple AI responses based on correctness and reasoning quality. T\n\nhis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, helping to improve the world’s premier AI models.",
    "keywords": "Kotlin, Android development, Backend engineering, Coroutines, Null safety, Ktor, Android Jetpack, Software architecture, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on Kotlin development experience. | Strong understanding of Kotlin null safety, coroutines, and functional idioms. | Professional software engineering experience using Kotlin for Android or backend services. | Minimum Bachelor’s degree in Computer Science or a closely related field. | Ability to critically evaluate idiomatic Kotlin style, design decisions, and architecture trade-offs. | Significant experience using large language models (LLMs) to assist with coding, debugging, and code review. | Excellent English writing skills with the ability to articulate complex technical concepts clearly. | Highly preferred: Experience with Ktor, Android Jetpack, and advanced coroutine usage in production systems. | Preferred: Merged pull requests in Kotlin open-source projects, competitive programming experience, or similar demonstration of strong problem-solving skills. | Experience reviewing app logic or mobile/backend architecture, with previous AI data training or model evaluation work strongly preferred and Minimum C1 English proficiency.",
    "sample_interview_questions": "Can you explain how Kotlin’s null safety system works and how it helps prevent common runtime errors compared to Java? | How do you decide when to use coroutines versus other concurrency mechanisms in a Kotlin-based system? | Describe a situation where you refactored Kotlin code to be more idiomatic. What changes did you make and why? | What are the key differences between suspend functions and regular functions in Kotlin, and how do you avoid blocking the main thread when using them? | How do you structure a multi-module Kotlin project to keep responsibilities clear and maintainable over time? | Can you walk through how you would analyze and improve the performance of a slow Kotlin API or Android feature? | What architectural patterns (for example, MVVM, Clean Architecture, hexagonal) have you used with Kotlin, and what trade-offs did you observe? | How do you handle error propagation and exception handling in coroutine-based code to keep logic clear and robust? | In your experience, what are some common anti-patterns or code smells you see in Kotlin codebases, and how do you address them? | How would you evaluate whether a piece of Kotlin code is truly idiomatic, beyond just compiling and working correctly? | Describe how you have used Kotlin features like extension functions, data classes, and sealed classes to model domain logic effectively. | If you have used Ktor, how would you design and organize routes, middleware, and configuration for a medium-sized backend service? | How do you approach reviewing another engineer’s Android or backend Kotlin code for correctness, readability, and architectural soundness? | What strategies do you use to ensure that asynchronous operations in an Android app (such as network calls) remain lifecycle-aware and safe? | Can you explain how you would reason about and debug a race condition or concurrency bug in coroutine-based code? | How do you typically leverage large language models while coding, and what are your criteria for trusting or questioning their suggestions? | Describe how you would validate that a proposed Kotlin implementation correctly fulfills a product requirement or user story. | How do you think about test strategy in Kotlin projects, including unit tests and integration tests, to ensure code reliability? | What factors do you consider when choosing between different dependency injection approaches in a Kotlin project (for example, Dagger/Hilt versus manual DI)? | Tell me about a time you had to evaluate or improve the architecture of a mobile or backend system built with Kotlin. What steps did you take and what was the outcome?",
    "source_url": "https://editor.superannotate.com/jobs/Kotlin-25-582"
  },
  {
    "job_id": "Kotlin-25-582",
    "title": "Kotlin Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "India",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Kotlin Engineer, you will review AI-generated responses and generate high-quality Kotlin-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical information; craft expert-level explanations and model solutions that demonstrate correct approaches; and rate and compare multiple AI responses based on correctness and reasoning quality. T\n\nhis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, helping to improve the world’s premier AI models.",
    "keywords": "Kotlin, Android development, Backend engineering, Coroutines, Null safety, Ktor, Android Jetpack, Software architecture, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on Kotlin development experience. | Strong understanding of Kotlin null safety, coroutines, and functional idioms. | Professional software engineering experience using Kotlin for Android or backend services. | Minimum Bachelor’s degree in Computer Science or a closely related field. | Ability to critically evaluate idiomatic Kotlin style, design decisions, and architecture trade-offs. | Significant experience using large language models (LLMs) to assist with coding, debugging, and code review. | Excellent English writing skills with the ability to articulate complex technical concepts clearly. | Highly preferred: Experience with Ktor, Android Jetpack, and advanced coroutine usage in production systems. | Preferred: Merged pull requests in Kotlin open-source projects, competitive programming experience, or similar demonstration of strong problem-solving skills. | Experience reviewing app logic or mobile/backend architecture, with previous AI data training or model evaluation work strongly preferred and Minimum C1 English proficiency.",
    "sample_interview_questions": "Can you explain how Kotlin’s null safety system works and how it helps prevent common runtime errors compared to Java? | How do you decide when to use coroutines versus other concurrency mechanisms in a Kotlin-based system? | Describe a situation where you refactored Kotlin code to be more idiomatic. What changes did you make and why? | What are the key differences between suspend functions and regular functions in Kotlin, and how do you avoid blocking the main thread when using them? | How do you structure a multi-module Kotlin project to keep responsibilities clear and maintainable over time? | Can you walk through how you would analyze and improve the performance of a slow Kotlin API or Android feature? | What architectural patterns (for example, MVVM, Clean Architecture, hexagonal) have you used with Kotlin, and what trade-offs did you observe? | How do you handle error propagation and exception handling in coroutine-based code to keep logic clear and robust? | In your experience, what are some common anti-patterns or code smells you see in Kotlin codebases, and how do you address them? | How would you evaluate whether a piece of Kotlin code is truly idiomatic, beyond just compiling and working correctly? | Describe how you have used Kotlin features like extension functions, data classes, and sealed classes to model domain logic effectively. | If you have used Ktor, how would you design and organize routes, middleware, and configuration for a medium-sized backend service? | How do you approach reviewing another engineer’s Android or backend Kotlin code for correctness, readability, and architectural soundness? | What strategies do you use to ensure that asynchronous operations in an Android app (such as network calls) remain lifecycle-aware and safe? | Can you explain how you would reason about and debug a race condition or concurrency bug in coroutine-based code? | How do you typically leverage large language models while coding, and what are your criteria for trusting or questioning their suggestions? | Describe how you would validate that a proposed Kotlin implementation correctly fulfills a product requirement or user story. | How do you think about test strategy in Kotlin projects, including unit tests and integration tests, to ensure code reliability? | What factors do you consider when choosing between different dependency injection approaches in a Kotlin project (for example, Dagger/Hilt versus manual DI)? | Tell me about a time you had to evaluate or improve the architecture of a mobile or backend system built with Kotlin. What steps did you take and what was the outcome?",
    "source_url": "https://editor.superannotate.com/jobs/Kotlin-25-582"
  },
  {
    "job_id": "Kotlin-25-582",
    "title": "Kotlin Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Indonesia",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Kotlin Engineer, you will review AI-generated responses and generate high-quality Kotlin-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical information; craft expert-level explanations and model solutions that demonstrate correct approaches; and rate and compare multiple AI responses based on correctness and reasoning quality. T\n\nhis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, helping to improve the world’s premier AI models.",
    "keywords": "Kotlin, Android development, Backend engineering, Coroutines, Null safety, Ktor, Android Jetpack, Software architecture, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on Kotlin development experience. | Strong understanding of Kotlin null safety, coroutines, and functional idioms. | Professional software engineering experience using Kotlin for Android or backend services. | Minimum Bachelor’s degree in Computer Science or a closely related field. | Ability to critically evaluate idiomatic Kotlin style, design decisions, and architecture trade-offs. | Significant experience using large language models (LLMs) to assist with coding, debugging, and code review. | Excellent English writing skills with the ability to articulate complex technical concepts clearly. | Highly preferred: Experience with Ktor, Android Jetpack, and advanced coroutine usage in production systems. | Preferred: Merged pull requests in Kotlin open-source projects, competitive programming experience, or similar demonstration of strong problem-solving skills. | Experience reviewing app logic or mobile/backend architecture, with previous AI data training or model evaluation work strongly preferred and Minimum C1 English proficiency.",
    "sample_interview_questions": "Can you explain how Kotlin’s null safety system works and how it helps prevent common runtime errors compared to Java? | How do you decide when to use coroutines versus other concurrency mechanisms in a Kotlin-based system? | Describe a situation where you refactored Kotlin code to be more idiomatic. What changes did you make and why? | What are the key differences between suspend functions and regular functions in Kotlin, and how do you avoid blocking the main thread when using them? | How do you structure a multi-module Kotlin project to keep responsibilities clear and maintainable over time? | Can you walk through how you would analyze and improve the performance of a slow Kotlin API or Android feature? | What architectural patterns (for example, MVVM, Clean Architecture, hexagonal) have you used with Kotlin, and what trade-offs did you observe? | How do you handle error propagation and exception handling in coroutine-based code to keep logic clear and robust? | In your experience, what are some common anti-patterns or code smells you see in Kotlin codebases, and how do you address them? | How would you evaluate whether a piece of Kotlin code is truly idiomatic, beyond just compiling and working correctly? | Describe how you have used Kotlin features like extension functions, data classes, and sealed classes to model domain logic effectively. | If you have used Ktor, how would you design and organize routes, middleware, and configuration for a medium-sized backend service? | How do you approach reviewing another engineer’s Android or backend Kotlin code for correctness, readability, and architectural soundness? | What strategies do you use to ensure that asynchronous operations in an Android app (such as network calls) remain lifecycle-aware and safe? | Can you explain how you would reason about and debug a race condition or concurrency bug in coroutine-based code? | How do you typically leverage large language models while coding, and what are your criteria for trusting or questioning their suggestions? | Describe how you would validate that a proposed Kotlin implementation correctly fulfills a product requirement or user story. | How do you think about test strategy in Kotlin projects, including unit tests and integration tests, to ensure code reliability? | What factors do you consider when choosing between different dependency injection approaches in a Kotlin project (for example, Dagger/Hilt versus manual DI)? | Tell me about a time you had to evaluate or improve the architecture of a mobile or backend system built with Kotlin. What steps did you take and what was the outcome?",
    "source_url": "https://editor.superannotate.com/jobs/Kotlin-25-582"
  },
  {
    "job_id": "Kotlin-25-582",
    "title": "Kotlin Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Egypt",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Kotlin Engineer, you will review AI-generated responses and generate high-quality Kotlin-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical information; craft expert-level explanations and model solutions that demonstrate correct approaches; and rate and compare multiple AI responses based on correctness and reasoning quality. T\n\nhis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, helping to improve the world’s premier AI models.",
    "keywords": "Kotlin, Android development, Backend engineering, Coroutines, Null safety, Ktor, Android Jetpack, Software architecture, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on Kotlin development experience. | Strong understanding of Kotlin null safety, coroutines, and functional idioms. | Professional software engineering experience using Kotlin for Android or backend services. | Minimum Bachelor’s degree in Computer Science or a closely related field. | Ability to critically evaluate idiomatic Kotlin style, design decisions, and architecture trade-offs. | Significant experience using large language models (LLMs) to assist with coding, debugging, and code review. | Excellent English writing skills with the ability to articulate complex technical concepts clearly. | Highly preferred: Experience with Ktor, Android Jetpack, and advanced coroutine usage in production systems. | Preferred: Merged pull requests in Kotlin open-source projects, competitive programming experience, or similar demonstration of strong problem-solving skills. | Experience reviewing app logic or mobile/backend architecture, with previous AI data training or model evaluation work strongly preferred and Minimum C1 English proficiency.",
    "sample_interview_questions": "Can you explain how Kotlin’s null safety system works and how it helps prevent common runtime errors compared to Java? | How do you decide when to use coroutines versus other concurrency mechanisms in a Kotlin-based system? | Describe a situation where you refactored Kotlin code to be more idiomatic. What changes did you make and why? | What are the key differences between suspend functions and regular functions in Kotlin, and how do you avoid blocking the main thread when using them? | How do you structure a multi-module Kotlin project to keep responsibilities clear and maintainable over time? | Can you walk through how you would analyze and improve the performance of a slow Kotlin API or Android feature? | What architectural patterns (for example, MVVM, Clean Architecture, hexagonal) have you used with Kotlin, and what trade-offs did you observe? | How do you handle error propagation and exception handling in coroutine-based code to keep logic clear and robust? | In your experience, what are some common anti-patterns or code smells you see in Kotlin codebases, and how do you address them? | How would you evaluate whether a piece of Kotlin code is truly idiomatic, beyond just compiling and working correctly? | Describe how you have used Kotlin features like extension functions, data classes, and sealed classes to model domain logic effectively. | If you have used Ktor, how would you design and organize routes, middleware, and configuration for a medium-sized backend service? | How do you approach reviewing another engineer’s Android or backend Kotlin code for correctness, readability, and architectural soundness? | What strategies do you use to ensure that asynchronous operations in an Android app (such as network calls) remain lifecycle-aware and safe? | Can you explain how you would reason about and debug a race condition or concurrency bug in coroutine-based code? | How do you typically leverage large language models while coding, and what are your criteria for trusting or questioning their suggestions? | Describe how you would validate that a proposed Kotlin implementation correctly fulfills a product requirement or user story. | How do you think about test strategy in Kotlin projects, including unit tests and integration tests, to ensure code reliability? | What factors do you consider when choosing between different dependency injection approaches in a Kotlin project (for example, Dagger/Hilt versus manual DI)? | Tell me about a time you had to evaluate or improve the architecture of a mobile or backend system built with Kotlin. What steps did you take and what was the outcome?",
    "source_url": "https://editor.superannotate.com/jobs/Kotlin-25-582"
  },
  {
    "job_id": "Kotlin-25-582",
    "title": "Kotlin Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Brazil",
    "pay_rate": "$-1 - $45 / hour",
    "pay_min": -1,
    "pay_max": 45,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Kotlin Engineer, you will review AI-generated responses and generate high-quality Kotlin-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical information; craft expert-level explanations and model solutions that demonstrate correct approaches; and rate and compare multiple AI responses based on correctness and reasoning quality. T\n\nhis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, helping to improve the world’s premier AI models.",
    "keywords": "Kotlin, Android development, Backend engineering, Coroutines, Null safety, Ktor, Android Jetpack, Software architecture, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on Kotlin development experience. | Strong understanding of Kotlin null safety, coroutines, and functional idioms. | Professional software engineering experience using Kotlin for Android or backend services. | Minimum Bachelor’s degree in Computer Science or a closely related field. | Ability to critically evaluate idiomatic Kotlin style, design decisions, and architecture trade-offs. | Significant experience using large language models (LLMs) to assist with coding, debugging, and code review. | Excellent English writing skills with the ability to articulate complex technical concepts clearly. | Highly preferred: Experience with Ktor, Android Jetpack, and advanced coroutine usage in production systems. | Preferred: Merged pull requests in Kotlin open-source projects, competitive programming experience, or similar demonstration of strong problem-solving skills. | Experience reviewing app logic or mobile/backend architecture, with previous AI data training or model evaluation work strongly preferred and Minimum C1 English proficiency.",
    "sample_interview_questions": "Can you explain how Kotlin’s null safety system works and how it helps prevent common runtime errors compared to Java? | How do you decide when to use coroutines versus other concurrency mechanisms in a Kotlin-based system? | Describe a situation where you refactored Kotlin code to be more idiomatic. What changes did you make and why? | What are the key differences between suspend functions and regular functions in Kotlin, and how do you avoid blocking the main thread when using them? | How do you structure a multi-module Kotlin project to keep responsibilities clear and maintainable over time? | Can you walk through how you would analyze and improve the performance of a slow Kotlin API or Android feature? | What architectural patterns (for example, MVVM, Clean Architecture, hexagonal) have you used with Kotlin, and what trade-offs did you observe? | How do you handle error propagation and exception handling in coroutine-based code to keep logic clear and robust? | In your experience, what are some common anti-patterns or code smells you see in Kotlin codebases, and how do you address them? | How would you evaluate whether a piece of Kotlin code is truly idiomatic, beyond just compiling and working correctly? | Describe how you have used Kotlin features like extension functions, data classes, and sealed classes to model domain logic effectively. | If you have used Ktor, how would you design and organize routes, middleware, and configuration for a medium-sized backend service? | How do you approach reviewing another engineer’s Android or backend Kotlin code for correctness, readability, and architectural soundness? | What strategies do you use to ensure that asynchronous operations in an Android app (such as network calls) remain lifecycle-aware and safe? | Can you explain how you would reason about and debug a race condition or concurrency bug in coroutine-based code? | How do you typically leverage large language models while coding, and what are your criteria for trusting or questioning their suggestions? | Describe how you would validate that a proposed Kotlin implementation correctly fulfills a product requirement or user story. | How do you think about test strategy in Kotlin projects, including unit tests and integration tests, to ensure code reliability? | What factors do you consider when choosing between different dependency injection approaches in a Kotlin project (for example, Dagger/Hilt versus manual DI)? | Tell me about a time you had to evaluate or improve the architecture of a mobile or backend system built with Kotlin. What steps did you take and what was the outcome?",
    "source_url": "https://editor.superannotate.com/jobs/Kotlin-25-582"
  },
  {
    "job_id": "Kotlin-25-582",
    "title": "Kotlin Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Portugal",
    "pay_rate": "$-1 - $50 / hour",
    "pay_min": -1,
    "pay_max": 50,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Kotlin Engineer, you will review AI-generated responses and generate high-quality Kotlin-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical information; craft expert-level explanations and model solutions that demonstrate correct approaches; and rate and compare multiple AI responses based on correctness and reasoning quality. T\n\nhis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, helping to improve the world’s premier AI models.",
    "keywords": "Kotlin, Android development, Backend engineering, Coroutines, Null safety, Ktor, Android Jetpack, Software architecture, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on Kotlin development experience. | Strong understanding of Kotlin null safety, coroutines, and functional idioms. | Professional software engineering experience using Kotlin for Android or backend services. | Minimum Bachelor’s degree in Computer Science or a closely related field. | Ability to critically evaluate idiomatic Kotlin style, design decisions, and architecture trade-offs. | Significant experience using large language models (LLMs) to assist with coding, debugging, and code review. | Excellent English writing skills with the ability to articulate complex technical concepts clearly. | Highly preferred: Experience with Ktor, Android Jetpack, and advanced coroutine usage in production systems. | Preferred: Merged pull requests in Kotlin open-source projects, competitive programming experience, or similar demonstration of strong problem-solving skills. | Experience reviewing app logic or mobile/backend architecture, with previous AI data training or model evaluation work strongly preferred and Minimum C1 English proficiency.",
    "sample_interview_questions": "Can you explain how Kotlin’s null safety system works and how it helps prevent common runtime errors compared to Java? | How do you decide when to use coroutines versus other concurrency mechanisms in a Kotlin-based system? | Describe a situation where you refactored Kotlin code to be more idiomatic. What changes did you make and why? | What are the key differences between suspend functions and regular functions in Kotlin, and how do you avoid blocking the main thread when using them? | How do you structure a multi-module Kotlin project to keep responsibilities clear and maintainable over time? | Can you walk through how you would analyze and improve the performance of a slow Kotlin API or Android feature? | What architectural patterns (for example, MVVM, Clean Architecture, hexagonal) have you used with Kotlin, and what trade-offs did you observe? | How do you handle error propagation and exception handling in coroutine-based code to keep logic clear and robust? | In your experience, what are some common anti-patterns or code smells you see in Kotlin codebases, and how do you address them? | How would you evaluate whether a piece of Kotlin code is truly idiomatic, beyond just compiling and working correctly? | Describe how you have used Kotlin features like extension functions, data classes, and sealed classes to model domain logic effectively. | If you have used Ktor, how would you design and organize routes, middleware, and configuration for a medium-sized backend service? | How do you approach reviewing another engineer’s Android or backend Kotlin code for correctness, readability, and architectural soundness? | What strategies do you use to ensure that asynchronous operations in an Android app (such as network calls) remain lifecycle-aware and safe? | Can you explain how you would reason about and debug a race condition or concurrency bug in coroutine-based code? | How do you typically leverage large language models while coding, and what are your criteria for trusting or questioning their suggestions? | Describe how you would validate that a proposed Kotlin implementation correctly fulfills a product requirement or user story. | How do you think about test strategy in Kotlin projects, including unit tests and integration tests, to ensure code reliability? | What factors do you consider when choosing between different dependency injection approaches in a Kotlin project (for example, Dagger/Hilt versus manual DI)? | Tell me about a time you had to evaluate or improve the architecture of a mobile or backend system built with Kotlin. What steps did you take and what was the outcome?",
    "source_url": "https://editor.superannotate.com/jobs/Kotlin-25-582"
  },
  {
    "job_id": "Kotlin-25-582",
    "title": "Kotlin Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Spain",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Kotlin Engineer, you will review AI-generated responses and generate high-quality Kotlin-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical information; craft expert-level explanations and model solutions that demonstrate correct approaches; and rate and compare multiple AI responses based on correctness and reasoning quality. T\n\nhis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, helping to improve the world’s premier AI models.",
    "keywords": "Kotlin, Android development, Backend engineering, Coroutines, Null safety, Ktor, Android Jetpack, Software architecture, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on Kotlin development experience. | Strong understanding of Kotlin null safety, coroutines, and functional idioms. | Professional software engineering experience using Kotlin for Android or backend services. | Minimum Bachelor’s degree in Computer Science or a closely related field. | Ability to critically evaluate idiomatic Kotlin style, design decisions, and architecture trade-offs. | Significant experience using large language models (LLMs) to assist with coding, debugging, and code review. | Excellent English writing skills with the ability to articulate complex technical concepts clearly. | Highly preferred: Experience with Ktor, Android Jetpack, and advanced coroutine usage in production systems. | Preferred: Merged pull requests in Kotlin open-source projects, competitive programming experience, or similar demonstration of strong problem-solving skills. | Experience reviewing app logic or mobile/backend architecture, with previous AI data training or model evaluation work strongly preferred and Minimum C1 English proficiency.",
    "sample_interview_questions": "Can you explain how Kotlin’s null safety system works and how it helps prevent common runtime errors compared to Java? | How do you decide when to use coroutines versus other concurrency mechanisms in a Kotlin-based system? | Describe a situation where you refactored Kotlin code to be more idiomatic. What changes did you make and why? | What are the key differences between suspend functions and regular functions in Kotlin, and how do you avoid blocking the main thread when using them? | How do you structure a multi-module Kotlin project to keep responsibilities clear and maintainable over time? | Can you walk through how you would analyze and improve the performance of a slow Kotlin API or Android feature? | What architectural patterns (for example, MVVM, Clean Architecture, hexagonal) have you used with Kotlin, and what trade-offs did you observe? | How do you handle error propagation and exception handling in coroutine-based code to keep logic clear and robust? | In your experience, what are some common anti-patterns or code smells you see in Kotlin codebases, and how do you address them? | How would you evaluate whether a piece of Kotlin code is truly idiomatic, beyond just compiling and working correctly? | Describe how you have used Kotlin features like extension functions, data classes, and sealed classes to model domain logic effectively. | If you have used Ktor, how would you design and organize routes, middleware, and configuration for a medium-sized backend service? | How do you approach reviewing another engineer’s Android or backend Kotlin code for correctness, readability, and architectural soundness? | What strategies do you use to ensure that asynchronous operations in an Android app (such as network calls) remain lifecycle-aware and safe? | Can you explain how you would reason about and debug a race condition or concurrency bug in coroutine-based code? | How do you typically leverage large language models while coding, and what are your criteria for trusting or questioning their suggestions? | Describe how you would validate that a proposed Kotlin implementation correctly fulfills a product requirement or user story. | How do you think about test strategy in Kotlin projects, including unit tests and integration tests, to ensure code reliability? | What factors do you consider when choosing between different dependency injection approaches in a Kotlin project (for example, Dagger/Hilt versus manual DI)? | Tell me about a time you had to evaluate or improve the architecture of a mobile or backend system built with Kotlin. What steps did you take and what was the outcome?",
    "source_url": "https://editor.superannotate.com/jobs/Kotlin-25-582"
  },
  {
    "job_id": "Swift25-913",
    "title": "Swift Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Egypt",
    "pay_rate": "$-1 - $35 / hour",
    "pay_min": -1,
    "pay_max": 35,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Swift Engineer, you will review AI-generated responses and generate high-quality Swift and iOS-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical details; write expert-level explanations and model solutions that demonstrate correct approaches in Swift and iOS; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, helping to improve the world’s premier AI models.",
    "keywords": "Swift, iOS development, SwiftUI, UIKit, Combine, Mobile architecture, ARC, UI logic review, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on Swift/iOS development experience. | Strong understanding of core Swift fundamentals, memory management, and ARC. | Professional iOS engineering experience shipping or maintaining production mobile applications. | Minimum Bachelor’s degree in Computer Science or a closely related field. | Ability to evaluate code for correctness, maintainability, performance, and UI logic quality. | Significant experience using large language models (LLMs) to assist with coding, debugging, and code review. | Excellent English writing skills with the ability to articulate complex technical concepts clearly. | Highly preferred: Experience with SwiftUI, UIKit, and Combine in modern iOS architectures such as MVVM. | Preferred: Merged pull requests in Swift/iOS open-source projects, competitive programming exposure, or similar evidence of strong algorithmic problem-solving skills. | Experience reviewing app logic or mobile architecture, with previous AI data training or model evaluation work strongly preferred, and Minimum C1 English proficiency.",
    "sample_interview_questions": "How does Automatic Reference Counting (ARC) work in Swift, and what are strong, weak, and unowned references used for? | Can you describe a situation where a retain cycle might occur in an iOS app and how you would detect and resolve it? | When would you choose a struct versus a class in Swift, and what practical implications does that choice have for iOS app design? | How do value types and reference types behave differently when passed into functions or stored in collections in Swift? | Explain how you would design a networking layer for an iOS app using URLSession or a similar abstraction. | What are the key differences between UIKit and SwiftUI, and when might you prefer one over the other in a new feature? | How do you approach organizing view and business logic when using MVVM or another modern iOS architecture pattern? | Can you walk through how Combine or another reactive framework can be used to manage asynchronous data flows in an iOS app? | What strategies do you use to keep view controllers lean and maintainable in a UIKit-based application? | How would you analyze and improve the performance of a scrolling list that feels laggy on certain devices? | What are some common pitfalls you have seen in iOS concurrency, and how do you avoid issues when working with the main thread? | How do you approach reviewing another engineer’s Swift code for readability, correctness, and long-term maintainability? | Describe how you would validate that a complex UI flow behaves correctly given different user inputs and edge cases. | What techniques do you use to debug tricky layout or Auto Layout issues in iOS interfaces? | How do you typically structure unit tests and UI tests for an iOS feature to ensure it is well covered? | In what cases would you use property wrappers in Swift, and how can they help keep code expressive and reusable? | How do you reason about and handle error propagation in a Swift codebase that uses Result types or async/await? | Describe how you have used large language models while coding and how you decide whether to accept or modify their suggestions. | When evaluating an existing iOS codebase, what signals tell you that the architecture is healthy or that it needs refactoring? | How do you balance short-term delivery pressure with the need to maintain a clean and scalable iOS codebase over time?",
    "source_url": "https://editor.superannotate.com/jobs/Swift25-913"
  },
  {
    "job_id": "Swift25-913",
    "title": "Swift Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "India",
    "pay_rate": "$-1 - $35 / hour",
    "pay_min": -1,
    "pay_max": 35,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Swift Engineer, you will review AI-generated responses and generate high-quality Swift and iOS-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical details; write expert-level explanations and model solutions that demonstrate correct approaches in Swift and iOS; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, helping to improve the world’s premier AI models.",
    "keywords": "Swift, iOS development, SwiftUI, UIKit, Combine, Mobile architecture, ARC, UI logic review, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on Swift/iOS development experience. | Strong understanding of core Swift fundamentals, memory management, and ARC. | Professional iOS engineering experience shipping or maintaining production mobile applications. | Minimum Bachelor’s degree in Computer Science or a closely related field. | Ability to evaluate code for correctness, maintainability, performance, and UI logic quality. | Significant experience using large language models (LLMs) to assist with coding, debugging, and code review. | Excellent English writing skills with the ability to articulate complex technical concepts clearly. | Highly preferred: Experience with SwiftUI, UIKit, and Combine in modern iOS architectures such as MVVM. | Preferred: Merged pull requests in Swift/iOS open-source projects, competitive programming exposure, or similar evidence of strong algorithmic problem-solving skills. | Experience reviewing app logic or mobile architecture, with previous AI data training or model evaluation work strongly preferred, and Minimum C1 English proficiency.",
    "sample_interview_questions": "How does Automatic Reference Counting (ARC) work in Swift, and what are strong, weak, and unowned references used for? | Can you describe a situation where a retain cycle might occur in an iOS app and how you would detect and resolve it? | When would you choose a struct versus a class in Swift, and what practical implications does that choice have for iOS app design? | How do value types and reference types behave differently when passed into functions or stored in collections in Swift? | Explain how you would design a networking layer for an iOS app using URLSession or a similar abstraction. | What are the key differences between UIKit and SwiftUI, and when might you prefer one over the other in a new feature? | How do you approach organizing view and business logic when using MVVM or another modern iOS architecture pattern? | Can you walk through how Combine or another reactive framework can be used to manage asynchronous data flows in an iOS app? | What strategies do you use to keep view controllers lean and maintainable in a UIKit-based application? | How would you analyze and improve the performance of a scrolling list that feels laggy on certain devices? | What are some common pitfalls you have seen in iOS concurrency, and how do you avoid issues when working with the main thread? | How do you approach reviewing another engineer’s Swift code for readability, correctness, and long-term maintainability? | Describe how you would validate that a complex UI flow behaves correctly given different user inputs and edge cases. | What techniques do you use to debug tricky layout or Auto Layout issues in iOS interfaces? | How do you typically structure unit tests and UI tests for an iOS feature to ensure it is well covered? | In what cases would you use property wrappers in Swift, and how can they help keep code expressive and reusable? | How do you reason about and handle error propagation in a Swift codebase that uses Result types or async/await? | Describe how you have used large language models while coding and how you decide whether to accept or modify their suggestions. | When evaluating an existing iOS codebase, what signals tell you that the architecture is healthy or that it needs refactoring? | How do you balance short-term delivery pressure with the need to maintain a clean and scalable iOS codebase over time?",
    "source_url": "https://editor.superannotate.com/jobs/Swift25-913"
  },
  {
    "job_id": "Swift25-913",
    "title": "Swift Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Indonesia",
    "pay_rate": "$-1 - $35 / hour",
    "pay_min": -1,
    "pay_max": 35,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Swift Engineer, you will review AI-generated responses and generate high-quality Swift and iOS-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical details; write expert-level explanations and model solutions that demonstrate correct approaches in Swift and iOS; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, helping to improve the world’s premier AI models.",
    "keywords": "Swift, iOS development, SwiftUI, UIKit, Combine, Mobile architecture, ARC, UI logic review, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on Swift/iOS development experience. | Strong understanding of core Swift fundamentals, memory management, and ARC. | Professional iOS engineering experience shipping or maintaining production mobile applications. | Minimum Bachelor’s degree in Computer Science or a closely related field. | Ability to evaluate code for correctness, maintainability, performance, and UI logic quality. | Significant experience using large language models (LLMs) to assist with coding, debugging, and code review. | Excellent English writing skills with the ability to articulate complex technical concepts clearly. | Highly preferred: Experience with SwiftUI, UIKit, and Combine in modern iOS architectures such as MVVM. | Preferred: Merged pull requests in Swift/iOS open-source projects, competitive programming exposure, or similar evidence of strong algorithmic problem-solving skills. | Experience reviewing app logic or mobile architecture, with previous AI data training or model evaluation work strongly preferred, and Minimum C1 English proficiency.",
    "sample_interview_questions": "How does Automatic Reference Counting (ARC) work in Swift, and what are strong, weak, and unowned references used for? | Can you describe a situation where a retain cycle might occur in an iOS app and how you would detect and resolve it? | When would you choose a struct versus a class in Swift, and what practical implications does that choice have for iOS app design? | How do value types and reference types behave differently when passed into functions or stored in collections in Swift? | Explain how you would design a networking layer for an iOS app using URLSession or a similar abstraction. | What are the key differences between UIKit and SwiftUI, and when might you prefer one over the other in a new feature? | How do you approach organizing view and business logic when using MVVM or another modern iOS architecture pattern? | Can you walk through how Combine or another reactive framework can be used to manage asynchronous data flows in an iOS app? | What strategies do you use to keep view controllers lean and maintainable in a UIKit-based application? | How would you analyze and improve the performance of a scrolling list that feels laggy on certain devices? | What are some common pitfalls you have seen in iOS concurrency, and how do you avoid issues when working with the main thread? | How do you approach reviewing another engineer’s Swift code for readability, correctness, and long-term maintainability? | Describe how you would validate that a complex UI flow behaves correctly given different user inputs and edge cases. | What techniques do you use to debug tricky layout or Auto Layout issues in iOS interfaces? | How do you typically structure unit tests and UI tests for an iOS feature to ensure it is well covered? | In what cases would you use property wrappers in Swift, and how can they help keep code expressive and reusable? | How do you reason about and handle error propagation in a Swift codebase that uses Result types or async/await? | Describe how you have used large language models while coding and how you decide whether to accept or modify their suggestions. | When evaluating an existing iOS codebase, what signals tell you that the architecture is healthy or that it needs refactoring? | How do you balance short-term delivery pressure with the need to maintain a clean and scalable iOS codebase over time?",
    "source_url": "https://editor.superannotate.com/jobs/Swift25-913"
  },
  {
    "job_id": "Swift25-913",
    "title": "Swift Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Bangladesh",
    "pay_rate": "$-1 - $25 / hour",
    "pay_min": -1,
    "pay_max": 25,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Swift Engineer, you will review AI-generated responses and generate high-quality Swift and iOS-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical details; write expert-level explanations and model solutions that demonstrate correct approaches in Swift and iOS; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, helping to improve the world’s premier AI models.",
    "keywords": "Swift, iOS development, SwiftUI, UIKit, Combine, Mobile architecture, ARC, UI logic review, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on Swift/iOS development experience. | Strong understanding of core Swift fundamentals, memory management, and ARC. | Professional iOS engineering experience shipping or maintaining production mobile applications. | Minimum Bachelor’s degree in Computer Science or a closely related field. | Ability to evaluate code for correctness, maintainability, performance, and UI logic quality. | Significant experience using large language models (LLMs) to assist with coding, debugging, and code review. | Excellent English writing skills with the ability to articulate complex technical concepts clearly. | Highly preferred: Experience with SwiftUI, UIKit, and Combine in modern iOS architectures such as MVVM. | Preferred: Merged pull requests in Swift/iOS open-source projects, competitive programming exposure, or similar evidence of strong algorithmic problem-solving skills. | Experience reviewing app logic or mobile architecture, with previous AI data training or model evaluation work strongly preferred, and Minimum C1 English proficiency.",
    "sample_interview_questions": "How does Automatic Reference Counting (ARC) work in Swift, and what are strong, weak, and unowned references used for? | Can you describe a situation where a retain cycle might occur in an iOS app and how you would detect and resolve it? | When would you choose a struct versus a class in Swift, and what practical implications does that choice have for iOS app design? | How do value types and reference types behave differently when passed into functions or stored in collections in Swift? | Explain how you would design a networking layer for an iOS app using URLSession or a similar abstraction. | What are the key differences between UIKit and SwiftUI, and when might you prefer one over the other in a new feature? | How do you approach organizing view and business logic when using MVVM or another modern iOS architecture pattern? | Can you walk through how Combine or another reactive framework can be used to manage asynchronous data flows in an iOS app? | What strategies do you use to keep view controllers lean and maintainable in a UIKit-based application? | How would you analyze and improve the performance of a scrolling list that feels laggy on certain devices? | What are some common pitfalls you have seen in iOS concurrency, and how do you avoid issues when working with the main thread? | How do you approach reviewing another engineer’s Swift code for readability, correctness, and long-term maintainability? | Describe how you would validate that a complex UI flow behaves correctly given different user inputs and edge cases. | What techniques do you use to debug tricky layout or Auto Layout issues in iOS interfaces? | How do you typically structure unit tests and UI tests for an iOS feature to ensure it is well covered? | In what cases would you use property wrappers in Swift, and how can they help keep code expressive and reusable? | How do you reason about and handle error propagation in a Swift codebase that uses Result types or async/await? | Describe how you have used large language models while coding and how you decide whether to accept or modify their suggestions. | When evaluating an existing iOS codebase, what signals tell you that the architecture is healthy or that it needs refactoring? | How do you balance short-term delivery pressure with the need to maintain a clean and scalable iOS codebase over time?",
    "source_url": "https://editor.superannotate.com/jobs/Swift25-913"
  },
  {
    "job_id": "Swift25-913",
    "title": "Swift Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Brazil",
    "pay_rate": "$-1 - $50 / hour",
    "pay_min": -1,
    "pay_max": 50,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Swift Engineer, you will review AI-generated responses and generate high-quality Swift and iOS-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical details; write expert-level explanations and model solutions that demonstrate correct approaches in Swift and iOS; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, helping to improve the world’s premier AI models.",
    "keywords": "Swift, iOS development, SwiftUI, UIKit, Combine, Mobile architecture, ARC, UI logic review, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on Swift/iOS development experience. | Strong understanding of core Swift fundamentals, memory management, and ARC. | Professional iOS engineering experience shipping or maintaining production mobile applications. | Minimum Bachelor’s degree in Computer Science or a closely related field. | Ability to evaluate code for correctness, maintainability, performance, and UI logic quality. | Significant experience using large language models (LLMs) to assist with coding, debugging, and code review. | Excellent English writing skills with the ability to articulate complex technical concepts clearly. | Highly preferred: Experience with SwiftUI, UIKit, and Combine in modern iOS architectures such as MVVM. | Preferred: Merged pull requests in Swift/iOS open-source projects, competitive programming exposure, or similar evidence of strong algorithmic problem-solving skills. | Experience reviewing app logic or mobile architecture, with previous AI data training or model evaluation work strongly preferred, and Minimum C1 English proficiency.",
    "sample_interview_questions": "How does Automatic Reference Counting (ARC) work in Swift, and what are strong, weak, and unowned references used for? | Can you describe a situation where a retain cycle might occur in an iOS app and how you would detect and resolve it? | When would you choose a struct versus a class in Swift, and what practical implications does that choice have for iOS app design? | How do value types and reference types behave differently when passed into functions or stored in collections in Swift? | Explain how you would design a networking layer for an iOS app using URLSession or a similar abstraction. | What are the key differences between UIKit and SwiftUI, and when might you prefer one over the other in a new feature? | How do you approach organizing view and business logic when using MVVM or another modern iOS architecture pattern? | Can you walk through how Combine or another reactive framework can be used to manage asynchronous data flows in an iOS app? | What strategies do you use to keep view controllers lean and maintainable in a UIKit-based application? | How would you analyze and improve the performance of a scrolling list that feels laggy on certain devices? | What are some common pitfalls you have seen in iOS concurrency, and how do you avoid issues when working with the main thread? | How do you approach reviewing another engineer’s Swift code for readability, correctness, and long-term maintainability? | Describe how you would validate that a complex UI flow behaves correctly given different user inputs and edge cases. | What techniques do you use to debug tricky layout or Auto Layout issues in iOS interfaces? | How do you typically structure unit tests and UI tests for an iOS feature to ensure it is well covered? | In what cases would you use property wrappers in Swift, and how can they help keep code expressive and reusable? | How do you reason about and handle error propagation in a Swift codebase that uses Result types or async/await? | Describe how you have used large language models while coding and how you decide whether to accept or modify their suggestions. | When evaluating an existing iOS codebase, what signals tell you that the architecture is healthy or that it needs refactoring? | How do you balance short-term delivery pressure with the need to maintain a clean and scalable iOS codebase over time?",
    "source_url": "https://editor.superannotate.com/jobs/Swift25-913"
  },
  {
    "job_id": "Swift25-913",
    "title": "Swift Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Spain",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Swift Engineer, you will review AI-generated responses and generate high-quality Swift and iOS-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical details; write expert-level explanations and model solutions that demonstrate correct approaches in Swift and iOS; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, helping to improve the world’s premier AI models.",
    "keywords": "Swift, iOS development, SwiftUI, UIKit, Combine, Mobile architecture, ARC, UI logic review, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on Swift/iOS development experience. | Strong understanding of core Swift fundamentals, memory management, and ARC. | Professional iOS engineering experience shipping or maintaining production mobile applications. | Minimum Bachelor’s degree in Computer Science or a closely related field. | Ability to evaluate code for correctness, maintainability, performance, and UI logic quality. | Significant experience using large language models (LLMs) to assist with coding, debugging, and code review. | Excellent English writing skills with the ability to articulate complex technical concepts clearly. | Highly preferred: Experience with SwiftUI, UIKit, and Combine in modern iOS architectures such as MVVM. | Preferred: Merged pull requests in Swift/iOS open-source projects, competitive programming exposure, or similar evidence of strong algorithmic problem-solving skills. | Experience reviewing app logic or mobile architecture, with previous AI data training or model evaluation work strongly preferred, and Minimum C1 English proficiency.",
    "sample_interview_questions": "How does Automatic Reference Counting (ARC) work in Swift, and what are strong, weak, and unowned references used for? | Can you describe a situation where a retain cycle might occur in an iOS app and how you would detect and resolve it? | When would you choose a struct versus a class in Swift, and what practical implications does that choice have for iOS app design? | How do value types and reference types behave differently when passed into functions or stored in collections in Swift? | Explain how you would design a networking layer for an iOS app using URLSession or a similar abstraction. | What are the key differences between UIKit and SwiftUI, and when might you prefer one over the other in a new feature? | How do you approach organizing view and business logic when using MVVM or another modern iOS architecture pattern? | Can you walk through how Combine or another reactive framework can be used to manage asynchronous data flows in an iOS app? | What strategies do you use to keep view controllers lean and maintainable in a UIKit-based application? | How would you analyze and improve the performance of a scrolling list that feels laggy on certain devices? | What are some common pitfalls you have seen in iOS concurrency, and how do you avoid issues when working with the main thread? | How do you approach reviewing another engineer’s Swift code for readability, correctness, and long-term maintainability? | Describe how you would validate that a complex UI flow behaves correctly given different user inputs and edge cases. | What techniques do you use to debug tricky layout or Auto Layout issues in iOS interfaces? | How do you typically structure unit tests and UI tests for an iOS feature to ensure it is well covered? | In what cases would you use property wrappers in Swift, and how can they help keep code expressive and reusable? | How do you reason about and handle error propagation in a Swift codebase that uses Result types or async/await? | Describe how you have used large language models while coding and how you decide whether to accept or modify their suggestions. | When evaluating an existing iOS codebase, what signals tell you that the architecture is healthy or that it needs refactoring? | How do you balance short-term delivery pressure with the need to maintain a clean and scalable iOS codebase over time?",
    "source_url": "https://editor.superannotate.com/jobs/Swift25-913"
  },
  {
    "job_id": "Swift25-913",
    "title": "Swift Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Portugal",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Swift Engineer, you will review AI-generated responses and generate high-quality Swift and iOS-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical details; write expert-level explanations and model solutions that demonstrate correct approaches in Swift and iOS; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, helping to improve the world’s premier AI models.",
    "keywords": "Swift, iOS development, SwiftUI, UIKit, Combine, Mobile architecture, ARC, UI logic review, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on Swift/iOS development experience. | Strong understanding of core Swift fundamentals, memory management, and ARC. | Professional iOS engineering experience shipping or maintaining production mobile applications. | Minimum Bachelor’s degree in Computer Science or a closely related field. | Ability to evaluate code for correctness, maintainability, performance, and UI logic quality. | Significant experience using large language models (LLMs) to assist with coding, debugging, and code review. | Excellent English writing skills with the ability to articulate complex technical concepts clearly. | Highly preferred: Experience with SwiftUI, UIKit, and Combine in modern iOS architectures such as MVVM. | Preferred: Merged pull requests in Swift/iOS open-source projects, competitive programming exposure, or similar evidence of strong algorithmic problem-solving skills. | Experience reviewing app logic or mobile architecture, with previous AI data training or model evaluation work strongly preferred, and Minimum C1 English proficiency.",
    "sample_interview_questions": "How does Automatic Reference Counting (ARC) work in Swift, and what are strong, weak, and unowned references used for? | Can you describe a situation where a retain cycle might occur in an iOS app and how you would detect and resolve it? | When would you choose a struct versus a class in Swift, and what practical implications does that choice have for iOS app design? | How do value types and reference types behave differently when passed into functions or stored in collections in Swift? | Explain how you would design a networking layer for an iOS app using URLSession or a similar abstraction. | What are the key differences between UIKit and SwiftUI, and when might you prefer one over the other in a new feature? | How do you approach organizing view and business logic when using MVVM or another modern iOS architecture pattern? | Can you walk through how Combine or another reactive framework can be used to manage asynchronous data flows in an iOS app? | What strategies do you use to keep view controllers lean and maintainable in a UIKit-based application? | How would you analyze and improve the performance of a scrolling list that feels laggy on certain devices? | What are some common pitfalls you have seen in iOS concurrency, and how do you avoid issues when working with the main thread? | How do you approach reviewing another engineer’s Swift code for readability, correctness, and long-term maintainability? | Describe how you would validate that a complex UI flow behaves correctly given different user inputs and edge cases. | What techniques do you use to debug tricky layout or Auto Layout issues in iOS interfaces? | How do you typically structure unit tests and UI tests for an iOS feature to ensure it is well covered? | In what cases would you use property wrappers in Swift, and how can they help keep code expressive and reusable? | How do you reason about and handle error propagation in a Swift codebase that uses Result types or async/await? | Describe how you have used large language models while coding and how you decide whether to accept or modify their suggestions. | When evaluating an existing iOS codebase, what signals tell you that the architecture is healthy or that it needs refactoring? | How do you balance short-term delivery pressure with the need to maintain a clean and scalable iOS codebase over time?",
    "source_url": "https://editor.superannotate.com/jobs/Swift25-913"
  },
  {
    "job_id": "Bash-25-913",
    "title": "Bash/PowerShell Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Bangladesh",
    "pay_rate": "$-1 - $18 / hour",
    "pay_min": -1,
    "pay_max": 18,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Bash/PowerShell Engineer, you will review AI-generated responses and generate high-quality automation-focused content, evaluating the reasoning quality and step-by-step problem-solving behind scripts and troubleshooting workflows. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in logic, safety, or portability; fact-check technical details; write expert-level explanations and model solutions that demonstrate correct scripting patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, directly helping to improve the world’s premier AI models.",
    "keywords": "Bash, PowerShell, Shell scripting, DevOps automation, Infrastructure scripting, CI/CD pipelines, Azure PowerShell, awk sed grep, LLM-assisted coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Expert-level proficiency in Bash and/or PowerShell scripting. | 2–3+ years of hands-on experience in at least one of Bash or PowerShell. | Strong experience writing, debugging, and maintaining shell or object-based automation scripts. | Ability to evaluate scripts for correctness, readability, safety, performance, and portability across environments. | Professional experience in software engineering, DevOps, IT automation, or infrastructure-focused roles. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Significant experience using large language models (LLMs) while coding, troubleshooting, and reviewing scripts. | Excellent English writing skills with the ability to document and explain complex automation clearly. | Highly preferred: For Bash, strong comfort with awk, sed, and grep; for PowerShell, experience with PowerShell Core, DSC, and Azure PowerShell modules. | Preferred: Merged PRs in CLI, DevOps, or infrastructure-related open-source projects; familiarity with CI pipelines, automation workflows, or cloud scripting; previous AI data training or model evaluation work strongly preferred and Minimum C1 English proficiency.",
    "sample_interview_questions": "How do you typically structure and organize a larger Bash or PowerShell script to keep it readable and maintainable over time? | Can you explain the main differences between Bash and PowerShell in terms of how they handle data, pipelines, and error handling? | When debugging a failing script in a production-like environment, what is your step-by-step approach to identifying and isolating the root cause? | How do you evaluate whether a script is safe to run on multiple environments, and what practices do you follow to avoid destructive operations? | What techniques do you use in Bash or PowerShell to make scripts idempotent and reliable when used in automation workflows? | Can you describe how you manage configuration, secrets, and environment-specific values within automation scripts? | How do you design scripts so they fail gracefully and provide useful diagnostic information when something goes wrong? | In Bash, how do tools like awk, sed, and grep fit into your typical data-processing or log-parsing workflows? | In PowerShell, how do you take advantage of objects, pipelines, and built-in cmdlets to manipulate and query data effectively? | What factors do you consider when deciding whether a task should be implemented as a one-off script, a reusable module, or part of a larger automation framework? | How do you approach reviewing another engineer’s shell or PowerShell script for correctness, readability, and long-term maintainability? | Can you describe an example where you improved the performance or robustness of an existing automation script and what changes you made? | How do you ensure your scripts are portable across different operating systems, shells, or PowerShell editions when needed? | What is your approach to logging, tracing, and monitoring within scripts that run as part of CI/CD pipelines or scheduled jobs? | How do you think about security concerns such as command injection, unsafe input handling, or misuse of elevated privileges in scripts? | When working with cloud platforms or remote infrastructure, how do you structure scripts that interact with APIs, CLIs, or remote sessions? | How have you used large language models while writing or reviewing scripts, and what criteria do you use to validate their suggestions? | What strategies do you follow to keep a growing collection of automation scripts discoverable, documented, and consistent across a team? | How do you test and validate changes to important automation scripts before they are used in production environments? | When examining an unfamiliar script, what aspects do you look at first to assess its quality, safety, and alignment with best practices?",
    "source_url": "https://editor.superannotate.com/jobs/Bash-25-913"
  },
  {
    "job_id": "Bash-25-913",
    "title": "Bash/PowerShell Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Egypt",
    "pay_rate": "$-1 - $25 / hour",
    "pay_min": -1,
    "pay_max": 25,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Bash/PowerShell Engineer, you will review AI-generated responses and generate high-quality automation-focused content, evaluating the reasoning quality and step-by-step problem-solving behind scripts and troubleshooting workflows. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in logic, safety, or portability; fact-check technical details; write expert-level explanations and model solutions that demonstrate correct scripting patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, directly helping to improve the world’s premier AI models.",
    "keywords": "Bash, PowerShell, Shell scripting, DevOps automation, Infrastructure scripting, CI/CD pipelines, Azure PowerShell, awk sed grep, LLM-assisted coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Expert-level proficiency in Bash and/or PowerShell scripting. | 2–3+ years of hands-on experience in at least one of Bash or PowerShell. | Strong experience writing, debugging, and maintaining shell or object-based automation scripts. | Ability to evaluate scripts for correctness, readability, safety, performance, and portability across environments. | Professional experience in software engineering, DevOps, IT automation, or infrastructure-focused roles. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Significant experience using large language models (LLMs) while coding, troubleshooting, and reviewing scripts. | Excellent English writing skills with the ability to document and explain complex automation clearly. | Highly preferred: For Bash, strong comfort with awk, sed, and grep; for PowerShell, experience with PowerShell Core, DSC, and Azure PowerShell modules. | Preferred: Merged PRs in CLI, DevOps, or infrastructure-related open-source projects; familiarity with CI pipelines, automation workflows, or cloud scripting; previous AI data training or model evaluation work strongly preferred and Minimum C1 English proficiency.",
    "sample_interview_questions": "How do you typically structure and organize a larger Bash or PowerShell script to keep it readable and maintainable over time? | Can you explain the main differences between Bash and PowerShell in terms of how they handle data, pipelines, and error handling? | When debugging a failing script in a production-like environment, what is your step-by-step approach to identifying and isolating the root cause? | How do you evaluate whether a script is safe to run on multiple environments, and what practices do you follow to avoid destructive operations? | What techniques do you use in Bash or PowerShell to make scripts idempotent and reliable when used in automation workflows? | Can you describe how you manage configuration, secrets, and environment-specific values within automation scripts? | How do you design scripts so they fail gracefully and provide useful diagnostic information when something goes wrong? | In Bash, how do tools like awk, sed, and grep fit into your typical data-processing or log-parsing workflows? | In PowerShell, how do you take advantage of objects, pipelines, and built-in cmdlets to manipulate and query data effectively? | What factors do you consider when deciding whether a task should be implemented as a one-off script, a reusable module, or part of a larger automation framework? | How do you approach reviewing another engineer’s shell or PowerShell script for correctness, readability, and long-term maintainability? | Can you describe an example where you improved the performance or robustness of an existing automation script and what changes you made? | How do you ensure your scripts are portable across different operating systems, shells, or PowerShell editions when needed? | What is your approach to logging, tracing, and monitoring within scripts that run as part of CI/CD pipelines or scheduled jobs? | How do you think about security concerns such as command injection, unsafe input handling, or misuse of elevated privileges in scripts? | When working with cloud platforms or remote infrastructure, how do you structure scripts that interact with APIs, CLIs, or remote sessions? | How have you used large language models while writing or reviewing scripts, and what criteria do you use to validate their suggestions? | What strategies do you follow to keep a growing collection of automation scripts discoverable, documented, and consistent across a team? | How do you test and validate changes to important automation scripts before they are used in production environments? | When examining an unfamiliar script, what aspects do you look at first to assess its quality, safety, and alignment with best practices?",
    "source_url": "https://editor.superannotate.com/jobs/Bash-25-913"
  },
  {
    "job_id": "Bash-25-913",
    "title": "Bash/PowerShell Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "India",
    "pay_rate": "$-1 - $25 / hour",
    "pay_min": -1,
    "pay_max": 25,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Bash/PowerShell Engineer, you will review AI-generated responses and generate high-quality automation-focused content, evaluating the reasoning quality and step-by-step problem-solving behind scripts and troubleshooting workflows. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in logic, safety, or portability; fact-check technical details; write expert-level explanations and model solutions that demonstrate correct scripting patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, directly helping to improve the world’s premier AI models.",
    "keywords": "Bash, PowerShell, Shell scripting, DevOps automation, Infrastructure scripting, CI/CD pipelines, Azure PowerShell, awk sed grep, LLM-assisted coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Expert-level proficiency in Bash and/or PowerShell scripting. | 2–3+ years of hands-on experience in at least one of Bash or PowerShell. | Strong experience writing, debugging, and maintaining shell or object-based automation scripts. | Ability to evaluate scripts for correctness, readability, safety, performance, and portability across environments. | Professional experience in software engineering, DevOps, IT automation, or infrastructure-focused roles. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Significant experience using large language models (LLMs) while coding, troubleshooting, and reviewing scripts. | Excellent English writing skills with the ability to document and explain complex automation clearly. | Highly preferred: For Bash, strong comfort with awk, sed, and grep; for PowerShell, experience with PowerShell Core, DSC, and Azure PowerShell modules. | Preferred: Merged PRs in CLI, DevOps, or infrastructure-related open-source projects; familiarity with CI pipelines, automation workflows, or cloud scripting; previous AI data training or model evaluation work strongly preferred and Minimum C1 English proficiency.",
    "sample_interview_questions": "How do you typically structure and organize a larger Bash or PowerShell script to keep it readable and maintainable over time? | Can you explain the main differences between Bash and PowerShell in terms of how they handle data, pipelines, and error handling? | When debugging a failing script in a production-like environment, what is your step-by-step approach to identifying and isolating the root cause? | How do you evaluate whether a script is safe to run on multiple environments, and what practices do you follow to avoid destructive operations? | What techniques do you use in Bash or PowerShell to make scripts idempotent and reliable when used in automation workflows? | Can you describe how you manage configuration, secrets, and environment-specific values within automation scripts? | How do you design scripts so they fail gracefully and provide useful diagnostic information when something goes wrong? | In Bash, how do tools like awk, sed, and grep fit into your typical data-processing or log-parsing workflows? | In PowerShell, how do you take advantage of objects, pipelines, and built-in cmdlets to manipulate and query data effectively? | What factors do you consider when deciding whether a task should be implemented as a one-off script, a reusable module, or part of a larger automation framework? | How do you approach reviewing another engineer’s shell or PowerShell script for correctness, readability, and long-term maintainability? | Can you describe an example where you improved the performance or robustness of an existing automation script and what changes you made? | How do you ensure your scripts are portable across different operating systems, shells, or PowerShell editions when needed? | What is your approach to logging, tracing, and monitoring within scripts that run as part of CI/CD pipelines or scheduled jobs? | How do you think about security concerns such as command injection, unsafe input handling, or misuse of elevated privileges in scripts? | When working with cloud platforms or remote infrastructure, how do you structure scripts that interact with APIs, CLIs, or remote sessions? | How have you used large language models while writing or reviewing scripts, and what criteria do you use to validate their suggestions? | What strategies do you follow to keep a growing collection of automation scripts discoverable, documented, and consistent across a team? | How do you test and validate changes to important automation scripts before they are used in production environments? | When examining an unfamiliar script, what aspects do you look at first to assess its quality, safety, and alignment with best practices?",
    "source_url": "https://editor.superannotate.com/jobs/Bash-25-913"
  },
  {
    "job_id": "Bash-25-913",
    "title": "Bash/PowerShell Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Indonesia",
    "pay_rate": "$-1 - $25 / hour",
    "pay_min": -1,
    "pay_max": 25,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Bash/PowerShell Engineer, you will review AI-generated responses and generate high-quality automation-focused content, evaluating the reasoning quality and step-by-step problem-solving behind scripts and troubleshooting workflows. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in logic, safety, or portability; fact-check technical details; write expert-level explanations and model solutions that demonstrate correct scripting patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, directly helping to improve the world’s premier AI models.",
    "keywords": "Bash, PowerShell, Shell scripting, DevOps automation, Infrastructure scripting, CI/CD pipelines, Azure PowerShell, awk sed grep, LLM-assisted coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Expert-level proficiency in Bash and/or PowerShell scripting. | 2–3+ years of hands-on experience in at least one of Bash or PowerShell. | Strong experience writing, debugging, and maintaining shell or object-based automation scripts. | Ability to evaluate scripts for correctness, readability, safety, performance, and portability across environments. | Professional experience in software engineering, DevOps, IT automation, or infrastructure-focused roles. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Significant experience using large language models (LLMs) while coding, troubleshooting, and reviewing scripts. | Excellent English writing skills with the ability to document and explain complex automation clearly. | Highly preferred: For Bash, strong comfort with awk, sed, and grep; for PowerShell, experience with PowerShell Core, DSC, and Azure PowerShell modules. | Preferred: Merged PRs in CLI, DevOps, or infrastructure-related open-source projects; familiarity with CI pipelines, automation workflows, or cloud scripting; previous AI data training or model evaluation work strongly preferred and Minimum C1 English proficiency.",
    "sample_interview_questions": "How do you typically structure and organize a larger Bash or PowerShell script to keep it readable and maintainable over time? | Can you explain the main differences between Bash and PowerShell in terms of how they handle data, pipelines, and error handling? | When debugging a failing script in a production-like environment, what is your step-by-step approach to identifying and isolating the root cause? | How do you evaluate whether a script is safe to run on multiple environments, and what practices do you follow to avoid destructive operations? | What techniques do you use in Bash or PowerShell to make scripts idempotent and reliable when used in automation workflows? | Can you describe how you manage configuration, secrets, and environment-specific values within automation scripts? | How do you design scripts so they fail gracefully and provide useful diagnostic information when something goes wrong? | In Bash, how do tools like awk, sed, and grep fit into your typical data-processing or log-parsing workflows? | In PowerShell, how do you take advantage of objects, pipelines, and built-in cmdlets to manipulate and query data effectively? | What factors do you consider when deciding whether a task should be implemented as a one-off script, a reusable module, or part of a larger automation framework? | How do you approach reviewing another engineer’s shell or PowerShell script for correctness, readability, and long-term maintainability? | Can you describe an example where you improved the performance or robustness of an existing automation script and what changes you made? | How do you ensure your scripts are portable across different operating systems, shells, or PowerShell editions when needed? | What is your approach to logging, tracing, and monitoring within scripts that run as part of CI/CD pipelines or scheduled jobs? | How do you think about security concerns such as command injection, unsafe input handling, or misuse of elevated privileges in scripts? | When working with cloud platforms or remote infrastructure, how do you structure scripts that interact with APIs, CLIs, or remote sessions? | How have you used large language models while writing or reviewing scripts, and what criteria do you use to validate their suggestions? | What strategies do you follow to keep a growing collection of automation scripts discoverable, documented, and consistent across a team? | How do you test and validate changes to important automation scripts before they are used in production environments? | When examining an unfamiliar script, what aspects do you look at first to assess its quality, safety, and alignment with best practices?",
    "source_url": "https://editor.superannotate.com/jobs/Bash-25-913"
  },
  {
    "job_id": "Bash-25-913",
    "title": "Bash/PowerShell Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Brazil",
    "pay_rate": "$-1 - $35 / hour",
    "pay_min": -1,
    "pay_max": 35,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Bash/PowerShell Engineer, you will review AI-generated responses and generate high-quality automation-focused content, evaluating the reasoning quality and step-by-step problem-solving behind scripts and troubleshooting workflows. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in logic, safety, or portability; fact-check technical details; write expert-level explanations and model solutions that demonstrate correct scripting patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, directly helping to improve the world’s premier AI models.",
    "keywords": "Bash, PowerShell, Shell scripting, DevOps automation, Infrastructure scripting, CI/CD pipelines, Azure PowerShell, awk sed grep, LLM-assisted coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Expert-level proficiency in Bash and/or PowerShell scripting. | 2–3+ years of hands-on experience in at least one of Bash or PowerShell. | Strong experience writing, debugging, and maintaining shell or object-based automation scripts. | Ability to evaluate scripts for correctness, readability, safety, performance, and portability across environments. | Professional experience in software engineering, DevOps, IT automation, or infrastructure-focused roles. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Significant experience using large language models (LLMs) while coding, troubleshooting, and reviewing scripts. | Excellent English writing skills with the ability to document and explain complex automation clearly. | Highly preferred: For Bash, strong comfort with awk, sed, and grep; for PowerShell, experience with PowerShell Core, DSC, and Azure PowerShell modules. | Preferred: Merged PRs in CLI, DevOps, or infrastructure-related open-source projects; familiarity with CI pipelines, automation workflows, or cloud scripting; previous AI data training or model evaluation work strongly preferred and Minimum C1 English proficiency.",
    "sample_interview_questions": "How do you typically structure and organize a larger Bash or PowerShell script to keep it readable and maintainable over time? | Can you explain the main differences between Bash and PowerShell in terms of how they handle data, pipelines, and error handling? | When debugging a failing script in a production-like environment, what is your step-by-step approach to identifying and isolating the root cause? | How do you evaluate whether a script is safe to run on multiple environments, and what practices do you follow to avoid destructive operations? | What techniques do you use in Bash or PowerShell to make scripts idempotent and reliable when used in automation workflows? | Can you describe how you manage configuration, secrets, and environment-specific values within automation scripts? | How do you design scripts so they fail gracefully and provide useful diagnostic information when something goes wrong? | In Bash, how do tools like awk, sed, and grep fit into your typical data-processing or log-parsing workflows? | In PowerShell, how do you take advantage of objects, pipelines, and built-in cmdlets to manipulate and query data effectively? | What factors do you consider when deciding whether a task should be implemented as a one-off script, a reusable module, or part of a larger automation framework? | How do you approach reviewing another engineer’s shell or PowerShell script for correctness, readability, and long-term maintainability? | Can you describe an example where you improved the performance or robustness of an existing automation script and what changes you made? | How do you ensure your scripts are portable across different operating systems, shells, or PowerShell editions when needed? | What is your approach to logging, tracing, and monitoring within scripts that run as part of CI/CD pipelines or scheduled jobs? | How do you think about security concerns such as command injection, unsafe input handling, or misuse of elevated privileges in scripts? | When working with cloud platforms or remote infrastructure, how do you structure scripts that interact with APIs, CLIs, or remote sessions? | How have you used large language models while writing or reviewing scripts, and what criteria do you use to validate their suggestions? | What strategies do you follow to keep a growing collection of automation scripts discoverable, documented, and consistent across a team? | How do you test and validate changes to important automation scripts before they are used in production environments? | When examining an unfamiliar script, what aspects do you look at first to assess its quality, safety, and alignment with best practices?",
    "source_url": "https://editor.superannotate.com/jobs/Bash-25-913"
  },
  {
    "job_id": "Bash-25-913",
    "title": "Bash/PowerShell Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Portugal",
    "pay_rate": "$-1 - $40 / hour",
    "pay_min": -1,
    "pay_max": 40,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Bash/PowerShell Engineer, you will review AI-generated responses and generate high-quality automation-focused content, evaluating the reasoning quality and step-by-step problem-solving behind scripts and troubleshooting workflows. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in logic, safety, or portability; fact-check technical details; write expert-level explanations and model solutions that demonstrate correct scripting patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, directly helping to improve the world’s premier AI models.",
    "keywords": "Bash, PowerShell, Shell scripting, DevOps automation, Infrastructure scripting, CI/CD pipelines, Azure PowerShell, awk sed grep, LLM-assisted coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Expert-level proficiency in Bash and/or PowerShell scripting. | 2–3+ years of hands-on experience in at least one of Bash or PowerShell. | Strong experience writing, debugging, and maintaining shell or object-based automation scripts. | Ability to evaluate scripts for correctness, readability, safety, performance, and portability across environments. | Professional experience in software engineering, DevOps, IT automation, or infrastructure-focused roles. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Significant experience using large language models (LLMs) while coding, troubleshooting, and reviewing scripts. | Excellent English writing skills with the ability to document and explain complex automation clearly. | Highly preferred: For Bash, strong comfort with awk, sed, and grep; for PowerShell, experience with PowerShell Core, DSC, and Azure PowerShell modules. | Preferred: Merged PRs in CLI, DevOps, or infrastructure-related open-source projects; familiarity with CI pipelines, automation workflows, or cloud scripting; previous AI data training or model evaluation work strongly preferred and Minimum C1 English proficiency.",
    "sample_interview_questions": "How do you typically structure and organize a larger Bash or PowerShell script to keep it readable and maintainable over time? | Can you explain the main differences between Bash and PowerShell in terms of how they handle data, pipelines, and error handling? | When debugging a failing script in a production-like environment, what is your step-by-step approach to identifying and isolating the root cause? | How do you evaluate whether a script is safe to run on multiple environments, and what practices do you follow to avoid destructive operations? | What techniques do you use in Bash or PowerShell to make scripts idempotent and reliable when used in automation workflows? | Can you describe how you manage configuration, secrets, and environment-specific values within automation scripts? | How do you design scripts so they fail gracefully and provide useful diagnostic information when something goes wrong? | In Bash, how do tools like awk, sed, and grep fit into your typical data-processing or log-parsing workflows? | In PowerShell, how do you take advantage of objects, pipelines, and built-in cmdlets to manipulate and query data effectively? | What factors do you consider when deciding whether a task should be implemented as a one-off script, a reusable module, or part of a larger automation framework? | How do you approach reviewing another engineer’s shell or PowerShell script for correctness, readability, and long-term maintainability? | Can you describe an example where you improved the performance or robustness of an existing automation script and what changes you made? | How do you ensure your scripts are portable across different operating systems, shells, or PowerShell editions when needed? | What is your approach to logging, tracing, and monitoring within scripts that run as part of CI/CD pipelines or scheduled jobs? | How do you think about security concerns such as command injection, unsafe input handling, or misuse of elevated privileges in scripts? | When working with cloud platforms or remote infrastructure, how do you structure scripts that interact with APIs, CLIs, or remote sessions? | How have you used large language models while writing or reviewing scripts, and what criteria do you use to validate their suggestions? | What strategies do you follow to keep a growing collection of automation scripts discoverable, documented, and consistent across a team? | How do you test and validate changes to important automation scripts before they are used in production environments? | When examining an unfamiliar script, what aspects do you look at first to assess its quality, safety, and alignment with best practices?",
    "source_url": "https://editor.superannotate.com/jobs/Bash-25-913"
  },
  {
    "job_id": "Bash-25-913",
    "title": "Bash/PowerShell Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Spain",
    "pay_rate": "$-1 - $45 / hour",
    "pay_min": -1,
    "pay_max": 45,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid Bash/PowerShell Engineer, you will review AI-generated responses and generate high-quality automation-focused content, evaluating the reasoning quality and step-by-step problem-solving behind scripts and troubleshooting workflows. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in logic, safety, or portability; fact-check technical details; write expert-level explanations and model solutions that demonstrate correct scripting patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, directly helping to improve the world’s premier AI models.",
    "keywords": "Bash, PowerShell, Shell scripting, DevOps automation, Infrastructure scripting, CI/CD pipelines, Azure PowerShell, awk sed grep, LLM-assisted coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Expert-level proficiency in Bash and/or PowerShell scripting. | 2–3+ years of hands-on experience in at least one of Bash or PowerShell. | Strong experience writing, debugging, and maintaining shell or object-based automation scripts. | Ability to evaluate scripts for correctness, readability, safety, performance, and portability across environments. | Professional experience in software engineering, DevOps, IT automation, or infrastructure-focused roles. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Significant experience using large language models (LLMs) while coding, troubleshooting, and reviewing scripts. | Excellent English writing skills with the ability to document and explain complex automation clearly. | Highly preferred: For Bash, strong comfort with awk, sed, and grep; for PowerShell, experience with PowerShell Core, DSC, and Azure PowerShell modules. | Preferred: Merged PRs in CLI, DevOps, or infrastructure-related open-source projects; familiarity with CI pipelines, automation workflows, or cloud scripting; previous AI data training or model evaluation work strongly preferred and Minimum C1 English proficiency.",
    "sample_interview_questions": "How do you typically structure and organize a larger Bash or PowerShell script to keep it readable and maintainable over time? | Can you explain the main differences between Bash and PowerShell in terms of how they handle data, pipelines, and error handling? | When debugging a failing script in a production-like environment, what is your step-by-step approach to identifying and isolating the root cause? | How do you evaluate whether a script is safe to run on multiple environments, and what practices do you follow to avoid destructive operations? | What techniques do you use in Bash or PowerShell to make scripts idempotent and reliable when used in automation workflows? | Can you describe how you manage configuration, secrets, and environment-specific values within automation scripts? | How do you design scripts so they fail gracefully and provide useful diagnostic information when something goes wrong? | In Bash, how do tools like awk, sed, and grep fit into your typical data-processing or log-parsing workflows? | In PowerShell, how do you take advantage of objects, pipelines, and built-in cmdlets to manipulate and query data effectively? | What factors do you consider when deciding whether a task should be implemented as a one-off script, a reusable module, or part of a larger automation framework? | How do you approach reviewing another engineer’s shell or PowerShell script for correctness, readability, and long-term maintainability? | Can you describe an example where you improved the performance or robustness of an existing automation script and what changes you made? | How do you ensure your scripts are portable across different operating systems, shells, or PowerShell editions when needed? | What is your approach to logging, tracing, and monitoring within scripts that run as part of CI/CD pipelines or scheduled jobs? | How do you think about security concerns such as command injection, unsafe input handling, or misuse of elevated privileges in scripts? | When working with cloud platforms or remote infrastructure, how do you structure scripts that interact with APIs, CLIs, or remote sessions? | How have you used large language models while writing or reviewing scripts, and what criteria do you use to validate their suggestions? | What strategies do you follow to keep a growing collection of automation scripts discoverable, documented, and consistent across a team? | How do you test and validate changes to important automation scripts before they are used in production environments? | When examining an unfamiliar script, what aspects do you look at first to assess its quality, safety, and alignment with best practices?",
    "source_url": "https://editor.superannotate.com/jobs/Bash-25-913"
  },
  {
    "job_id": "Ruby-25-982",
    "title": "Ruby Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Bangladesh",
    "pay_rate": "$-1 - $22 / hour",
    "pay_min": -1,
    "pay_max": 22,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Ruby Engineer for AI Data Training, you will review AI-generated Ruby and Rails code or generate your own solutions, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for readability, maintainability, and correctness; identify errors in MVC structure, domain modeling, or control flow; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Ruby patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Ruby, Ruby on Rails, MVC, Backend development, Web applications, RSpec, Metaprogramming, Refactoring, Code review, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Ruby programming experience in production environments. | Strong understanding of Ruby on Rails, MVC architecture, and idiomatic Ruby patterns for structuring application code. | Hands-on software engineering experience building and maintaining Ruby or Rails applications, with exposure to real-world production constraints. | Ability to evaluate readability, maintainability, and logical correctness in Ruby and Rails code, including model, controller, and service-layer design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating and reviewing their output. | Excellent English writing skills, capable of producing clear, structured, and pedagogical technical explanations and code reviews. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, large-scale code review, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: experience with frameworks and tools such as Ruby on Rails, Sinatra, and RSpec in real projects. | Preferred: merged PRs in Ruby open-source projects, experience with metaprogramming or test-heavy systems, and competitive programming exposure.",
    "sample_interview_questions": "How would you describe the Ruby language’s philosophy and core idioms, and how does that influence how you structure code in real projects? | Can you explain the MVC pattern in the context of Ruby on Rails and how responsibilities should be divided between models, views, and controllers? | When reviewing a Rails controller, what are the main indicators that too much business logic has leaked into the controller layer? | How do you decide when to extract code into service objects, concerns, or modules in a Rails application to improve maintainability? | What are some common Ruby idioms you look for when evaluating whether code is idiomatic versus simply functional but harder to read? | How do you approach reasoning about callbacks, validations, and associations in Active Record models to keep them understandable and testable? | When you encounter metaprogramming in a Ruby codebase, what criteria do you use to decide whether it is justified or overcomplicating the design? | How do you evaluate the readability and maintainability of a complex Ruby method that chains multiple enumerable operations or uses blocks heavily? | What strategies do you use to ensure that Rails routing and controller structure remain coherent as an application grows in size and features? | How do you think about error handling and exception usage in Ruby and Rails applications, and what patterns help keep this logic clear? | Can you describe how you would review an RSpec test suite for clarity, coverage, and resilience to refactoring? | When looking at database queries generated by Active Record, what do you look for to identify potential performance or N+1 query problems? | How do you evaluate whether a Rails application’s configuration, environment handling, and secrets management follow best practices for security and maintainability? | What is your approach to reviewing code that heavily uses Ruby blocks, procs, or lambdas, and how do you judge whether the abstraction is appropriate? | How do you ensure that view templates or view components remain simple, testable, and free from excessive business logic? | When reviewing a pull request in a Ruby or Rails project, what is your personal checklist for readability, logic correctness, and alignment with style guidelines? | How do you assess the impact of a proposed refactor on an existing Ruby codebase, especially in systems with a large and critical test suite? | What aspects of a Ruby on Rails application do you pay attention to when evaluating for long-term maintainability, such as naming, directory structure, or dependency management? | How do you critically evaluate suggestions from LLMs or code assistants for Ruby or Rails code, and what types of mistakes do you most often catch? | Can you describe an instance where you improved an existing Ruby or Rails codebase for clarity, maintainability, or testability, and what concrete steps you took?",
    "source_url": "https://editor.superannotate.com/jobs/Ruby-25-982"
  },
  {
    "job_id": "Ruby-25-982",
    "title": "Ruby Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Egypt",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Ruby Engineer for AI Data Training, you will review AI-generated Ruby and Rails code or generate your own solutions, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for readability, maintainability, and correctness; identify errors in MVC structure, domain modeling, or control flow; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Ruby patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Ruby, Ruby on Rails, MVC, Backend development, Web applications, RSpec, Metaprogramming, Refactoring, Code review, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Ruby programming experience in production environments. | Strong understanding of Ruby on Rails, MVC architecture, and idiomatic Ruby patterns for structuring application code. | Hands-on software engineering experience building and maintaining Ruby or Rails applications, with exposure to real-world production constraints. | Ability to evaluate readability, maintainability, and logical correctness in Ruby and Rails code, including model, controller, and service-layer design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating and reviewing their output. | Excellent English writing skills, capable of producing clear, structured, and pedagogical technical explanations and code reviews. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, large-scale code review, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: experience with frameworks and tools such as Ruby on Rails, Sinatra, and RSpec in real projects. | Preferred: merged PRs in Ruby open-source projects, experience with metaprogramming or test-heavy systems, and competitive programming exposure.",
    "sample_interview_questions": "How would you describe the Ruby language’s philosophy and core idioms, and how does that influence how you structure code in real projects? | Can you explain the MVC pattern in the context of Ruby on Rails and how responsibilities should be divided between models, views, and controllers? | When reviewing a Rails controller, what are the main indicators that too much business logic has leaked into the controller layer? | How do you decide when to extract code into service objects, concerns, or modules in a Rails application to improve maintainability? | What are some common Ruby idioms you look for when evaluating whether code is idiomatic versus simply functional but harder to read? | How do you approach reasoning about callbacks, validations, and associations in Active Record models to keep them understandable and testable? | When you encounter metaprogramming in a Ruby codebase, what criteria do you use to decide whether it is justified or overcomplicating the design? | How do you evaluate the readability and maintainability of a complex Ruby method that chains multiple enumerable operations or uses blocks heavily? | What strategies do you use to ensure that Rails routing and controller structure remain coherent as an application grows in size and features? | How do you think about error handling and exception usage in Ruby and Rails applications, and what patterns help keep this logic clear? | Can you describe how you would review an RSpec test suite for clarity, coverage, and resilience to refactoring? | When looking at database queries generated by Active Record, what do you look for to identify potential performance or N+1 query problems? | How do you evaluate whether a Rails application’s configuration, environment handling, and secrets management follow best practices for security and maintainability? | What is your approach to reviewing code that heavily uses Ruby blocks, procs, or lambdas, and how do you judge whether the abstraction is appropriate? | How do you ensure that view templates or view components remain simple, testable, and free from excessive business logic? | When reviewing a pull request in a Ruby or Rails project, what is your personal checklist for readability, logic correctness, and alignment with style guidelines? | How do you assess the impact of a proposed refactor on an existing Ruby codebase, especially in systems with a large and critical test suite? | What aspects of a Ruby on Rails application do you pay attention to when evaluating for long-term maintainability, such as naming, directory structure, or dependency management? | How do you critically evaluate suggestions from LLMs or code assistants for Ruby or Rails code, and what types of mistakes do you most often catch? | Can you describe an instance where you improved an existing Ruby or Rails codebase for clarity, maintainability, or testability, and what concrete steps you took?",
    "source_url": "https://editor.superannotate.com/jobs/Ruby-25-982"
  },
  {
    "job_id": "Ruby-25-982",
    "title": "Ruby Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Indonesia",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Ruby Engineer for AI Data Training, you will review AI-generated Ruby and Rails code or generate your own solutions, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for readability, maintainability, and correctness; identify errors in MVC structure, domain modeling, or control flow; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Ruby patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Ruby, Ruby on Rails, MVC, Backend development, Web applications, RSpec, Metaprogramming, Refactoring, Code review, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Ruby programming experience in production environments. | Strong understanding of Ruby on Rails, MVC architecture, and idiomatic Ruby patterns for structuring application code. | Hands-on software engineering experience building and maintaining Ruby or Rails applications, with exposure to real-world production constraints. | Ability to evaluate readability, maintainability, and logical correctness in Ruby and Rails code, including model, controller, and service-layer design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating and reviewing their output. | Excellent English writing skills, capable of producing clear, structured, and pedagogical technical explanations and code reviews. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, large-scale code review, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: experience with frameworks and tools such as Ruby on Rails, Sinatra, and RSpec in real projects. | Preferred: merged PRs in Ruby open-source projects, experience with metaprogramming or test-heavy systems, and competitive programming exposure.",
    "sample_interview_questions": "How would you describe the Ruby language’s philosophy and core idioms, and how does that influence how you structure code in real projects? | Can you explain the MVC pattern in the context of Ruby on Rails and how responsibilities should be divided between models, views, and controllers? | When reviewing a Rails controller, what are the main indicators that too much business logic has leaked into the controller layer? | How do you decide when to extract code into service objects, concerns, or modules in a Rails application to improve maintainability? | What are some common Ruby idioms you look for when evaluating whether code is idiomatic versus simply functional but harder to read? | How do you approach reasoning about callbacks, validations, and associations in Active Record models to keep them understandable and testable? | When you encounter metaprogramming in a Ruby codebase, what criteria do you use to decide whether it is justified or overcomplicating the design? | How do you evaluate the readability and maintainability of a complex Ruby method that chains multiple enumerable operations or uses blocks heavily? | What strategies do you use to ensure that Rails routing and controller structure remain coherent as an application grows in size and features? | How do you think about error handling and exception usage in Ruby and Rails applications, and what patterns help keep this logic clear? | Can you describe how you would review an RSpec test suite for clarity, coverage, and resilience to refactoring? | When looking at database queries generated by Active Record, what do you look for to identify potential performance or N+1 query problems? | How do you evaluate whether a Rails application’s configuration, environment handling, and secrets management follow best practices for security and maintainability? | What is your approach to reviewing code that heavily uses Ruby blocks, procs, or lambdas, and how do you judge whether the abstraction is appropriate? | How do you ensure that view templates or view components remain simple, testable, and free from excessive business logic? | When reviewing a pull request in a Ruby or Rails project, what is your personal checklist for readability, logic correctness, and alignment with style guidelines? | How do you assess the impact of a proposed refactor on an existing Ruby codebase, especially in systems with a large and critical test suite? | What aspects of a Ruby on Rails application do you pay attention to when evaluating for long-term maintainability, such as naming, directory structure, or dependency management? | How do you critically evaluate suggestions from LLMs or code assistants for Ruby or Rails code, and what types of mistakes do you most often catch? | Can you describe an instance where you improved an existing Ruby or Rails codebase for clarity, maintainability, or testability, and what concrete steps you took?",
    "source_url": "https://editor.superannotate.com/jobs/Ruby-25-982"
  },
  {
    "job_id": "Ruby-25-982",
    "title": "Ruby Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "India",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Ruby Engineer for AI Data Training, you will review AI-generated Ruby and Rails code or generate your own solutions, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for readability, maintainability, and correctness; identify errors in MVC structure, domain modeling, or control flow; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Ruby patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Ruby, Ruby on Rails, MVC, Backend development, Web applications, RSpec, Metaprogramming, Refactoring, Code review, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Ruby programming experience in production environments. | Strong understanding of Ruby on Rails, MVC architecture, and idiomatic Ruby patterns for structuring application code. | Hands-on software engineering experience building and maintaining Ruby or Rails applications, with exposure to real-world production constraints. | Ability to evaluate readability, maintainability, and logical correctness in Ruby and Rails code, including model, controller, and service-layer design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating and reviewing their output. | Excellent English writing skills, capable of producing clear, structured, and pedagogical technical explanations and code reviews. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, large-scale code review, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: experience with frameworks and tools such as Ruby on Rails, Sinatra, and RSpec in real projects. | Preferred: merged PRs in Ruby open-source projects, experience with metaprogramming or test-heavy systems, and competitive programming exposure.",
    "sample_interview_questions": "How would you describe the Ruby language’s philosophy and core idioms, and how does that influence how you structure code in real projects? | Can you explain the MVC pattern in the context of Ruby on Rails and how responsibilities should be divided between models, views, and controllers? | When reviewing a Rails controller, what are the main indicators that too much business logic has leaked into the controller layer? | How do you decide when to extract code into service objects, concerns, or modules in a Rails application to improve maintainability? | What are some common Ruby idioms you look for when evaluating whether code is idiomatic versus simply functional but harder to read? | How do you approach reasoning about callbacks, validations, and associations in Active Record models to keep them understandable and testable? | When you encounter metaprogramming in a Ruby codebase, what criteria do you use to decide whether it is justified or overcomplicating the design? | How do you evaluate the readability and maintainability of a complex Ruby method that chains multiple enumerable operations or uses blocks heavily? | What strategies do you use to ensure that Rails routing and controller structure remain coherent as an application grows in size and features? | How do you think about error handling and exception usage in Ruby and Rails applications, and what patterns help keep this logic clear? | Can you describe how you would review an RSpec test suite for clarity, coverage, and resilience to refactoring? | When looking at database queries generated by Active Record, what do you look for to identify potential performance or N+1 query problems? | How do you evaluate whether a Rails application’s configuration, environment handling, and secrets management follow best practices for security and maintainability? | What is your approach to reviewing code that heavily uses Ruby blocks, procs, or lambdas, and how do you judge whether the abstraction is appropriate? | How do you ensure that view templates or view components remain simple, testable, and free from excessive business logic? | When reviewing a pull request in a Ruby or Rails project, what is your personal checklist for readability, logic correctness, and alignment with style guidelines? | How do you assess the impact of a proposed refactor on an existing Ruby codebase, especially in systems with a large and critical test suite? | What aspects of a Ruby on Rails application do you pay attention to when evaluating for long-term maintainability, such as naming, directory structure, or dependency management? | How do you critically evaluate suggestions from LLMs or code assistants for Ruby or Rails code, and what types of mistakes do you most often catch? | Can you describe an instance where you improved an existing Ruby or Rails codebase for clarity, maintainability, or testability, and what concrete steps you took?",
    "source_url": "https://editor.superannotate.com/jobs/Ruby-25-982"
  },
  {
    "job_id": "Ruby-25-982",
    "title": "Ruby Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Brazil",
    "pay_rate": "$-1 - $45 / hour",
    "pay_min": -1,
    "pay_max": 45,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Ruby Engineer for AI Data Training, you will review AI-generated Ruby and Rails code or generate your own solutions, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for readability, maintainability, and correctness; identify errors in MVC structure, domain modeling, or control flow; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Ruby patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Ruby, Ruby on Rails, MVC, Backend development, Web applications, RSpec, Metaprogramming, Refactoring, Code review, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Ruby programming experience in production environments. | Strong understanding of Ruby on Rails, MVC architecture, and idiomatic Ruby patterns for structuring application code. | Hands-on software engineering experience building and maintaining Ruby or Rails applications, with exposure to real-world production constraints. | Ability to evaluate readability, maintainability, and logical correctness in Ruby and Rails code, including model, controller, and service-layer design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating and reviewing their output. | Excellent English writing skills, capable of producing clear, structured, and pedagogical technical explanations and code reviews. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, large-scale code review, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: experience with frameworks and tools such as Ruby on Rails, Sinatra, and RSpec in real projects. | Preferred: merged PRs in Ruby open-source projects, experience with metaprogramming or test-heavy systems, and competitive programming exposure.",
    "sample_interview_questions": "How would you describe the Ruby language’s philosophy and core idioms, and how does that influence how you structure code in real projects? | Can you explain the MVC pattern in the context of Ruby on Rails and how responsibilities should be divided between models, views, and controllers? | When reviewing a Rails controller, what are the main indicators that too much business logic has leaked into the controller layer? | How do you decide when to extract code into service objects, concerns, or modules in a Rails application to improve maintainability? | What are some common Ruby idioms you look for when evaluating whether code is idiomatic versus simply functional but harder to read? | How do you approach reasoning about callbacks, validations, and associations in Active Record models to keep them understandable and testable? | When you encounter metaprogramming in a Ruby codebase, what criteria do you use to decide whether it is justified or overcomplicating the design? | How do you evaluate the readability and maintainability of a complex Ruby method that chains multiple enumerable operations or uses blocks heavily? | What strategies do you use to ensure that Rails routing and controller structure remain coherent as an application grows in size and features? | How do you think about error handling and exception usage in Ruby and Rails applications, and what patterns help keep this logic clear? | Can you describe how you would review an RSpec test suite for clarity, coverage, and resilience to refactoring? | When looking at database queries generated by Active Record, what do you look for to identify potential performance or N+1 query problems? | How do you evaluate whether a Rails application’s configuration, environment handling, and secrets management follow best practices for security and maintainability? | What is your approach to reviewing code that heavily uses Ruby blocks, procs, or lambdas, and how do you judge whether the abstraction is appropriate? | How do you ensure that view templates or view components remain simple, testable, and free from excessive business logic? | When reviewing a pull request in a Ruby or Rails project, what is your personal checklist for readability, logic correctness, and alignment with style guidelines? | How do you assess the impact of a proposed refactor on an existing Ruby codebase, especially in systems with a large and critical test suite? | What aspects of a Ruby on Rails application do you pay attention to when evaluating for long-term maintainability, such as naming, directory structure, or dependency management? | How do you critically evaluate suggestions from LLMs or code assistants for Ruby or Rails code, and what types of mistakes do you most often catch? | Can you describe an instance where you improved an existing Ruby or Rails codebase for clarity, maintainability, or testability, and what concrete steps you took?",
    "source_url": "https://editor.superannotate.com/jobs/Ruby-25-982"
  },
  {
    "job_id": "Ruby-25-982",
    "title": "Ruby Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Portugal",
    "pay_rate": "$-1 - $50 / hour",
    "pay_min": -1,
    "pay_max": 50,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Ruby Engineer for AI Data Training, you will review AI-generated Ruby and Rails code or generate your own solutions, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for readability, maintainability, and correctness; identify errors in MVC structure, domain modeling, or control flow; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Ruby patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Ruby, Ruby on Rails, MVC, Backend development, Web applications, RSpec, Metaprogramming, Refactoring, Code review, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Ruby programming experience in production environments. | Strong understanding of Ruby on Rails, MVC architecture, and idiomatic Ruby patterns for structuring application code. | Hands-on software engineering experience building and maintaining Ruby or Rails applications, with exposure to real-world production constraints. | Ability to evaluate readability, maintainability, and logical correctness in Ruby and Rails code, including model, controller, and service-layer design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating and reviewing their output. | Excellent English writing skills, capable of producing clear, structured, and pedagogical technical explanations and code reviews. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, large-scale code review, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: experience with frameworks and tools such as Ruby on Rails, Sinatra, and RSpec in real projects. | Preferred: merged PRs in Ruby open-source projects, experience with metaprogramming or test-heavy systems, and competitive programming exposure.",
    "sample_interview_questions": "How would you describe the Ruby language’s philosophy and core idioms, and how does that influence how you structure code in real projects? | Can you explain the MVC pattern in the context of Ruby on Rails and how responsibilities should be divided between models, views, and controllers? | When reviewing a Rails controller, what are the main indicators that too much business logic has leaked into the controller layer? | How do you decide when to extract code into service objects, concerns, or modules in a Rails application to improve maintainability? | What are some common Ruby idioms you look for when evaluating whether code is idiomatic versus simply functional but harder to read? | How do you approach reasoning about callbacks, validations, and associations in Active Record models to keep them understandable and testable? | When you encounter metaprogramming in a Ruby codebase, what criteria do you use to decide whether it is justified or overcomplicating the design? | How do you evaluate the readability and maintainability of a complex Ruby method that chains multiple enumerable operations or uses blocks heavily? | What strategies do you use to ensure that Rails routing and controller structure remain coherent as an application grows in size and features? | How do you think about error handling and exception usage in Ruby and Rails applications, and what patterns help keep this logic clear? | Can you describe how you would review an RSpec test suite for clarity, coverage, and resilience to refactoring? | When looking at database queries generated by Active Record, what do you look for to identify potential performance or N+1 query problems? | How do you evaluate whether a Rails application’s configuration, environment handling, and secrets management follow best practices for security and maintainability? | What is your approach to reviewing code that heavily uses Ruby blocks, procs, or lambdas, and how do you judge whether the abstraction is appropriate? | How do you ensure that view templates or view components remain simple, testable, and free from excessive business logic? | When reviewing a pull request in a Ruby or Rails project, what is your personal checklist for readability, logic correctness, and alignment with style guidelines? | How do you assess the impact of a proposed refactor on an existing Ruby codebase, especially in systems with a large and critical test suite? | What aspects of a Ruby on Rails application do you pay attention to when evaluating for long-term maintainability, such as naming, directory structure, or dependency management? | How do you critically evaluate suggestions from LLMs or code assistants for Ruby or Rails code, and what types of mistakes do you most often catch? | Can you describe an instance where you improved an existing Ruby or Rails codebase for clarity, maintainability, or testability, and what concrete steps you took?",
    "source_url": "https://editor.superannotate.com/jobs/Ruby-25-982"
  },
  {
    "job_id": "Ruby-25-982",
    "title": "Ruby Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Spain",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Ruby Engineer for AI Data Training, you will review AI-generated Ruby and Rails code or generate your own solutions, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for readability, maintainability, and correctness; identify errors in MVC structure, domain modeling, or control flow; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Ruby patterns; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Ruby, Ruby on Rails, MVC, Backend development, Web applications, RSpec, Metaprogramming, Refactoring, Code review, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Ruby programming experience in production environments. | Strong understanding of Ruby on Rails, MVC architecture, and idiomatic Ruby patterns for structuring application code. | Hands-on software engineering experience building and maintaining Ruby or Rails applications, with exposure to real-world production constraints. | Ability to evaluate readability, maintainability, and logical correctness in Ruby and Rails code, including model, controller, and service-layer design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating and reviewing their output. | Excellent English writing skills, capable of producing clear, structured, and pedagogical technical explanations and code reviews. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, large-scale code review, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: experience with frameworks and tools such as Ruby on Rails, Sinatra, and RSpec in real projects. | Preferred: merged PRs in Ruby open-source projects, experience with metaprogramming or test-heavy systems, and competitive programming exposure.",
    "sample_interview_questions": "How would you describe the Ruby language’s philosophy and core idioms, and how does that influence how you structure code in real projects? | Can you explain the MVC pattern in the context of Ruby on Rails and how responsibilities should be divided between models, views, and controllers? | When reviewing a Rails controller, what are the main indicators that too much business logic has leaked into the controller layer? | How do you decide when to extract code into service objects, concerns, or modules in a Rails application to improve maintainability? | What are some common Ruby idioms you look for when evaluating whether code is idiomatic versus simply functional but harder to read? | How do you approach reasoning about callbacks, validations, and associations in Active Record models to keep them understandable and testable? | When you encounter metaprogramming in a Ruby codebase, what criteria do you use to decide whether it is justified or overcomplicating the design? | How do you evaluate the readability and maintainability of a complex Ruby method that chains multiple enumerable operations or uses blocks heavily? | What strategies do you use to ensure that Rails routing and controller structure remain coherent as an application grows in size and features? | How do you think about error handling and exception usage in Ruby and Rails applications, and what patterns help keep this logic clear? | Can you describe how you would review an RSpec test suite for clarity, coverage, and resilience to refactoring? | When looking at database queries generated by Active Record, what do you look for to identify potential performance or N+1 query problems? | How do you evaluate whether a Rails application’s configuration, environment handling, and secrets management follow best practices for security and maintainability? | What is your approach to reviewing code that heavily uses Ruby blocks, procs, or lambdas, and how do you judge whether the abstraction is appropriate? | How do you ensure that view templates or view components remain simple, testable, and free from excessive business logic? | When reviewing a pull request in a Ruby or Rails project, what is your personal checklist for readability, logic correctness, and alignment with style guidelines? | How do you assess the impact of a proposed refactor on an existing Ruby codebase, especially in systems with a large and critical test suite? | What aspects of a Ruby on Rails application do you pay attention to when evaluating for long-term maintainability, such as naming, directory structure, or dependency management? | How do you critically evaluate suggestions from LLMs or code assistants for Ruby or Rails code, and what types of mistakes do you most often catch? | Can you describe an instance where you improved an existing Ruby or Rails codebase for clarity, maintainability, or testability, and what concrete steps you took?",
    "source_url": "https://editor.superannotate.com/jobs/Ruby-25-982"
  },
  {
    "job_id": "R-25-613",
    "title": "R Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Bangladesh",
    "pay_rate": "$-1 - $22 / hour",
    "pay_min": -1,
    "pay_max": 22,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid R Engineer, you will review AI-generated responses and generate high-quality R and data-analysis-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in statistical methodology or data-wrangling workflows; fact-check analytical results; write expert-level explanations and model solutions that demonstrate correct use of R; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, directly helping to improve the world’s premier AI models.",
    "keywords": "R programming, Statistical modeling, Data analysis, Data visualization, Reproducible research, Data cleaning, Time series analysis, Machine learning in R, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on experience using R for data analysis, statistics, or data science work. | Strong proficiency in R programming, including data wrangling, functional programming patterns, and writing reusable functions or packages. | Solid grounding in applied statistics, including regression, inference, and model validation, with practical experience implementing these methods in R. | Experience building end-to-end analyses in R that include data cleaning, exploratory analysis, modeling, and visualization. | Familiarity with common R ecosystems such as tidyverse, data.table, and ggplot2, and the ability to choose appropriate tools for a given task. | Professional experience in a data-focused role such as data scientist, statistician, quantitative analyst, or similar. | Minimum Bachelor’s degree in Statistics, Mathematics, Computer Science, or a closely related quantitative field. | Significant experience using large language models (LLMs) to assist with coding, analysis design, and code review in R. | Excellent English writing skills with the ability to document analyses and explain complex statistical ideas clearly to non-experts. | Previous experience with AI data training or model evaluation is strongly preferred, and Minimum C1 English proficiency is required.",
    "sample_interview_questions": "How do you typically structure an R analysis project to keep data loading, cleaning, modeling, and reporting clear and organized? | When would you choose base R functions versus tidyverse tools such as dplyr or purrr for data manipulation, and why? | Can you explain the differences between data.frame, tibble, and data.table objects and when you might prefer each in practice? | How do you approach handling missing data in R, and how do you decide between methods such as deletion, imputation, or modeling-based approaches? | What are the advantages of vectorized operations in R compared with explicit loops, and how do you identify code that would benefit from vectorization? | How do you design an analysis pipeline in R to be reproducible across machines and over time, including package versions and random seeds? | Describe your process for debugging a failing R script or notebook, including the tools and functions you rely on most often. | How do you evaluate whether an R script or function is readable and maintainable for other analysts or engineers on your team? | Can you walk through how you would choose and fit an appropriate statistical model in R for a regression problem with both numeric and categorical predictors? | How do you typically validate and compare models in R, for example using cross-validation, train/test splits, or information criteria? | Describe how you use ggplot2 or another visualization library to explore data, and how you decide which visual encodings best communicate specific relationships. | What strategies do you use in R to work efficiently with large datasets that do not fit comfortably into memory? | How would you approach building a small internal R package to share reusable functions, including documentation and testing? | What are common pitfalls you have seen when working with factors or categorical variables in R, and how do you avoid them? | How do you connect R to external data sources such as relational databases or APIs, and what best practices do you follow when doing so? | Describe your approach to writing unit tests and regression tests for R code, and how you decide what needs to be tested. | When reviewing another person’s R code for an analysis, what aspects do you focus on to judge correctness, clarity, and statistical soundness? | Can you give an example of a complex analysis you completed in R, including how you structured the work and communicated the results? | How do you collaborate with others on R-based projects using version control and shared environments or notebooks? | How have you used large language models while working in R, and what checks do you perform to confirm that their suggestions are statistically and technically sound?",
    "source_url": "https://editor.superannotate.com/jobs/R-25-613"
  },
  {
    "job_id": "R-25-613",
    "title": "R Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "India",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid R Engineer, you will review AI-generated responses and generate high-quality R and data-analysis-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in statistical methodology or data-wrangling workflows; fact-check analytical results; write expert-level explanations and model solutions that demonstrate correct use of R; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, directly helping to improve the world’s premier AI models.",
    "keywords": "R programming, Statistical modeling, Data analysis, Data visualization, Reproducible research, Data cleaning, Time series analysis, Machine learning in R, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on experience using R for data analysis, statistics, or data science work. | Strong proficiency in R programming, including data wrangling, functional programming patterns, and writing reusable functions or packages. | Solid grounding in applied statistics, including regression, inference, and model validation, with practical experience implementing these methods in R. | Experience building end-to-end analyses in R that include data cleaning, exploratory analysis, modeling, and visualization. | Familiarity with common R ecosystems such as tidyverse, data.table, and ggplot2, and the ability to choose appropriate tools for a given task. | Professional experience in a data-focused role such as data scientist, statistician, quantitative analyst, or similar. | Minimum Bachelor’s degree in Statistics, Mathematics, Computer Science, or a closely related quantitative field. | Significant experience using large language models (LLMs) to assist with coding, analysis design, and code review in R. | Excellent English writing skills with the ability to document analyses and explain complex statistical ideas clearly to non-experts. | Previous experience with AI data training or model evaluation is strongly preferred, and Minimum C1 English proficiency is required.",
    "sample_interview_questions": "How do you typically structure an R analysis project to keep data loading, cleaning, modeling, and reporting clear and organized? | When would you choose base R functions versus tidyverse tools such as dplyr or purrr for data manipulation, and why? | Can you explain the differences between data.frame, tibble, and data.table objects and when you might prefer each in practice? | How do you approach handling missing data in R, and how do you decide between methods such as deletion, imputation, or modeling-based approaches? | What are the advantages of vectorized operations in R compared with explicit loops, and how do you identify code that would benefit from vectorization? | How do you design an analysis pipeline in R to be reproducible across machines and over time, including package versions and random seeds? | Describe your process for debugging a failing R script or notebook, including the tools and functions you rely on most often. | How do you evaluate whether an R script or function is readable and maintainable for other analysts or engineers on your team? | Can you walk through how you would choose and fit an appropriate statistical model in R for a regression problem with both numeric and categorical predictors? | How do you typically validate and compare models in R, for example using cross-validation, train/test splits, or information criteria? | Describe how you use ggplot2 or another visualization library to explore data, and how you decide which visual encodings best communicate specific relationships. | What strategies do you use in R to work efficiently with large datasets that do not fit comfortably into memory? | How would you approach building a small internal R package to share reusable functions, including documentation and testing? | What are common pitfalls you have seen when working with factors or categorical variables in R, and how do you avoid them? | How do you connect R to external data sources such as relational databases or APIs, and what best practices do you follow when doing so? | Describe your approach to writing unit tests and regression tests for R code, and how you decide what needs to be tested. | When reviewing another person’s R code for an analysis, what aspects do you focus on to judge correctness, clarity, and statistical soundness? | Can you give an example of a complex analysis you completed in R, including how you structured the work and communicated the results? | How do you collaborate with others on R-based projects using version control and shared environments or notebooks? | How have you used large language models while working in R, and what checks do you perform to confirm that their suggestions are statistically and technically sound?",
    "source_url": "https://editor.superannotate.com/jobs/R-25-613"
  },
  {
    "job_id": "R-25-613",
    "title": "R Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Egypt",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid R Engineer, you will review AI-generated responses and generate high-quality R and data-analysis-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in statistical methodology or data-wrangling workflows; fact-check analytical results; write expert-level explanations and model solutions that demonstrate correct use of R; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, directly helping to improve the world’s premier AI models.",
    "keywords": "R programming, Statistical modeling, Data analysis, Data visualization, Reproducible research, Data cleaning, Time series analysis, Machine learning in R, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on experience using R for data analysis, statistics, or data science work. | Strong proficiency in R programming, including data wrangling, functional programming patterns, and writing reusable functions or packages. | Solid grounding in applied statistics, including regression, inference, and model validation, with practical experience implementing these methods in R. | Experience building end-to-end analyses in R that include data cleaning, exploratory analysis, modeling, and visualization. | Familiarity with common R ecosystems such as tidyverse, data.table, and ggplot2, and the ability to choose appropriate tools for a given task. | Professional experience in a data-focused role such as data scientist, statistician, quantitative analyst, or similar. | Minimum Bachelor’s degree in Statistics, Mathematics, Computer Science, or a closely related quantitative field. | Significant experience using large language models (LLMs) to assist with coding, analysis design, and code review in R. | Excellent English writing skills with the ability to document analyses and explain complex statistical ideas clearly to non-experts. | Previous experience with AI data training or model evaluation is strongly preferred, and Minimum C1 English proficiency is required.",
    "sample_interview_questions": "How do you typically structure an R analysis project to keep data loading, cleaning, modeling, and reporting clear and organized? | When would you choose base R functions versus tidyverse tools such as dplyr or purrr for data manipulation, and why? | Can you explain the differences between data.frame, tibble, and data.table objects and when you might prefer each in practice? | How do you approach handling missing data in R, and how do you decide between methods such as deletion, imputation, or modeling-based approaches? | What are the advantages of vectorized operations in R compared with explicit loops, and how do you identify code that would benefit from vectorization? | How do you design an analysis pipeline in R to be reproducible across machines and over time, including package versions and random seeds? | Describe your process for debugging a failing R script or notebook, including the tools and functions you rely on most often. | How do you evaluate whether an R script or function is readable and maintainable for other analysts or engineers on your team? | Can you walk through how you would choose and fit an appropriate statistical model in R for a regression problem with both numeric and categorical predictors? | How do you typically validate and compare models in R, for example using cross-validation, train/test splits, or information criteria? | Describe how you use ggplot2 or another visualization library to explore data, and how you decide which visual encodings best communicate specific relationships. | What strategies do you use in R to work efficiently with large datasets that do not fit comfortably into memory? | How would you approach building a small internal R package to share reusable functions, including documentation and testing? | What are common pitfalls you have seen when working with factors or categorical variables in R, and how do you avoid them? | How do you connect R to external data sources such as relational databases or APIs, and what best practices do you follow when doing so? | Describe your approach to writing unit tests and regression tests for R code, and how you decide what needs to be tested. | When reviewing another person’s R code for an analysis, what aspects do you focus on to judge correctness, clarity, and statistical soundness? | Can you give an example of a complex analysis you completed in R, including how you structured the work and communicated the results? | How do you collaborate with others on R-based projects using version control and shared environments or notebooks? | How have you used large language models while working in R, and what checks do you perform to confirm that their suggestions are statistically and technically sound?",
    "source_url": "https://editor.superannotate.com/jobs/R-25-613"
  },
  {
    "job_id": "R-25-613",
    "title": "R Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Indonesia",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid R Engineer, you will review AI-generated responses and generate high-quality R and data-analysis-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in statistical methodology or data-wrangling workflows; fact-check analytical results; write expert-level explanations and model solutions that demonstrate correct use of R; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, directly helping to improve the world’s premier AI models.",
    "keywords": "R programming, Statistical modeling, Data analysis, Data visualization, Reproducible research, Data cleaning, Time series analysis, Machine learning in R, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on experience using R for data analysis, statistics, or data science work. | Strong proficiency in R programming, including data wrangling, functional programming patterns, and writing reusable functions or packages. | Solid grounding in applied statistics, including regression, inference, and model validation, with practical experience implementing these methods in R. | Experience building end-to-end analyses in R that include data cleaning, exploratory analysis, modeling, and visualization. | Familiarity with common R ecosystems such as tidyverse, data.table, and ggplot2, and the ability to choose appropriate tools for a given task. | Professional experience in a data-focused role such as data scientist, statistician, quantitative analyst, or similar. | Minimum Bachelor’s degree in Statistics, Mathematics, Computer Science, or a closely related quantitative field. | Significant experience using large language models (LLMs) to assist with coding, analysis design, and code review in R. | Excellent English writing skills with the ability to document analyses and explain complex statistical ideas clearly to non-experts. | Previous experience with AI data training or model evaluation is strongly preferred, and Minimum C1 English proficiency is required.",
    "sample_interview_questions": "How do you typically structure an R analysis project to keep data loading, cleaning, modeling, and reporting clear and organized? | When would you choose base R functions versus tidyverse tools such as dplyr or purrr for data manipulation, and why? | Can you explain the differences between data.frame, tibble, and data.table objects and when you might prefer each in practice? | How do you approach handling missing data in R, and how do you decide between methods such as deletion, imputation, or modeling-based approaches? | What are the advantages of vectorized operations in R compared with explicit loops, and how do you identify code that would benefit from vectorization? | How do you design an analysis pipeline in R to be reproducible across machines and over time, including package versions and random seeds? | Describe your process for debugging a failing R script or notebook, including the tools and functions you rely on most often. | How do you evaluate whether an R script or function is readable and maintainable for other analysts or engineers on your team? | Can you walk through how you would choose and fit an appropriate statistical model in R for a regression problem with both numeric and categorical predictors? | How do you typically validate and compare models in R, for example using cross-validation, train/test splits, or information criteria? | Describe how you use ggplot2 or another visualization library to explore data, and how you decide which visual encodings best communicate specific relationships. | What strategies do you use in R to work efficiently with large datasets that do not fit comfortably into memory? | How would you approach building a small internal R package to share reusable functions, including documentation and testing? | What are common pitfalls you have seen when working with factors or categorical variables in R, and how do you avoid them? | How do you connect R to external data sources such as relational databases or APIs, and what best practices do you follow when doing so? | Describe your approach to writing unit tests and regression tests for R code, and how you decide what needs to be tested. | When reviewing another person’s R code for an analysis, what aspects do you focus on to judge correctness, clarity, and statistical soundness? | Can you give an example of a complex analysis you completed in R, including how you structured the work and communicated the results? | How do you collaborate with others on R-based projects using version control and shared environments or notebooks? | How have you used large language models while working in R, and what checks do you perform to confirm that their suggestions are statistically and technically sound?",
    "source_url": "https://editor.superannotate.com/jobs/R-25-613"
  },
  {
    "job_id": "R-25-613",
    "title": "R Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Brazil",
    "pay_rate": "$-1 - $45 / hour",
    "pay_min": -1,
    "pay_max": 45,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid R Engineer, you will review AI-generated responses and generate high-quality R and data-analysis-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in statistical methodology or data-wrangling workflows; fact-check analytical results; write expert-level explanations and model solutions that demonstrate correct use of R; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, directly helping to improve the world’s premier AI models.",
    "keywords": "R programming, Statistical modeling, Data analysis, Data visualization, Reproducible research, Data cleaning, Time series analysis, Machine learning in R, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on experience using R for data analysis, statistics, or data science work. | Strong proficiency in R programming, including data wrangling, functional programming patterns, and writing reusable functions or packages. | Solid grounding in applied statistics, including regression, inference, and model validation, with practical experience implementing these methods in R. | Experience building end-to-end analyses in R that include data cleaning, exploratory analysis, modeling, and visualization. | Familiarity with common R ecosystems such as tidyverse, data.table, and ggplot2, and the ability to choose appropriate tools for a given task. | Professional experience in a data-focused role such as data scientist, statistician, quantitative analyst, or similar. | Minimum Bachelor’s degree in Statistics, Mathematics, Computer Science, or a closely related quantitative field. | Significant experience using large language models (LLMs) to assist with coding, analysis design, and code review in R. | Excellent English writing skills with the ability to document analyses and explain complex statistical ideas clearly to non-experts. | Previous experience with AI data training or model evaluation is strongly preferred, and Minimum C1 English proficiency is required.",
    "sample_interview_questions": "How do you typically structure an R analysis project to keep data loading, cleaning, modeling, and reporting clear and organized? | When would you choose base R functions versus tidyverse tools such as dplyr or purrr for data manipulation, and why? | Can you explain the differences between data.frame, tibble, and data.table objects and when you might prefer each in practice? | How do you approach handling missing data in R, and how do you decide between methods such as deletion, imputation, or modeling-based approaches? | What are the advantages of vectorized operations in R compared with explicit loops, and how do you identify code that would benefit from vectorization? | How do you design an analysis pipeline in R to be reproducible across machines and over time, including package versions and random seeds? | Describe your process for debugging a failing R script or notebook, including the tools and functions you rely on most often. | How do you evaluate whether an R script or function is readable and maintainable for other analysts or engineers on your team? | Can you walk through how you would choose and fit an appropriate statistical model in R for a regression problem with both numeric and categorical predictors? | How do you typically validate and compare models in R, for example using cross-validation, train/test splits, or information criteria? | Describe how you use ggplot2 or another visualization library to explore data, and how you decide which visual encodings best communicate specific relationships. | What strategies do you use in R to work efficiently with large datasets that do not fit comfortably into memory? | How would you approach building a small internal R package to share reusable functions, including documentation and testing? | What are common pitfalls you have seen when working with factors or categorical variables in R, and how do you avoid them? | How do you connect R to external data sources such as relational databases or APIs, and what best practices do you follow when doing so? | Describe your approach to writing unit tests and regression tests for R code, and how you decide what needs to be tested. | When reviewing another person’s R code for an analysis, what aspects do you focus on to judge correctness, clarity, and statistical soundness? | Can you give an example of a complex analysis you completed in R, including how you structured the work and communicated the results? | How do you collaborate with others on R-based projects using version control and shared environments or notebooks? | How have you used large language models while working in R, and what checks do you perform to confirm that their suggestions are statistically and technically sound?",
    "source_url": "https://editor.superannotate.com/jobs/R-25-613"
  },
  {
    "job_id": "R-25-613",
    "title": "R Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Portugal",
    "pay_rate": "$-1 - $50 / hour",
    "pay_min": -1,
    "pay_max": 50,
    "currency": "USD",
    "num_people": 54,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid R Engineer, you will review AI-generated responses and generate high-quality R and data-analysis-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in statistical methodology or data-wrangling workflows; fact-check analytical results; write expert-level explanations and model solutions that demonstrate correct use of R; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, directly helping to improve the world’s premier AI models.",
    "keywords": "R programming, Statistical modeling, Data analysis, Data visualization, Reproducible research, Data cleaning, Time series analysis, Machine learning in R, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on experience using R for data analysis, statistics, or data science work. | Strong proficiency in R programming, including data wrangling, functional programming patterns, and writing reusable functions or packages. | Solid grounding in applied statistics, including regression, inference, and model validation, with practical experience implementing these methods in R. | Experience building end-to-end analyses in R that include data cleaning, exploratory analysis, modeling, and visualization. | Familiarity with common R ecosystems such as tidyverse, data.table, and ggplot2, and the ability to choose appropriate tools for a given task. | Professional experience in a data-focused role such as data scientist, statistician, quantitative analyst, or similar. | Minimum Bachelor’s degree in Statistics, Mathematics, Computer Science, or a closely related quantitative field. | Significant experience using large language models (LLMs) to assist with coding, analysis design, and code review in R. | Excellent English writing skills with the ability to document analyses and explain complex statistical ideas clearly to non-experts. | Previous experience with AI data training or model evaluation is strongly preferred, and Minimum C1 English proficiency is required.",
    "sample_interview_questions": "How do you typically structure an R analysis project to keep data loading, cleaning, modeling, and reporting clear and organized? | When would you choose base R functions versus tidyverse tools such as dplyr or purrr for data manipulation, and why? | Can you explain the differences between data.frame, tibble, and data.table objects and when you might prefer each in practice? | How do you approach handling missing data in R, and how do you decide between methods such as deletion, imputation, or modeling-based approaches? | What are the advantages of vectorized operations in R compared with explicit loops, and how do you identify code that would benefit from vectorization? | How do you design an analysis pipeline in R to be reproducible across machines and over time, including package versions and random seeds? | Describe your process for debugging a failing R script or notebook, including the tools and functions you rely on most often. | How do you evaluate whether an R script or function is readable and maintainable for other analysts or engineers on your team? | Can you walk through how you would choose and fit an appropriate statistical model in R for a regression problem with both numeric and categorical predictors? | How do you typically validate and compare models in R, for example using cross-validation, train/test splits, or information criteria? | Describe how you use ggplot2 or another visualization library to explore data, and how you decide which visual encodings best communicate specific relationships. | What strategies do you use in R to work efficiently with large datasets that do not fit comfortably into memory? | How would you approach building a small internal R package to share reusable functions, including documentation and testing? | What are common pitfalls you have seen when working with factors or categorical variables in R, and how do you avoid them? | How do you connect R to external data sources such as relational databases or APIs, and what best practices do you follow when doing so? | Describe your approach to writing unit tests and regression tests for R code, and how you decide what needs to be tested. | When reviewing another person’s R code for an analysis, what aspects do you focus on to judge correctness, clarity, and statistical soundness? | Can you give an example of a complex analysis you completed in R, including how you structured the work and communicated the results? | How do you collaborate with others on R-based projects using version control and shared environments or notebooks? | How have you used large language models while working in R, and what checks do you perform to confirm that their suggestions are statistically and technically sound?",
    "source_url": "https://editor.superannotate.com/jobs/R-25-613"
  },
  {
    "job_id": "R-25-613",
    "title": "R Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Spain",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "As a remote, hourly paid R Engineer, you will review AI-generated responses and generate high-quality R and data-analysis-focused content, evaluating the reasoning quality and step-by-step problem-solving behind each solution. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in statistical methodology or data-wrangling workflows; fact-check analytical results; write expert-level explanations and model solutions that demonstrate correct use of R; and rate and compare multiple AI responses based on correctness and reasoning quality. This contract role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, directly helping to improve the world’s premier AI models.",
    "keywords": "R programming, Statistical modeling, Data analysis, Data visualization, Reproducible research, Data cleaning, Time series analysis, Machine learning in R, LLM coding, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "2+ years of hands-on experience using R for data analysis, statistics, or data science work. | Strong proficiency in R programming, including data wrangling, functional programming patterns, and writing reusable functions or packages. | Solid grounding in applied statistics, including regression, inference, and model validation, with practical experience implementing these methods in R. | Experience building end-to-end analyses in R that include data cleaning, exploratory analysis, modeling, and visualization. | Familiarity with common R ecosystems such as tidyverse, data.table, and ggplot2, and the ability to choose appropriate tools for a given task. | Professional experience in a data-focused role such as data scientist, statistician, quantitative analyst, or similar. | Minimum Bachelor’s degree in Statistics, Mathematics, Computer Science, or a closely related quantitative field. | Significant experience using large language models (LLMs) to assist with coding, analysis design, and code review in R. | Excellent English writing skills with the ability to document analyses and explain complex statistical ideas clearly to non-experts. | Previous experience with AI data training or model evaluation is strongly preferred, and Minimum C1 English proficiency is required.",
    "sample_interview_questions": "How do you typically structure an R analysis project to keep data loading, cleaning, modeling, and reporting clear and organized? | When would you choose base R functions versus tidyverse tools such as dplyr or purrr for data manipulation, and why? | Can you explain the differences between data.frame, tibble, and data.table objects and when you might prefer each in practice? | How do you approach handling missing data in R, and how do you decide between methods such as deletion, imputation, or modeling-based approaches? | What are the advantages of vectorized operations in R compared with explicit loops, and how do you identify code that would benefit from vectorization? | How do you design an analysis pipeline in R to be reproducible across machines and over time, including package versions and random seeds? | Describe your process for debugging a failing R script or notebook, including the tools and functions you rely on most often. | How do you evaluate whether an R script or function is readable and maintainable for other analysts or engineers on your team? | Can you walk through how you would choose and fit an appropriate statistical model in R for a regression problem with both numeric and categorical predictors? | How do you typically validate and compare models in R, for example using cross-validation, train/test splits, or information criteria? | Describe how you use ggplot2 or another visualization library to explore data, and how you decide which visual encodings best communicate specific relationships. | What strategies do you use in R to work efficiently with large datasets that do not fit comfortably into memory? | How would you approach building a small internal R package to share reusable functions, including documentation and testing? | What are common pitfalls you have seen when working with factors or categorical variables in R, and how do you avoid them? | How do you connect R to external data sources such as relational databases or APIs, and what best practices do you follow when doing so? | Describe your approach to writing unit tests and regression tests for R code, and how you decide what needs to be tested. | When reviewing another person’s R code for an analysis, what aspects do you focus on to judge correctness, clarity, and statistical soundness? | Can you give an example of a complex analysis you completed in R, including how you structured the work and communicated the results? | How do you collaborate with others on R-based projects using version control and shared environments or notebooks? | How have you used large language models while working in R, and what checks do you perform to confirm that their suggestions are statistically and technically sound?",
    "source_url": "https://editor.superannotate.com/jobs/R-25-613"
  },
  {
    "job_id": "Go-25-319",
    "title": "Go Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "India",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 100,
    "priority_level": 3,
    "role_description": "As an hourly paid, fully remote Go Engineer for AI Data Training, you will review AI-generated Go code and explanations or generate your own, evaluate the reasoning quality and step-by-step problem-solving, and provide expert feedback that helps models produce answers that are accurate, logical, and clearly explained. You will assess solutions for correctness, readability, and adherence to backend-style best practices; identify errors in concurrency, error handling, or conceptual understanding; fact-check information; write high-quality explanations and model solutions that demonstrate idiomatic Go; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation model labs, and your work will directly help improve the world’s premier AI models while giving you the flexibility of impactful, detail-oriented remote contract work.",
    "keywords": "Go, Golang, Concurrency, Backend development, Microservices, gRPC, Cloud-native, Error handling, Profiling, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "1–2+ years of professional Go programming experience, ideally focused on backend services, APIs, or systems work. | Strong understanding of Go idioms, including clear error handling, composition over inheritance, and simple, readable interfaces. | Hands-on software engineering experience building and maintaining backend-style Go applications in production environments. | Ability to evaluate correctness and readability in backend-oriented Go code, including concurrency patterns, error propagation, and API design. | Significant experience using LLMs or AI coding assistants while programming, combined with a disciplined approach to validating their output. | Excellent English writing skills, capable of producing precise, structured, and pedagogical technical explanations. | Minimum Bachelor’s degree in Computer Science or a closely related technical field. | Previous experience with AI data training, code review at scale, or evaluation of AI-generated technical content is strongly preferred; Minimum C1 English proficiency and an extremely detail-oriented working style are required. | Highly preferred: hands-on experience with Go frameworks and libraries such as Gin, gRPC, or Echo in real projects. | Preferred: merged PRs in Go open-source projects, competitive programming experience, cloud-native development exposure, and familiarity with Go performance profiling tools and techniques.",
    "sample_interview_questions": "How would you describe Go’s philosophy and core idioms to a developer coming from an object-oriented language like Java or C#? | Can you explain how goroutines and channels work together to implement concurrency in Go, and when you would favor one pattern over another? | When reviewing Go code that uses shared state, what signs tell you that data races or concurrency issues may be present? | How do you typically structure error handling in Go, and what patterns do you consider idiomatic versus overly complex? | What factors do you evaluate to judge whether a Go function or method is sufficiently readable and maintainable for a production codebase? | Can you walk through how the Go scheduler works at a high level and how that influences your design of concurrent systems? | How do you approach designing and reviewing HTTP or gRPC handlers in Go for clarity, separation of concerns, and testability? | What strategies do you use to organize Go packages and modules so that dependencies remain clear and the architecture stays modular? | How do you evaluate the trade-offs between using interfaces and concrete types in Go APIs, and what are common mistakes you watch for? | When reading Go code that interacts with context.Context, what patterns indicate that timeouts, cancellations, and deadlines are being handled correctly? | How do you reason about performance in Go, and what kinds of issues do you look for before resorting to profiling tools? | What is your process for using Go’s profiling tools (pprof, trace) to diagnose bottlenecks or memory leaks in a service? | How do you review Go code that uses third-party libraries or frameworks like Gin or Echo to ensure they are integrated safely and consistently? | When evaluating persistence-related code in Go (for example, database access), what patterns help ensure correctness, transaction safety, and clear error propagation? | How do you decide whether to use channels versus other synchronization primitives like mutexes or wait groups in a given design? | What are some common anti-patterns you have seen in Go projects, and how do you recommend refactoring them toward more idiomatic solutions? | How do you ensure that logging, metrics, and observability concerns are handled cleanly within Go services without polluting business logic? | When reviewing a Go-based microservice, what aspects of configuration, deployment, and cloud-native integration do you pay attention to? | How do you critically evaluate suggestions from LLMs or code assistants for Go, and what types of errors or non-idiomatic patterns do you most frequently catch? | Can you describe a concrete example where you improved an existing Go codebase for correctness, concurrency behavior, or readability, and what steps you took to get there?",
    "source_url": "https://editor.superannotate.com/jobs/Go-25-319"
  },
  {
    "job_id": "C-25-417",
    "title": "Senior C Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Bangladesh",
    "pay_rate": "$-1 - $22 / hour",
    "pay_min": -1,
    "pay_max": 22,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 3,
    "role_description": "As a Senior C Engineer, you will work remotely on an hourly paid basis to review AI-generated C code, low-level systems designs, and technical explanations, as well as generate high-quality reference implementations and step-by-step reasoning for complex engineering problems. You will assess solutions for accuracy, clarity, safety, and adherence to the prompt; identify errors in logic, memory handling, concurrency, or performance; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality.\n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs. Your systems-level C expertise will directly help improve the world’s premier AI models used by millions of developers worldwide.",
    "keywords": "C programming, C, Systems programming, Embedded development, Memory management, Concurrency, Debugging, Performance optimization, Code review, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Electrical/Computer Engineering, or a closely related technical field. | 4+ years of professional experience developing production software in C for systems, embedded, or performance-critical applications. | Expert-level proficiency in C, including deep understanding of pointers, memory management, undefined behavior, and low-level debugging. | Strong background in systems programming concepts such as concurrency, operating systems, hardware interaction, and performance optimization. | Hands-on experience with build systems, compilers, linkers, and debugging tools commonly used in C development. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Proven experience conducting detailed code reviews and enforcing coding standards for safety and maintainability in C codebases. | Comfort working with version control, CI/CD workflows, and automated testing for C projects. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in low-level code.",
    "sample_interview_questions": "How do you structure and organize a large C codebase so that it remains maintainable and testable over time? | Can you explain how memory allocation works in C, including the differences between stack and heap allocation and common pitfalls? | What techniques do you use to prevent and detect buffer overflows and other memory safety issues in C programs? | How does undefined behavior in C affect the way you write, review, and reason about low-level code? | Describe how you would design and document a clear interface in C using header files and opaque data types. | What is your approach to debugging hard-to-reproduce bugs in a C application running on a target system you cannot easily instrument? | How do you use tools such as static analyzers, sanitizers, or valgrind to improve code quality in C projects? | Explain how you would design a thread-safe C module and what synchronization primitives or patterns you might use. | How do you reason about and measure the performance of a C function that is on the critical path of a system? | What considerations are important when working directly with hardware registers, memory-mapped I/O, or other low-level constructs in C? | How do you manage error handling in C in a way that keeps control flow clear and avoids resource leaks? | Describe how you would design and implement a logging or tracing approach for a C system with tight resource constraints. | What are common portability issues you watch for when writing C code intended to run on multiple platforms or compilers? | How do you approach code review for C projects, and what specific issues do you pay the most attention to? | What strategies do you use to organize unit tests and integration tests for C code, especially when hardware dependencies are involved? | How do you design data structures in C to balance memory footprint, cache behavior, and ease of use? | Explain how the C build toolchain (compiler options, linker scripts, build systems) influences your development and debugging workflow. | How do you communicate complex low-level design decisions to less experienced engineers so that they can follow your reasoning? | When comparing two alternative C implementations of the same subsystem, what concrete criteria do you use to decide which is better? | Tell me about a complex C-based system you have worked on, and how you ensured the overall design and implementation were robust and well-reasoned.",
    "source_url": "https://editor.superannotate.com/jobs/C-25-417"
  },
  {
    "job_id": "C-25-417",
    "title": "Senior C Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Egypt",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 3,
    "role_description": "As a Senior C Engineer, you will work remotely on an hourly paid basis to review AI-generated C code, low-level systems designs, and technical explanations, as well as generate high-quality reference implementations and step-by-step reasoning for complex engineering problems. You will assess solutions for accuracy, clarity, safety, and adherence to the prompt; identify errors in logic, memory handling, concurrency, or performance; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality.\n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs. Your systems-level C expertise will directly help improve the world’s premier AI models used by millions of developers worldwide.",
    "keywords": "C programming, C, Systems programming, Embedded development, Memory management, Concurrency, Debugging, Performance optimization, Code review, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Electrical/Computer Engineering, or a closely related technical field. | 4+ years of professional experience developing production software in C for systems, embedded, or performance-critical applications. | Expert-level proficiency in C, including deep understanding of pointers, memory management, undefined behavior, and low-level debugging. | Strong background in systems programming concepts such as concurrency, operating systems, hardware interaction, and performance optimization. | Hands-on experience with build systems, compilers, linkers, and debugging tools commonly used in C development. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Proven experience conducting detailed code reviews and enforcing coding standards for safety and maintainability in C codebases. | Comfort working with version control, CI/CD workflows, and automated testing for C projects. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in low-level code.",
    "sample_interview_questions": "How do you structure and organize a large C codebase so that it remains maintainable and testable over time? | Can you explain how memory allocation works in C, including the differences between stack and heap allocation and common pitfalls? | What techniques do you use to prevent and detect buffer overflows and other memory safety issues in C programs? | How does undefined behavior in C affect the way you write, review, and reason about low-level code? | Describe how you would design and document a clear interface in C using header files and opaque data types. | What is your approach to debugging hard-to-reproduce bugs in a C application running on a target system you cannot easily instrument? | How do you use tools such as static analyzers, sanitizers, or valgrind to improve code quality in C projects? | Explain how you would design a thread-safe C module and what synchronization primitives or patterns you might use. | How do you reason about and measure the performance of a C function that is on the critical path of a system? | What considerations are important when working directly with hardware registers, memory-mapped I/O, or other low-level constructs in C? | How do you manage error handling in C in a way that keeps control flow clear and avoids resource leaks? | Describe how you would design and implement a logging or tracing approach for a C system with tight resource constraints. | What are common portability issues you watch for when writing C code intended to run on multiple platforms or compilers? | How do you approach code review for C projects, and what specific issues do you pay the most attention to? | What strategies do you use to organize unit tests and integration tests for C code, especially when hardware dependencies are involved? | How do you design data structures in C to balance memory footprint, cache behavior, and ease of use? | Explain how the C build toolchain (compiler options, linker scripts, build systems) influences your development and debugging workflow. | How do you communicate complex low-level design decisions to less experienced engineers so that they can follow your reasoning? | When comparing two alternative C implementations of the same subsystem, what concrete criteria do you use to decide which is better? | Tell me about a complex C-based system you have worked on, and how you ensured the overall design and implementation were robust and well-reasoned.",
    "source_url": "https://editor.superannotate.com/jobs/C-25-417"
  },
  {
    "job_id": "C-25-417",
    "title": "Senior C Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "India",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 3,
    "role_description": "As a Senior C Engineer, you will work remotely on an hourly paid basis to review AI-generated C code, low-level systems designs, and technical explanations, as well as generate high-quality reference implementations and step-by-step reasoning for complex engineering problems. You will assess solutions for accuracy, clarity, safety, and adherence to the prompt; identify errors in logic, memory handling, concurrency, or performance; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality.\n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs. Your systems-level C expertise will directly help improve the world’s premier AI models used by millions of developers worldwide.",
    "keywords": "C programming, C, Systems programming, Embedded development, Memory management, Concurrency, Debugging, Performance optimization, Code review, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Electrical/Computer Engineering, or a closely related technical field. | 4+ years of professional experience developing production software in C for systems, embedded, or performance-critical applications. | Expert-level proficiency in C, including deep understanding of pointers, memory management, undefined behavior, and low-level debugging. | Strong background in systems programming concepts such as concurrency, operating systems, hardware interaction, and performance optimization. | Hands-on experience with build systems, compilers, linkers, and debugging tools commonly used in C development. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Proven experience conducting detailed code reviews and enforcing coding standards for safety and maintainability in C codebases. | Comfort working with version control, CI/CD workflows, and automated testing for C projects. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in low-level code.",
    "sample_interview_questions": "How do you structure and organize a large C codebase so that it remains maintainable and testable over time? | Can you explain how memory allocation works in C, including the differences between stack and heap allocation and common pitfalls? | What techniques do you use to prevent and detect buffer overflows and other memory safety issues in C programs? | How does undefined behavior in C affect the way you write, review, and reason about low-level code? | Describe how you would design and document a clear interface in C using header files and opaque data types. | What is your approach to debugging hard-to-reproduce bugs in a C application running on a target system you cannot easily instrument? | How do you use tools such as static analyzers, sanitizers, or valgrind to improve code quality in C projects? | Explain how you would design a thread-safe C module and what synchronization primitives or patterns you might use. | How do you reason about and measure the performance of a C function that is on the critical path of a system? | What considerations are important when working directly with hardware registers, memory-mapped I/O, or other low-level constructs in C? | How do you manage error handling in C in a way that keeps control flow clear and avoids resource leaks? | Describe how you would design and implement a logging or tracing approach for a C system with tight resource constraints. | What are common portability issues you watch for when writing C code intended to run on multiple platforms or compilers? | How do you approach code review for C projects, and what specific issues do you pay the most attention to? | What strategies do you use to organize unit tests and integration tests for C code, especially when hardware dependencies are involved? | How do you design data structures in C to balance memory footprint, cache behavior, and ease of use? | Explain how the C build toolchain (compiler options, linker scripts, build systems) influences your development and debugging workflow. | How do you communicate complex low-level design decisions to less experienced engineers so that they can follow your reasoning? | When comparing two alternative C implementations of the same subsystem, what concrete criteria do you use to decide which is better? | Tell me about a complex C-based system you have worked on, and how you ensured the overall design and implementation were robust and well-reasoned.",
    "source_url": "https://editor.superannotate.com/jobs/C-25-417"
  },
  {
    "job_id": "C-25-417",
    "title": "Senior C Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Indonesia",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 3,
    "role_description": "As a Senior C Engineer, you will work remotely on an hourly paid basis to review AI-generated C code, low-level systems designs, and technical explanations, as well as generate high-quality reference implementations and step-by-step reasoning for complex engineering problems. You will assess solutions for accuracy, clarity, safety, and adherence to the prompt; identify errors in logic, memory handling, concurrency, or performance; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality.\n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs. Your systems-level C expertise will directly help improve the world’s premier AI models used by millions of developers worldwide.",
    "keywords": "C programming, C, Systems programming, Embedded development, Memory management, Concurrency, Debugging, Performance optimization, Code review, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Electrical/Computer Engineering, or a closely related technical field. | 4+ years of professional experience developing production software in C for systems, embedded, or performance-critical applications. | Expert-level proficiency in C, including deep understanding of pointers, memory management, undefined behavior, and low-level debugging. | Strong background in systems programming concepts such as concurrency, operating systems, hardware interaction, and performance optimization. | Hands-on experience with build systems, compilers, linkers, and debugging tools commonly used in C development. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Proven experience conducting detailed code reviews and enforcing coding standards for safety and maintainability in C codebases. | Comfort working with version control, CI/CD workflows, and automated testing for C projects. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in low-level code.",
    "sample_interview_questions": "How do you structure and organize a large C codebase so that it remains maintainable and testable over time? | Can you explain how memory allocation works in C, including the differences between stack and heap allocation and common pitfalls? | What techniques do you use to prevent and detect buffer overflows and other memory safety issues in C programs? | How does undefined behavior in C affect the way you write, review, and reason about low-level code? | Describe how you would design and document a clear interface in C using header files and opaque data types. | What is your approach to debugging hard-to-reproduce bugs in a C application running on a target system you cannot easily instrument? | How do you use tools such as static analyzers, sanitizers, or valgrind to improve code quality in C projects? | Explain how you would design a thread-safe C module and what synchronization primitives or patterns you might use. | How do you reason about and measure the performance of a C function that is on the critical path of a system? | What considerations are important when working directly with hardware registers, memory-mapped I/O, or other low-level constructs in C? | How do you manage error handling in C in a way that keeps control flow clear and avoids resource leaks? | Describe how you would design and implement a logging or tracing approach for a C system with tight resource constraints. | What are common portability issues you watch for when writing C code intended to run on multiple platforms or compilers? | How do you approach code review for C projects, and what specific issues do you pay the most attention to? | What strategies do you use to organize unit tests and integration tests for C code, especially when hardware dependencies are involved? | How do you design data structures in C to balance memory footprint, cache behavior, and ease of use? | Explain how the C build toolchain (compiler options, linker scripts, build systems) influences your development and debugging workflow. | How do you communicate complex low-level design decisions to less experienced engineers so that they can follow your reasoning? | When comparing two alternative C implementations of the same subsystem, what concrete criteria do you use to decide which is better? | Tell me about a complex C-based system you have worked on, and how you ensured the overall design and implementation were robust and well-reasoned.",
    "source_url": "https://editor.superannotate.com/jobs/C-25-417"
  },
  {
    "job_id": "C-25-417",
    "title": "Senior C Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Brazil",
    "pay_rate": "$-1 - $45 / hour",
    "pay_min": -1,
    "pay_max": 45,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 3,
    "role_description": "As a Senior C Engineer, you will work remotely on an hourly paid basis to review AI-generated C code, low-level systems designs, and technical explanations, as well as generate high-quality reference implementations and step-by-step reasoning for complex engineering problems. You will assess solutions for accuracy, clarity, safety, and adherence to the prompt; identify errors in logic, memory handling, concurrency, or performance; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality.\n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs. Your systems-level C expertise will directly help improve the world’s premier AI models used by millions of developers worldwide.",
    "keywords": "C programming, C, Systems programming, Embedded development, Memory management, Concurrency, Debugging, Performance optimization, Code review, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Electrical/Computer Engineering, or a closely related technical field. | 4+ years of professional experience developing production software in C for systems, embedded, or performance-critical applications. | Expert-level proficiency in C, including deep understanding of pointers, memory management, undefined behavior, and low-level debugging. | Strong background in systems programming concepts such as concurrency, operating systems, hardware interaction, and performance optimization. | Hands-on experience with build systems, compilers, linkers, and debugging tools commonly used in C development. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Proven experience conducting detailed code reviews and enforcing coding standards for safety and maintainability in C codebases. | Comfort working with version control, CI/CD workflows, and automated testing for C projects. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in low-level code.",
    "sample_interview_questions": "How do you structure and organize a large C codebase so that it remains maintainable and testable over time? | Can you explain how memory allocation works in C, including the differences between stack and heap allocation and common pitfalls? | What techniques do you use to prevent and detect buffer overflows and other memory safety issues in C programs? | How does undefined behavior in C affect the way you write, review, and reason about low-level code? | Describe how you would design and document a clear interface in C using header files and opaque data types. | What is your approach to debugging hard-to-reproduce bugs in a C application running on a target system you cannot easily instrument? | How do you use tools such as static analyzers, sanitizers, or valgrind to improve code quality in C projects? | Explain how you would design a thread-safe C module and what synchronization primitives or patterns you might use. | How do you reason about and measure the performance of a C function that is on the critical path of a system? | What considerations are important when working directly with hardware registers, memory-mapped I/O, or other low-level constructs in C? | How do you manage error handling in C in a way that keeps control flow clear and avoids resource leaks? | Describe how you would design and implement a logging or tracing approach for a C system with tight resource constraints. | What are common portability issues you watch for when writing C code intended to run on multiple platforms or compilers? | How do you approach code review for C projects, and what specific issues do you pay the most attention to? | What strategies do you use to organize unit tests and integration tests for C code, especially when hardware dependencies are involved? | How do you design data structures in C to balance memory footprint, cache behavior, and ease of use? | Explain how the C build toolchain (compiler options, linker scripts, build systems) influences your development and debugging workflow. | How do you communicate complex low-level design decisions to less experienced engineers so that they can follow your reasoning? | When comparing two alternative C implementations of the same subsystem, what concrete criteria do you use to decide which is better? | Tell me about a complex C-based system you have worked on, and how you ensured the overall design and implementation were robust and well-reasoned.",
    "source_url": "https://editor.superannotate.com/jobs/C-25-417"
  },
  {
    "job_id": "C-25-417",
    "title": "Senior C Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Portugal",
    "pay_rate": "$-1 - $50 / hour",
    "pay_min": -1,
    "pay_max": 50,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 3,
    "role_description": "As a Senior C Engineer, you will work remotely on an hourly paid basis to review AI-generated C code, low-level systems designs, and technical explanations, as well as generate high-quality reference implementations and step-by-step reasoning for complex engineering problems. You will assess solutions for accuracy, clarity, safety, and adherence to the prompt; identify errors in logic, memory handling, concurrency, or performance; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality.\n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs. Your systems-level C expertise will directly help improve the world’s premier AI models used by millions of developers worldwide.",
    "keywords": "C programming, C, Systems programming, Embedded development, Memory management, Concurrency, Debugging, Performance optimization, Code review, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Electrical/Computer Engineering, or a closely related technical field. | 4+ years of professional experience developing production software in C for systems, embedded, or performance-critical applications. | Expert-level proficiency in C, including deep understanding of pointers, memory management, undefined behavior, and low-level debugging. | Strong background in systems programming concepts such as concurrency, operating systems, hardware interaction, and performance optimization. | Hands-on experience with build systems, compilers, linkers, and debugging tools commonly used in C development. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Proven experience conducting detailed code reviews and enforcing coding standards for safety and maintainability in C codebases. | Comfort working with version control, CI/CD workflows, and automated testing for C projects. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in low-level code.",
    "sample_interview_questions": "How do you structure and organize a large C codebase so that it remains maintainable and testable over time? | Can you explain how memory allocation works in C, including the differences between stack and heap allocation and common pitfalls? | What techniques do you use to prevent and detect buffer overflows and other memory safety issues in C programs? | How does undefined behavior in C affect the way you write, review, and reason about low-level code? | Describe how you would design and document a clear interface in C using header files and opaque data types. | What is your approach to debugging hard-to-reproduce bugs in a C application running on a target system you cannot easily instrument? | How do you use tools such as static analyzers, sanitizers, or valgrind to improve code quality in C projects? | Explain how you would design a thread-safe C module and what synchronization primitives or patterns you might use. | How do you reason about and measure the performance of a C function that is on the critical path of a system? | What considerations are important when working directly with hardware registers, memory-mapped I/O, or other low-level constructs in C? | How do you manage error handling in C in a way that keeps control flow clear and avoids resource leaks? | Describe how you would design and implement a logging or tracing approach for a C system with tight resource constraints. | What are common portability issues you watch for when writing C code intended to run on multiple platforms or compilers? | How do you approach code review for C projects, and what specific issues do you pay the most attention to? | What strategies do you use to organize unit tests and integration tests for C code, especially when hardware dependencies are involved? | How do you design data structures in C to balance memory footprint, cache behavior, and ease of use? | Explain how the C build toolchain (compiler options, linker scripts, build systems) influences your development and debugging workflow. | How do you communicate complex low-level design decisions to less experienced engineers so that they can follow your reasoning? | When comparing two alternative C implementations of the same subsystem, what concrete criteria do you use to decide which is better? | Tell me about a complex C-based system you have worked on, and how you ensured the overall design and implementation were robust and well-reasoned.",
    "source_url": "https://editor.superannotate.com/jobs/C-25-417"
  },
  {
    "job_id": "C-25-417",
    "title": "Senior C Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Spain",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 3,
    "role_description": "As a Senior C Engineer, you will work remotely on an hourly paid basis to review AI-generated C code, low-level systems designs, and technical explanations, as well as generate high-quality reference implementations and step-by-step reasoning for complex engineering problems. You will assess solutions for accuracy, clarity, safety, and adherence to the prompt; identify errors in logic, memory handling, concurrency, or performance; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality.\n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs. Your systems-level C expertise will directly help improve the world’s premier AI models used by millions of developers worldwide.",
    "keywords": "C programming, C, Systems programming, Embedded development, Memory management, Concurrency, Debugging, Performance optimization, Code review, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Electrical/Computer Engineering, or a closely related technical field. | 4+ years of professional experience developing production software in C for systems, embedded, or performance-critical applications. | Expert-level proficiency in C, including deep understanding of pointers, memory management, undefined behavior, and low-level debugging. | Strong background in systems programming concepts such as concurrency, operating systems, hardware interaction, and performance optimization. | Hands-on experience with build systems, compilers, linkers, and debugging tools commonly used in C development. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Proven experience conducting detailed code reviews and enforcing coding standards for safety and maintainability in C codebases. | Comfort working with version control, CI/CD workflows, and automated testing for C projects. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in low-level code.",
    "sample_interview_questions": "How do you structure and organize a large C codebase so that it remains maintainable and testable over time? | Can you explain how memory allocation works in C, including the differences between stack and heap allocation and common pitfalls? | What techniques do you use to prevent and detect buffer overflows and other memory safety issues in C programs? | How does undefined behavior in C affect the way you write, review, and reason about low-level code? | Describe how you would design and document a clear interface in C using header files and opaque data types. | What is your approach to debugging hard-to-reproduce bugs in a C application running on a target system you cannot easily instrument? | How do you use tools such as static analyzers, sanitizers, or valgrind to improve code quality in C projects? | Explain how you would design a thread-safe C module and what synchronization primitives or patterns you might use. | How do you reason about and measure the performance of a C function that is on the critical path of a system? | What considerations are important when working directly with hardware registers, memory-mapped I/O, or other low-level constructs in C? | How do you manage error handling in C in a way that keeps control flow clear and avoids resource leaks? | Describe how you would design and implement a logging or tracing approach for a C system with tight resource constraints. | What are common portability issues you watch for when writing C code intended to run on multiple platforms or compilers? | How do you approach code review for C projects, and what specific issues do you pay the most attention to? | What strategies do you use to organize unit tests and integration tests for C code, especially when hardware dependencies are involved? | How do you design data structures in C to balance memory footprint, cache behavior, and ease of use? | Explain how the C build toolchain (compiler options, linker scripts, build systems) influences your development and debugging workflow. | How do you communicate complex low-level design decisions to less experienced engineers so that they can follow your reasoning? | When comparing two alternative C implementations of the same subsystem, what concrete criteria do you use to decide which is better? | Tell me about a complex C-based system you have worked on, and how you ensured the overall design and implementation were robust and well-reasoned.",
    "source_url": "https://editor.superannotate.com/jobs/C-25-417"
  },
  {
    "job_id": "Cpp-25-593",
    "title": "Senior C++ Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$-1 - $55 / hour",
    "pay_min": -1,
    "pay_max": 55,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 3,
    "role_description": "As a Senior C++ Engineer, you will work remotely on an hourly paid basis to review AI-generated C++ code, systems designs, and technical explanations, as well as generate high-quality reference implementations and step-by-step reasoning for complex engineering problems. You will assess solutions for accuracy, clarity, safety, and adherence to the prompt; identify errors in logic, memory handling, concurrency, or performance; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs, where your C++ and systems expertise will directly help improve the world’s premier AI models used by millions of developers worldwide.",
    "keywords": "C++ programming, Systems programming, Low-latency development, Memory management, Concurrency, Code review, Performance optimization, Debugging, English, Remote contractor",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Electrical/Computer Engineering, or a closely related technical field. | 4+ years of professional experience developing production software in C++ for systems, performance-critical, or large-scale applications. | Expert-level proficiency in modern C++ (C++11 and later), including templates, move semantics, smart pointers, and the STL. | Strong background in systems programming concepts such as concurrency, operating systems, low-level performance optimization, and memory management. | Hands-on experience with C++ build systems, compilers, linkers, debuggers, and profiling or analysis tools. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Proven experience conducting detailed C++ code reviews and enforcing coding standards for safety, performance, and maintainability. | Comfort working with version control, CI/CD workflows, and automated testing frameworks in modern C++ projects. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in complex C++ code.",
    "sample_interview_questions": "How do you structure and organize a large C++ codebase so that it remains modular, testable, and maintainable over time? | Can you explain the differences between stack and heap allocation in C++ and how RAII helps manage resources safely? | What are the main differences between modern C++ (C++11 and later) and earlier standards that you rely on in day-to-day development? | How do move semantics and rvalue references work in C++, and when would you explicitly define move constructors or move assignment operators? | Explain how smart pointers (unique_ptr, shared_ptr, weak_ptr) differ and when you would choose each in a real system. | How do you diagnose and prevent common memory issues such as leaks, dangling pointers, and use-after-free bugs in C++? | What is your approach to designing exception-safe C++ code, and how do you handle error propagation in performance-critical paths? | How do you use templates and generic programming in C++ without sacrificing readability and compile times? | Describe how you would design and implement a thread-safe C++ component and what concurrency primitives or libraries you would use. | What techniques do you use to reason about and optimize the performance of C++ code on the critical path, including cache behavior and allocations? | How do you approach interoperability between C++ and C or other languages, and what pitfalls do you watch out for? | Explain your strategy for organizing unit tests and integration tests for C++ code, especially when hardware or OS-specific behavior is involved. | What are common pitfalls you look for during C++ code reviews, particularly around lifetime management and ownership semantics? | How do you use tools such as sanitizers, profilers, and static analyzers to improve the quality of C++ codebases? | Describe how you would structure the build system and dependency management for a large C++ project using CMake or a similar tool. | What considerations are important when writing cross-platform C++ code intended to run on multiple operating systems and architectures? | How do you document complex C++ APIs or subsystems so that other engineers can understand the design decisions and usage patterns? | When comparing two alternative C++ implementations of the same subsystem, what concrete criteria do you use to decide which one is better? | Describe a challenging C++ performance or correctness issue you have solved and how you systematically narrowed down the root cause? | How would you explain advanced C++ concepts such as SFINAE or template metaprogramming to a less experienced engineer in a clear, step-by-step way?",
    "source_url": "https://editor.superannotate.com/jobs/Cpp-25-593"
  },
  {
    "job_id": "Cpp-25-593",
    "title": "Senior C++ Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Indonesia",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 3,
    "role_description": "As a Senior C++ Engineer, you will work remotely on an hourly paid basis to review AI-generated C++ code, systems designs, and technical explanations, as well as generate high-quality reference implementations and step-by-step reasoning for complex engineering problems. You will assess solutions for accuracy, clarity, safety, and adherence to the prompt; identify errors in logic, memory handling, concurrency, or performance; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs, where your C++ and systems expertise will directly help improve the world’s premier AI models used by millions of developers worldwide.",
    "keywords": "C++ programming, Systems programming, Low-latency development, Memory management, Concurrency, Code review, Performance optimization, Debugging, English, Remote contractor",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Electrical/Computer Engineering, or a closely related technical field. | 4+ years of professional experience developing production software in C++ for systems, performance-critical, or large-scale applications. | Expert-level proficiency in modern C++ (C++11 and later), including templates, move semantics, smart pointers, and the STL. | Strong background in systems programming concepts such as concurrency, operating systems, low-level performance optimization, and memory management. | Hands-on experience with C++ build systems, compilers, linkers, debuggers, and profiling or analysis tools. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Proven experience conducting detailed C++ code reviews and enforcing coding standards for safety, performance, and maintainability. | Comfort working with version control, CI/CD workflows, and automated testing frameworks in modern C++ projects. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in complex C++ code.",
    "sample_interview_questions": "How do you structure and organize a large C++ codebase so that it remains modular, testable, and maintainable over time? | Can you explain the differences between stack and heap allocation in C++ and how RAII helps manage resources safely? | What are the main differences between modern C++ (C++11 and later) and earlier standards that you rely on in day-to-day development? | How do move semantics and rvalue references work in C++, and when would you explicitly define move constructors or move assignment operators? | Explain how smart pointers (unique_ptr, shared_ptr, weak_ptr) differ and when you would choose each in a real system. | How do you diagnose and prevent common memory issues such as leaks, dangling pointers, and use-after-free bugs in C++? | What is your approach to designing exception-safe C++ code, and how do you handle error propagation in performance-critical paths? | How do you use templates and generic programming in C++ without sacrificing readability and compile times? | Describe how you would design and implement a thread-safe C++ component and what concurrency primitives or libraries you would use. | What techniques do you use to reason about and optimize the performance of C++ code on the critical path, including cache behavior and allocations? | How do you approach interoperability between C++ and C or other languages, and what pitfalls do you watch out for? | Explain your strategy for organizing unit tests and integration tests for C++ code, especially when hardware or OS-specific behavior is involved. | What are common pitfalls you look for during C++ code reviews, particularly around lifetime management and ownership semantics? | How do you use tools such as sanitizers, profilers, and static analyzers to improve the quality of C++ codebases? | Describe how you would structure the build system and dependency management for a large C++ project using CMake or a similar tool. | What considerations are important when writing cross-platform C++ code intended to run on multiple operating systems and architectures? | How do you document complex C++ APIs or subsystems so that other engineers can understand the design decisions and usage patterns? | When comparing two alternative C++ implementations of the same subsystem, what concrete criteria do you use to decide which one is better? | Describe a challenging C++ performance or correctness issue you have solved and how you systematically narrowed down the root cause? | How would you explain advanced C++ concepts such as SFINAE or template metaprogramming to a less experienced engineer in a clear, step-by-step way?",
    "source_url": "https://editor.superannotate.com/jobs/Cpp-25-593"
  },
  {
    "job_id": "Cpp-25-593",
    "title": "Senior C++ Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "India",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 3,
    "role_description": "As a Senior C++ Engineer, you will work remotely on an hourly paid basis to review AI-generated C++ code, systems designs, and technical explanations, as well as generate high-quality reference implementations and step-by-step reasoning for complex engineering problems. You will assess solutions for accuracy, clarity, safety, and adherence to the prompt; identify errors in logic, memory handling, concurrency, or performance; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs, where your C++ and systems expertise will directly help improve the world’s premier AI models used by millions of developers worldwide.",
    "keywords": "C++ programming, Systems programming, Low-latency development, Memory management, Concurrency, Code review, Performance optimization, Debugging, English, Remote contractor",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Electrical/Computer Engineering, or a closely related technical field. | 4+ years of professional experience developing production software in C++ for systems, performance-critical, or large-scale applications. | Expert-level proficiency in modern C++ (C++11 and later), including templates, move semantics, smart pointers, and the STL. | Strong background in systems programming concepts such as concurrency, operating systems, low-level performance optimization, and memory management. | Hands-on experience with C++ build systems, compilers, linkers, debuggers, and profiling or analysis tools. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Proven experience conducting detailed C++ code reviews and enforcing coding standards for safety, performance, and maintainability. | Comfort working with version control, CI/CD workflows, and automated testing frameworks in modern C++ projects. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in complex C++ code.",
    "sample_interview_questions": "How do you structure and organize a large C++ codebase so that it remains modular, testable, and maintainable over time? | Can you explain the differences between stack and heap allocation in C++ and how RAII helps manage resources safely? | What are the main differences between modern C++ (C++11 and later) and earlier standards that you rely on in day-to-day development? | How do move semantics and rvalue references work in C++, and when would you explicitly define move constructors or move assignment operators? | Explain how smart pointers (unique_ptr, shared_ptr, weak_ptr) differ and when you would choose each in a real system. | How do you diagnose and prevent common memory issues such as leaks, dangling pointers, and use-after-free bugs in C++? | What is your approach to designing exception-safe C++ code, and how do you handle error propagation in performance-critical paths? | How do you use templates and generic programming in C++ without sacrificing readability and compile times? | Describe how you would design and implement a thread-safe C++ component and what concurrency primitives or libraries you would use. | What techniques do you use to reason about and optimize the performance of C++ code on the critical path, including cache behavior and allocations? | How do you approach interoperability between C++ and C or other languages, and what pitfalls do you watch out for? | Explain your strategy for organizing unit tests and integration tests for C++ code, especially when hardware or OS-specific behavior is involved. | What are common pitfalls you look for during C++ code reviews, particularly around lifetime management and ownership semantics? | How do you use tools such as sanitizers, profilers, and static analyzers to improve the quality of C++ codebases? | Describe how you would structure the build system and dependency management for a large C++ project using CMake or a similar tool. | What considerations are important when writing cross-platform C++ code intended to run on multiple operating systems and architectures? | How do you document complex C++ APIs or subsystems so that other engineers can understand the design decisions and usage patterns? | When comparing two alternative C++ implementations of the same subsystem, what concrete criteria do you use to decide which one is better? | Describe a challenging C++ performance or correctness issue you have solved and how you systematically narrowed down the root cause? | How would you explain advanced C++ concepts such as SFINAE or template metaprogramming to a less experienced engineer in a clear, step-by-step way?",
    "source_url": "https://editor.superannotate.com/jobs/Cpp-25-593"
  },
  {
    "job_id": "Cpp-25-593",
    "title": "Senior C++ Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Egypt",
    "pay_rate": "$-1 - $30 / hour",
    "pay_min": -1,
    "pay_max": 30,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 3,
    "role_description": "As a Senior C++ Engineer, you will work remotely on an hourly paid basis to review AI-generated C++ code, systems designs, and technical explanations, as well as generate high-quality reference implementations and step-by-step reasoning for complex engineering problems. You will assess solutions for accuracy, clarity, safety, and adherence to the prompt; identify errors in logic, memory handling, concurrency, or performance; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs, where your C++ and systems expertise will directly help improve the world’s premier AI models used by millions of developers worldwide.",
    "keywords": "C++ programming, Systems programming, Low-latency development, Memory management, Concurrency, Code review, Performance optimization, Debugging, English, Remote contractor",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Electrical/Computer Engineering, or a closely related technical field. | 4+ years of professional experience developing production software in C++ for systems, performance-critical, or large-scale applications. | Expert-level proficiency in modern C++ (C++11 and later), including templates, move semantics, smart pointers, and the STL. | Strong background in systems programming concepts such as concurrency, operating systems, low-level performance optimization, and memory management. | Hands-on experience with C++ build systems, compilers, linkers, debuggers, and profiling or analysis tools. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Proven experience conducting detailed C++ code reviews and enforcing coding standards for safety, performance, and maintainability. | Comfort working with version control, CI/CD workflows, and automated testing frameworks in modern C++ projects. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in complex C++ code.",
    "sample_interview_questions": "How do you structure and organize a large C++ codebase so that it remains modular, testable, and maintainable over time? | Can you explain the differences between stack and heap allocation in C++ and how RAII helps manage resources safely? | What are the main differences between modern C++ (C++11 and later) and earlier standards that you rely on in day-to-day development? | How do move semantics and rvalue references work in C++, and when would you explicitly define move constructors or move assignment operators? | Explain how smart pointers (unique_ptr, shared_ptr, weak_ptr) differ and when you would choose each in a real system. | How do you diagnose and prevent common memory issues such as leaks, dangling pointers, and use-after-free bugs in C++? | What is your approach to designing exception-safe C++ code, and how do you handle error propagation in performance-critical paths? | How do you use templates and generic programming in C++ without sacrificing readability and compile times? | Describe how you would design and implement a thread-safe C++ component and what concurrency primitives or libraries you would use. | What techniques do you use to reason about and optimize the performance of C++ code on the critical path, including cache behavior and allocations? | How do you approach interoperability between C++ and C or other languages, and what pitfalls do you watch out for? | Explain your strategy for organizing unit tests and integration tests for C++ code, especially when hardware or OS-specific behavior is involved. | What are common pitfalls you look for during C++ code reviews, particularly around lifetime management and ownership semantics? | How do you use tools such as sanitizers, profilers, and static analyzers to improve the quality of C++ codebases? | Describe how you would structure the build system and dependency management for a large C++ project using CMake or a similar tool. | What considerations are important when writing cross-platform C++ code intended to run on multiple operating systems and architectures? | How do you document complex C++ APIs or subsystems so that other engineers can understand the design decisions and usage patterns? | When comparing two alternative C++ implementations of the same subsystem, what concrete criteria do you use to decide which one is better? | Describe a challenging C++ performance or correctness issue you have solved and how you systematically narrowed down the root cause? | How would you explain advanced C++ concepts such as SFINAE or template metaprogramming to a less experienced engineer in a clear, step-by-step way?",
    "source_url": "https://editor.superannotate.com/jobs/Cpp-25-593"
  },
  {
    "job_id": "Cpp-25-593",
    "title": "Senior C++ Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Brazil",
    "pay_rate": "$-1 - $40 / hour",
    "pay_min": -1,
    "pay_max": 40,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 3,
    "role_description": "As a Senior C++ Engineer, you will work remotely on an hourly paid basis to review AI-generated C++ code, systems designs, and technical explanations, as well as generate high-quality reference implementations and step-by-step reasoning for complex engineering problems. You will assess solutions for accuracy, clarity, safety, and adherence to the prompt; identify errors in logic, memory handling, concurrency, or performance; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs, where your C++ and systems expertise will directly help improve the world’s premier AI models used by millions of developers worldwide.",
    "keywords": "C++ programming, Systems programming, Low-latency development, Memory management, Concurrency, Code review, Performance optimization, Debugging, English, Remote contractor",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Electrical/Computer Engineering, or a closely related technical field. | 4+ years of professional experience developing production software in C++ for systems, performance-critical, or large-scale applications. | Expert-level proficiency in modern C++ (C++11 and later), including templates, move semantics, smart pointers, and the STL. | Strong background in systems programming concepts such as concurrency, operating systems, low-level performance optimization, and memory management. | Hands-on experience with C++ build systems, compilers, linkers, debuggers, and profiling or analysis tools. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Proven experience conducting detailed C++ code reviews and enforcing coding standards for safety, performance, and maintainability. | Comfort working with version control, CI/CD workflows, and automated testing frameworks in modern C++ projects. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in complex C++ code.",
    "sample_interview_questions": "How do you structure and organize a large C++ codebase so that it remains modular, testable, and maintainable over time? | Can you explain the differences between stack and heap allocation in C++ and how RAII helps manage resources safely? | What are the main differences between modern C++ (C++11 and later) and earlier standards that you rely on in day-to-day development? | How do move semantics and rvalue references work in C++, and when would you explicitly define move constructors or move assignment operators? | Explain how smart pointers (unique_ptr, shared_ptr, weak_ptr) differ and when you would choose each in a real system. | How do you diagnose and prevent common memory issues such as leaks, dangling pointers, and use-after-free bugs in C++? | What is your approach to designing exception-safe C++ code, and how do you handle error propagation in performance-critical paths? | How do you use templates and generic programming in C++ without sacrificing readability and compile times? | Describe how you would design and implement a thread-safe C++ component and what concurrency primitives or libraries you would use. | What techniques do you use to reason about and optimize the performance of C++ code on the critical path, including cache behavior and allocations? | How do you approach interoperability between C++ and C or other languages, and what pitfalls do you watch out for? | Explain your strategy for organizing unit tests and integration tests for C++ code, especially when hardware or OS-specific behavior is involved. | What are common pitfalls you look for during C++ code reviews, particularly around lifetime management and ownership semantics? | How do you use tools such as sanitizers, profilers, and static analyzers to improve the quality of C++ codebases? | Describe how you would structure the build system and dependency management for a large C++ project using CMake or a similar tool. | What considerations are important when writing cross-platform C++ code intended to run on multiple operating systems and architectures? | How do you document complex C++ APIs or subsystems so that other engineers can understand the design decisions and usage patterns? | When comparing two alternative C++ implementations of the same subsystem, what concrete criteria do you use to decide which one is better? | Describe a challenging C++ performance or correctness issue you have solved and how you systematically narrowed down the root cause? | How would you explain advanced C++ concepts such as SFINAE or template metaprogramming to a less experienced engineer in a clear, step-by-step way?",
    "source_url": "https://editor.superannotate.com/jobs/Cpp-25-593"
  },
  {
    "job_id": "Cpp-25-593",
    "title": "Senior C++ Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Portugal",
    "pay_rate": "$-1 - $45 / hour",
    "pay_min": -1,
    "pay_max": 45,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 3,
    "role_description": "As a Senior C++ Engineer, you will work remotely on an hourly paid basis to review AI-generated C++ code, systems designs, and technical explanations, as well as generate high-quality reference implementations and step-by-step reasoning for complex engineering problems. You will assess solutions for accuracy, clarity, safety, and adherence to the prompt; identify errors in logic, memory handling, concurrency, or performance; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs, where your C++ and systems expertise will directly help improve the world’s premier AI models used by millions of developers worldwide.",
    "keywords": "C++ programming, Systems programming, Low-latency development, Memory management, Concurrency, Code review, Performance optimization, Debugging, English, Remote contractor",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Electrical/Computer Engineering, or a closely related technical field. | 4+ years of professional experience developing production software in C++ for systems, performance-critical, or large-scale applications. | Expert-level proficiency in modern C++ (C++11 and later), including templates, move semantics, smart pointers, and the STL. | Strong background in systems programming concepts such as concurrency, operating systems, low-level performance optimization, and memory management. | Hands-on experience with C++ build systems, compilers, linkers, debuggers, and profiling or analysis tools. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Proven experience conducting detailed C++ code reviews and enforcing coding standards for safety, performance, and maintainability. | Comfort working with version control, CI/CD workflows, and automated testing frameworks in modern C++ projects. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in complex C++ code.",
    "sample_interview_questions": "How do you structure and organize a large C++ codebase so that it remains modular, testable, and maintainable over time? | Can you explain the differences between stack and heap allocation in C++ and how RAII helps manage resources safely? | What are the main differences between modern C++ (C++11 and later) and earlier standards that you rely on in day-to-day development? | How do move semantics and rvalue references work in C++, and when would you explicitly define move constructors or move assignment operators? | Explain how smart pointers (unique_ptr, shared_ptr, weak_ptr) differ and when you would choose each in a real system. | How do you diagnose and prevent common memory issues such as leaks, dangling pointers, and use-after-free bugs in C++? | What is your approach to designing exception-safe C++ code, and how do you handle error propagation in performance-critical paths? | How do you use templates and generic programming in C++ without sacrificing readability and compile times? | Describe how you would design and implement a thread-safe C++ component and what concurrency primitives or libraries you would use. | What techniques do you use to reason about and optimize the performance of C++ code on the critical path, including cache behavior and allocations? | How do you approach interoperability between C++ and C or other languages, and what pitfalls do you watch out for? | Explain your strategy for organizing unit tests and integration tests for C++ code, especially when hardware or OS-specific behavior is involved. | What are common pitfalls you look for during C++ code reviews, particularly around lifetime management and ownership semantics? | How do you use tools such as sanitizers, profilers, and static analyzers to improve the quality of C++ codebases? | Describe how you would structure the build system and dependency management for a large C++ project using CMake or a similar tool. | What considerations are important when writing cross-platform C++ code intended to run on multiple operating systems and architectures? | How do you document complex C++ APIs or subsystems so that other engineers can understand the design decisions and usage patterns? | When comparing two alternative C++ implementations of the same subsystem, what concrete criteria do you use to decide which one is better? | Describe a challenging C++ performance or correctness issue you have solved and how you systematically narrowed down the root cause? | How would you explain advanced C++ concepts such as SFINAE or template metaprogramming to a less experienced engineer in a clear, step-by-step way?",
    "source_url": "https://editor.superannotate.com/jobs/Cpp-25-593"
  },
  {
    "job_id": "Cpp-25-593",
    "title": "Senior C++ Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Spain",
    "pay_rate": "$-1 - $50 / hour",
    "pay_min": -1,
    "pay_max": 50,
    "currency": "USD",
    "num_people": 150,
    "priority_level": 3,
    "role_description": "As a Senior C++ Engineer, you will work remotely on an hourly paid basis to review AI-generated C++ code, systems designs, and technical explanations, as well as generate high-quality reference implementations and step-by-step reasoning for complex engineering problems. You will assess solutions for accuracy, clarity, safety, and adherence to the prompt; identify errors in logic, memory handling, concurrency, or performance; fact-check technical information; write high-quality explanations and model solutions that demonstrate correct methods; and rate and compare multiple AI responses based on correctness and reasoning quality. \n\nThis fully remote, hourly paid contractor role is with SME Careers, a fast-growing AI data services company and subsidiary of SuperAnnotate that provides AI training data to many of the world’s largest AI companies and foundation model labs, where your C++ and systems expertise will directly help improve the world’s premier AI models used by millions of developers worldwide.",
    "keywords": "C++ programming, Systems programming, Low-latency development, Memory management, Concurrency, Code review, Performance optimization, Debugging, English, Remote contractor",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Electrical/Computer Engineering, or a closely related technical field. | 4+ years of professional experience developing production software in C++ for systems, performance-critical, or large-scale applications. | Expert-level proficiency in modern C++ (C++11 and later), including templates, move semantics, smart pointers, and the STL. | Strong background in systems programming concepts such as concurrency, operating systems, low-level performance optimization, and memory management. | Hands-on experience with C++ build systems, compilers, linkers, debuggers, and profiling or analysis tools. | Minimum C1 English proficiency (written and spoken), with the ability to write clear technical explanations and follow detailed English-language guidelines. | Proven experience conducting detailed C++ code reviews and enforcing coding standards for safety, performance, and maintainability. | Comfort working with version control, CI/CD workflows, and automated testing frameworks in modern C++ projects. | Previous experience with AI data training, annotation, or evaluating AI-generated technical content is a strong plus. | Highly detail-oriented and systematic, with a methodical approach to evaluating reasoning quality and identifying subtle issues in complex C++ code.",
    "sample_interview_questions": "How do you structure and organize a large C++ codebase so that it remains modular, testable, and maintainable over time? | Can you explain the differences between stack and heap allocation in C++ and how RAII helps manage resources safely? | What are the main differences between modern C++ (C++11 and later) and earlier standards that you rely on in day-to-day development? | How do move semantics and rvalue references work in C++, and when would you explicitly define move constructors or move assignment operators? | Explain how smart pointers (unique_ptr, shared_ptr, weak_ptr) differ and when you would choose each in a real system. | How do you diagnose and prevent common memory issues such as leaks, dangling pointers, and use-after-free bugs in C++? | What is your approach to designing exception-safe C++ code, and how do you handle error propagation in performance-critical paths? | How do you use templates and generic programming in C++ without sacrificing readability and compile times? | Describe how you would design and implement a thread-safe C++ component and what concurrency primitives or libraries you would use. | What techniques do you use to reason about and optimize the performance of C++ code on the critical path, including cache behavior and allocations? | How do you approach interoperability between C++ and C or other languages, and what pitfalls do you watch out for? | Explain your strategy for organizing unit tests and integration tests for C++ code, especially when hardware or OS-specific behavior is involved. | What are common pitfalls you look for during C++ code reviews, particularly around lifetime management and ownership semantics? | How do you use tools such as sanitizers, profilers, and static analyzers to improve the quality of C++ codebases? | Describe how you would structure the build system and dependency management for a large C++ project using CMake or a similar tool. | What considerations are important when writing cross-platform C++ code intended to run on multiple operating systems and architectures? | How do you document complex C++ APIs or subsystems so that other engineers can understand the design decisions and usage patterns? | When comparing two alternative C++ implementations of the same subsystem, what concrete criteria do you use to decide which one is better? | Describe a challenging C++ performance or correctness issue you have solved and how you systematically narrowed down the root cause? | How would you explain advanced C++ concepts such as SFINAE or template metaprogramming to a less experienced engineer in a clear, step-by-step way?",
    "source_url": "https://editor.superannotate.com/jobs/Cpp-25-593"
  },
  {
    "job_id": "Legal-25-177",
    "title": "Legal Expert",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Canada",
    "pay_rate": "$-1 - $100 / hour",
    "pay_min": -1,
    "pay_max": 100,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated legal analyses and/or generate expert legal content, evaluating reasoning quality and step-by-step issue-spotting while providing precise written feedback. You will assess answers for accuracy, clarity, and adherence to the prompt; identify errors in legal methodology or doctrine; fact-check citations and stated rules; write high-quality explanations and model solutions; and rate and compare multiple responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate, delivering training data for many of the world’s largest AI companies and foundation-model labs. Your legal expertise directly helps improve the world’s premier AI models by making their legal reasoning more accurate, reliable, and clearly explained.",
    "keywords": "Legal Reasoning, Statutory Interpretation, Case Law Analysis, Contracts, Torts, Civil Procedure, Criminal Law, Regulatory & Administrative Law, Compliance, English (C1+)",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Law (JD/LLB), Legal Studies, Public Policy, or Political Science, with strong grounding in Constitutional Law, Contracts, Torts, Criminal Law, Civil Procedure, and Regulatory/Administrative Law. | 5+ years of professional experience in Law, Legal Studies, Public Policy, or Political Science. | Strong legal reasoning and issue-spotting skills, including statutory interpretation, case analysis, and translating facts into elements and defenses. | Fluent in compliance concepts, rights & obligations analysis, and policy frameworks; able to identify missing facts and assumptions that change outcomes. | Exceptional attention to detail when checking rule statements, citations, jurisdictional relevance, procedural posture, and logical consistency; Minimum C1 English proficiency. | Able to write clear, structured feedback that explains errors and the correct reasoning path without unnecessary verbosity. | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation, expert review, legal editing, or QA is strongly preferred.",
    "sample_interview_questions": "You’re reviewing a memo that treats statutory silence as authorization. What interpretive steps would you expect before accepting that conclusion (text, structure, canons, context, legislative history, purpose), and what red flags indicate overreach? | A proposed rule includes a broad definition that appears to exceed the agency’s enabling statute. What analytical checkpoints distinguish a permissible interpretation from an ultra vires expansion? | A contract dispute analysis concludes ‘no breach’ because damages are hard to quantify. What doctrinal mistakes might that reflect, and what facts would you probe to assess breach, causation, and remedies separately? | A force majeure clause is invoked after a supply disruption. What is your framework for evaluating whether the event qualifies, whether performance is excused or delayed, and what notice/mitigation duties typically matter most? | Two cases are cited as controlling precedent, but they are from different jurisdictions and procedural postures. What questions do you ask to determine their binding weight and relevance to the issue presented? | A tort analysis assumes foreseeability is satisfied because harm occurred. What nuanced distinctions in duty/breach/proximate cause would you expect to see addressed to avoid outcome-driven reasoning? | A negligence memo cites res ipsa loquitur as shifting the burden of proof universally. What clarifying conditions and jurisdictional variations would you look for before accepting that statement? | A criminal law analysis treats mens rea as interchangeable across offenses. What issue-spotting steps ensure the correct mental state is matched to each element, and what common pitfalls should be caught? | A Fourth Amendment discussion assumes any warrantless search is unconstitutional. What structured exceptions and threshold questions must be considered to evaluate reasonableness properly? | A civil procedure answer asserts personal jurisdiction based solely on internet accessibility. What facts and doctrinal tests would you require to evaluate minimum contacts and purposeful availment credibly? | A removal analysis ignores timing and the forum-defendant rule. What procedural failure points do you check first when reviewing removal/ remand arguments? | A summary judgment discussion argues that credibility disputes can be resolved by the judge. What is the correct standard, and what indicators tell you the writer misunderstood the role of inferences and fact disputes? | A compliance assessment concludes ‘no risk’ because the policy exists. What evidence would you require to evaluate program effectiveness (controls, training, monitoring, enforcement) rather than paper compliance? | A regulatory memo cites a guidance document as if it were binding law. What questions distinguish legislative rules, interpretive rules, policy statements, and guidance—and why does it matter for enforcement risk? | An analysis claims a private right of action under a federal statute without addressing the test for implied rights. What doctrinal pathway must be analyzed, and what signals suggest the claim is weak? | A constitutional law answer applies strict scrutiny to a regulation without identifying the right or classification precisely. What triage approach do you use to select the correct level of scrutiny? | A due process argument focuses only on fairness, ignoring property/liberty interest and procedures due. What elements must be established before balancing tests become relevant? | A contracts analysis treats an ‘agreement in principle’ as fully enforceable. What formation issues (intent, definiteness, consideration, conditions precedent) do you examine to determine enforceability? | You are comparing two legal analyses: one is very confident but cites no authority; the other cites authority but uses conclusory reasoning. What criteria do you use to judge which is more trustworthy and where each fails? | When reviewing a legal answer for correctness, what are your top three ‘sanity checks’ (e.g., rule statement precision, element-by-element application, procedural posture fit), and what specific defects trigger an immediate rewrite?",
    "source_url": "https://editor.superannotate.com/jobs/Legal-25-177"
  },
  {
    "job_id": "Finance-25-141",
    "title": "Senior Finance Specialist",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Canada",
    "pay_rate": "$-1 - $70 / hour",
    "pay_min": -1,
    "pay_max": 70,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated finance analyses and/or generate expert finance content, evaluating reasoning quality and step-by-step problem-solving while providing precise written feedback. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify methodological or conceptual errors; fact-check financial claims and assumptions; write high-quality explanations and model solutions; and rate and compare multiple responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate, delivering training data for many of the world’s largest AI companies and foundation-model labs. Your finance expertise directly helps improve the world’s premier AI models by making their financial reasoning more accurate, reliable, and clearly explained.",
    "keywords": "Financial Statement Analysis, Valuation, Corporate Finance, Investments, Risk/Return, Financial Modeling, Econometrics, Macroeconomics, Accounting, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Finance, Economics, Accounting, Business, Financial Engineering, or Quantitative Finance (including coursework in Micro/Macro, Corporate Finance, Investments, Econometrics, and Accounting). | 5+ years of professional experience in Finance, Economics, Accounting, Business, Financial Engineering, or Quantitative Finance. | Strong command of financial statements, valuation (DCF and multiples), time value of money, and markets/instruments. | Confident in risk/return reasoning, basic macro & microeconomic intuition, and financial modeling logic (assumptions, drivers, sensitivities, consistency checks). | Able to rigorously review and explain reasoning, identify methodological errors, and fact-check claims with high attention to detail; Minimum C1 English proficiency. | Comfortable applying structured sanity checks (conservation-style checks for finance: reconciliation, sign/units consistency, boundary cases, and plausibility bounds). | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation, expert review, or editorial QA is strongly preferred.",
    "sample_interview_questions": "You’re reviewing a DCF where FCF grows faster than revenue for 10 years. What specific reconciliation checks would you apply to determine whether the margin expansion and reinvestment assumptions are internally consistent? | An analysis uses WACC with a beta from a different sector and a single-point ERP assumption. What evidence would you require to accept the beta and ERP choices, and what failure modes do you watch for in the resulting valuation? | A model treats stock-based compensation as a non-cash add-back with no dilution impact. What are the most common ways this misstates value, and what would you look for to correct it? | A company reports rising EBITDA but consistently negative operating cash flow. What are the highest-signal drivers you would investigate first, and what patterns would differentiate working-capital timing from earnings quality issues? | A leveraged buyout case shows a very high IRR despite modest multiple expansion. What are the likely hidden assumptions that can create this outcome, and how would you stress-test them conceptually? | You’re given two comparable-company sets: one yields a much higher multiple range than the other. What selection biases or capital-structure effects would you check before concluding the target is mispriced? | A model forecasts constant gross margin while input costs are volatile and partially hedged. What questions would you ask to assess whether hedging policy and pass-through dynamics are modeled credibly? | An analyst uses CAPM for a firm with meaningful country risk and concentrated customer exposure. What adjustments or alternative frameworks would you consider, and what decision criteria guide whether to apply them? | You suspect a model double-counts growth by combining high terminal growth with aggressive near-term share gains. What are the telltale signs of double-counting, and how would you reframe the growth story to avoid it? | A financial statement review shows capitalized costs rising rapidly. What accounting and cash-flow impacts would you expect, and what would you ask to determine whether capitalization is appropriate versus earnings management? | An NPV analysis for a project uses nominal cash flows but a real discount rate. What symptoms would appear in the output, and what quick checks would you use to catch this mismatch? | A bank model assumes deposit costs remain flat even as benchmark rates rise. What balance-sheet and competitive dynamics would you examine to judge whether the assumption is defensible? | A credit memo claims strong interest coverage, but most cash generation is from non-recurring working-capital releases. How would you re-assess debt service capacity and what indicators would you prioritize? | Two valuation methods disagree materially: DCF suggests upside while multiples imply downside. What structured diagnostic steps would you take to isolate whether the divergence is driven by cash-flow forecast, discounting, or comp selection? | A portfolio optimization write-up claims higher expected return with lower risk after adding a new asset. What assumptions about correlations and estimation error would you challenge, and why? | A risk analysis uses VaR without discussing fat tails or liquidity horizons. What scenario-based critiques would you raise, and what alternative risk views would you insist on before trusting the conclusion? | A model uses ‘plug’ items to balance the balance sheet. Under what circumstances can plugs be acceptable, and what are the most dangerous plugs that typically conceal major errors? | A merger model shows EPS accretion, but you suspect value destruction. What conceptual checks would you use to distinguish accounting accretion from economic value creation? | You find a sensitivity table that varies only revenue growth and discount rate. What missing sensitivities are usually most decision-relevant for this type of valuation, and what tells you which to prioritize? | When comparing two competing analyses, one is numerically polished but assumption-light; the other is assumption-rich but messier. What criteria do you use to judge which is more credible and decision-safe?",
    "source_url": "https://editor.superannotate.com/jobs/Finance-25-141"
  },
  {
    "job_id": "Medical-25-104",
    "title": "Medical General Expert",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Canada",
    "pay_rate": "$40 - $90 / hour",
    "pay_min": 40,
    "pay_max": 90,
    "currency": "USD",
    "num_people": 50,
    "priority_level": 3,
    "role_description": "In this hourly, remote contractor role, you will review AI-generated medical responses and/or generate expert healthcare content, evaluating reasoning quality and step-by-step clinical problem-solving while providing precise written feedback. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in clinical methodology or conceptual understanding; fact-check medical information; write high-quality explanations and model solutions that demonstrate correct reasoning; and rate and compare multiple responses based on correctness and reasoning quality. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate, delivering training data for many of the world’s largest AI companies and foundation-model labs. Your healthcare expertise directly helps improve the world’s premier AI models by making their clinical and public-health reasoning more accurate, reliable, and clearly explained.",
    "keywords": "Clinical Reasoning, Disease Processes, Patient Care, Epidemiology, Public Health, Healthcare Systems, Medical Terminology, Risk Stratification, Evidence-Based Medicine, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree (or higher) in Medicine (MD/DO), Nursing, Public Health (MPH), Health Sciences, or Allied Health, with strong grounding in Epidemiology, Clinical Medicine, Healthcare Systems, and Patient Care. | 5+ years of professional experience in Nursing, Public Health, Health Sciences, or Allied Health. | Confident in clinical reasoning (differential diagnosis, risk stratification, red-flag recognition) and explaining why a conclusion follows from the evidence. | Strong understanding of disease processes, patient care concepts, public health principles, healthcare systems, and medical terminology. | Exceptional attention to detail when fact-checking medical content and identifying unsafe assumptions, missing contraindications, or misinterpretation of tests; Minimum C1 English proficiency. | Comfortable evaluating answers for internal consistency (timelines, physiology, dosing logic), appropriateness for setting (ED vs outpatient), and patient safety implications. | Reliable, self-directed, and able to deliver consistent quality in an hourly, remote contractor workflow across time zones. | Previous experience with AI data training/annotation, clinical documentation review, utilization review, or healthcare editorial QA is strongly preferred.",
    "sample_interview_questions": "A patient has chest pain with a nondiagnostic initial ECG and a borderline troponin. What clinical features and risk-stratification considerations most strongly influence whether you treat this as ACS, and what common reasoning errors cause unsafe reassurance? | A workup plan proposes broad-spectrum antibiotics for suspected pneumonia without assessing severity or likely pathogen. What decision points should be used to justify outpatient vs inpatient care and narrow vs broad coverage in principle? | You’re reviewing a differential diagnosis that lists many conditions but fails to prioritize. What criteria do you use to rank likely vs dangerous causes, and how do you identify when the differential is missing a must-not-miss diagnosis? | A note attributes dyspnea to anxiety after a normal chest X-ray. What additional history, exam, and basic test patterns would you expect to be considered before accepting a functional diagnosis? | A clinical summary treats ‘normal vital signs’ as excluding sepsis. What physiologic and timing considerations make this reasoning unreliable, and what alternative framing is more clinically sound? | An interpretation claims a normal anion gap rules out metabolic acidosis. What conceptual checks (mixed disorders, albumin effect, respiratory compensation) would you apply to validate acid–base conclusions? | A diabetes management answer recommends intensifying insulin based only on a single high glucose. What factors should be assessed first (measurement context, hypoglycemia risk, adherence, renal function, intercurrent illness) to avoid harm? | A medication safety review misses a high-risk drug–drug interaction. What systematic approach do you use to surface the most clinically significant interactions (severity, mechanism, timing, patient factors)? | A patient-care rationale uses relative risk reduction to argue a therapy is ‘highly effective’ without absolute risk. What questions would you ask to translate that into absolute benefit and number needed to treat for decision-making? | A screening recommendation is made without considering pretest probability and harms. What principles guide when screening is net-beneficial, and what red flags suggest over-screening? | A pediatric fever case is managed as viral illness with no safety-netting. What clinical reasoning elements justify no immediate testing, and what return precautions are most essential conceptually? | An analysis concludes that a positive test confirms disease without considering base rates. How do you explain, at a clinical level, why PPV changes with prevalence and what that implies for interpreting results? | A public-health summary claims an intervention ‘caused’ a mortality decrease based on a before/after comparison. What alternative explanations (confounding, secular trends, regression to the mean) must be assessed before causal language is acceptable? | An outbreak investigation proposal selects a convenient sample and then generalizes findings. What bias risks does this create, and what design choices would make conclusions more credible? | A guideline-based care answer copies recommendations but ignores contraindications and patient preferences. What checks ensure applicability to the individual patient and avoid guideline misapplication? | A triage recommendation underestimates risk in an elderly patient with atypical symptoms. What age-related presentation differences and baseline risks should alter threshold decisions? | A case analysis cites ‘normal labs’ as excluding appendicitis. What is the correct reasoning about sensitivity/specificity of common tests here, and how do you avoid test-anchoring? | A discharge plan fails to address social determinants that affect adherence. What key nonclinical factors most predict failure of outpatient management, and how should they alter disposition reasoning? | A clinician assumes oxygen saturation is a complete measure of respiratory status. What physiologic limitations make that incomplete, and what additional indicators help avoid missed deterioration? | When comparing two competing clinical explanations, one is elegant but ignores edge cases, and the other is more complex but safer. What criteria do you use to judge which reasoning is more defensible for patient safety?",
    "source_url": "https://editor.superannotate.com/jobs/Medical-25-104"
  },
  {
    "job_id": "CSharp-26-482",
    "title": "C# Software Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "The United States",
    "pay_rate": "$30 - $65 / hour",
    "pay_min": 30,
    "pay_max": 65,
    "currency": "USD",
    "num_people": 30,
    "priority_level": 3,
    "role_description": "This is an hourly-paid, fully remote contractor role where you will review AI-generated responses and/or generate C#/.NET engineering content, evaluating reasoning quality and step-by-step problem-solving. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical claims; and rate/compare multiple responses based on correctness and reasoning quality. You will also produce high-quality explanations and model solutions that demonstrate correct methods and clear communication. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation-model labs—your work directly helps improve the world’s premier AI models.",
    "keywords": "C#, .NET, ASP.NET Core, Web APIs, async/await, LINQ, SQL, EF Core, Docker, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Software Engineering, or a related field (or equivalent professional experience). | 2+ years of professional experience using C#. | Strong experience with C# and the .NET ecosystem (ASP.NET Core, Web APIs, async/await, LINQ). | Knowledge of relational databases and data access patterns (SQL Server and/or PostgreSQL; EF Core and/or Dapper). | Experience with modern engineering practices and tooling (Git, CI/CD, Docker). | Strong advantage: experience solving Medium–Hard algorithmic problems (e.g., LeetCode/HackerRank-style). | Strong English writing skills; Minimum C1 English proficiency. | Highly detail-oriented, able to spot subtle logical/technical issues and explain corrections clearly. | Preferred: prior experience with AI data training, evaluation, annotation, or technical QA of model outputs. | Able to work as an hourly, remote contractor with reliable internet and consistent availability.",
    "sample_interview_questions": "When would you choose a Web API endpoint to return 200 vs 204 vs 400 vs 404 vs 409, and what practical client behaviors are you optimizing for? | In ASP.NET Core, what are the trade-offs between middleware, filters, and endpoint handlers for cross-cutting concerns like validation, logging, and auth? | How do you reason about async/await deadlocks in .NET, and what patterns prevent them in library code vs ASP.NET Core request pipelines? | Describe a scenario where using Task.Run is an anti-pattern in server code. What would you do instead and why? | How do you decide between IAsyncEnumerable, streaming responses, and pagination for large datasets exposed via Web APIs? | EF Core: what issues can arise from tracking too many entities, and how do you mitigate them while keeping correctness? | Explain how you’d approach diagnosing an intermittent performance regression in an EF Core query without changing the database schema. | When would you choose Dapper over EF Core (or vice versa) for a feature, and what risks do you explicitly manage with each choice? | What are common causes of N+1 query problems in ORMs, and how do you detect them in a production system? | How do you design transactional boundaries when a request touches multiple aggregates/tables, and how do you handle partial failure? | What concurrency problems can occur with optimistic concurrency in EF Core, and how should an API surface those conflicts to callers? | How do you structure domain validation so it’s consistent across API layer, domain layer, and persistence layer without duplicating rules? | What are the practical differences between exception-based flow and result-based flow (e.g., OneOf/Result types) in C# services, and when do you prefer each? | How do you evaluate and select a logging strategy (structured logs, correlation IDs, log levels) that supports debugging under load? | What are the main security pitfalls you watch for in Web APIs (authz, injection, deserialization, secrets), and how do you systematically prevent them? | In Dockerized .NET services, what are common production issues related to configuration, networking, and resource limits, and how do you detect them early? | How do you design a CI/CD pipeline for a .NET API to balance speed and safety (tests, static analysis, deployment gates)? | What testing strategy do you use to balance unit tests, integration tests (DB), and contract tests for APIs, and what failure modes does each catch? | Algorithmic reasoning: how do you choose between a hash-based approach vs sorting vs two-pointer techniques when optimizing for time and memory, and what edge cases often break naïve solutions? | Describe a time you had to refactor code for maintainability without changing behavior. What signals told you it was necessary, and how did you validate you didn’t introduce regressions?",
    "source_url": "https://editor.superannotate.com/jobs/CSharp-26-482"
  },
  {
    "job_id": "CSharp-26-482",
    "title": "C# Software Engineer",
    "company": "SuperAnnotate",
    "status": "open",
    "employment_type": "Contract",
    "location_type": "Remote",
    "country": "Armenia",
    "pay_rate": "$25 - $50 / hour",
    "pay_min": 25,
    "pay_max": 50,
    "currency": "USD",
    "num_people": 30,
    "priority_level": 3,
    "role_description": "This is an hourly-paid, fully remote contractor role where you will review AI-generated responses and/or generate C#/.NET engineering content, evaluating reasoning quality and step-by-step problem-solving. You will assess solutions for accuracy, clarity, and adherence to the prompt; identify errors in methodology or conceptual understanding; fact-check technical claims; and rate/compare multiple responses based on correctness and reasoning quality. You will also produce high-quality explanations and model solutions that demonstrate correct methods and clear communication. This role is with SME Careers, a fast-growing AI Data Services company and subsidiary of SuperAnnotate that provides AI training data for many of the world’s largest AI companies and foundation-model labs—your work directly helps improve the world’s premier AI models.",
    "keywords": "C#, .NET, ASP.NET Core, Web APIs, async/await, LINQ, SQL, EF Core, Docker, English",
    "key_responsibilities": "Develop AI Training Content: Create detailed prompts in various topics and responses to guide AI learning, ensuring the models reflect a comprehensive understanding of diverse subjects. | Optimize AI Performance: Evaluate and rank AI responses to enhance the model's accuracy, fluency, and contextual relevance. | Ensure Model Integrity: Test AI models for potential inaccuracies or biases, validating their reliability across use cases.",
    "your_profile": "Bachelor’s degree or higher in Computer Science, Software Engineering, or a related field (or equivalent professional experience). | 2+ years of professional experience using C#. | Strong experience with C# and the .NET ecosystem (ASP.NET Core, Web APIs, async/await, LINQ). | Knowledge of relational databases and data access patterns (SQL Server and/or PostgreSQL; EF Core and/or Dapper). | Experience with modern engineering practices and tooling (Git, CI/CD, Docker). | Strong advantage: experience solving Medium–Hard algorithmic problems (e.g., LeetCode/HackerRank-style). | Strong English writing skills; Minimum C1 English proficiency. | Highly detail-oriented, able to spot subtle logical/technical issues and explain corrections clearly. | Preferred: prior experience with AI data training, evaluation, annotation, or technical QA of model outputs. | Able to work as an hourly, remote contractor with reliable internet and consistent availability.",
    "sample_interview_questions": "When would you choose a Web API endpoint to return 200 vs 204 vs 400 vs 404 vs 409, and what practical client behaviors are you optimizing for? | In ASP.NET Core, what are the trade-offs between middleware, filters, and endpoint handlers for cross-cutting concerns like validation, logging, and auth? | How do you reason about async/await deadlocks in .NET, and what patterns prevent them in library code vs ASP.NET Core request pipelines? | Describe a scenario where using Task.Run is an anti-pattern in server code. What would you do instead and why? | How do you decide between IAsyncEnumerable, streaming responses, and pagination for large datasets exposed via Web APIs? | EF Core: what issues can arise from tracking too many entities, and how do you mitigate them while keeping correctness? | Explain how you’d approach diagnosing an intermittent performance regression in an EF Core query without changing the database schema. | When would you choose Dapper over EF Core (or vice versa) for a feature, and what risks do you explicitly manage with each choice? | What are common causes of N+1 query problems in ORMs, and how do you detect them in a production system? | How do you design transactional boundaries when a request touches multiple aggregates/tables, and how do you handle partial failure? | What concurrency problems can occur with optimistic concurrency in EF Core, and how should an API surface those conflicts to callers? | How do you structure domain validation so it’s consistent across API layer, domain layer, and persistence layer without duplicating rules? | What are the practical differences between exception-based flow and result-based flow (e.g., OneOf/Result types) in C# services, and when do you prefer each? | How do you evaluate and select a logging strategy (structured logs, correlation IDs, log levels) that supports debugging under load? | What are the main security pitfalls you watch for in Web APIs (authz, injection, deserialization, secrets), and how do you systematically prevent them? | In Dockerized .NET services, what are common production issues related to configuration, networking, and resource limits, and how do you detect them early? | How do you design a CI/CD pipeline for a .NET API to balance speed and safety (tests, static analysis, deployment gates)? | What testing strategy do you use to balance unit tests, integration tests (DB), and contract tests for APIs, and what failure modes does each catch? | Algorithmic reasoning: how do you choose between a hash-based approach vs sorting vs two-pointer techniques when optimizing for time and memory, and what edge cases often break naïve solutions? | Describe a time you had to refactor code for maintainability without changing behavior. What signals told you it was necessary, and how did you validate you didn’t introduce regressions?",
    "source_url": "https://editor.superannotate.com/jobs/CSharp-26-482"
  }
]